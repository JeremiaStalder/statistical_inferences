[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"\"book can ever finished. working learn just enough find immature moment turn away .\"\nKarl Popper, Open Society EnemiesThis open educational resource integrates information blog, MOOCs Improving Statistical Inferences Improving Statistical Questions, scientific work. goal make information accessible, easier update future.re-used adapted (parts ) open access articles, without adding quotation marks. Immense gratitude collaborators Casper Albers, Farid Anvari, Aaron Caldwell, Harlan Cambell, Nicholas Coles, Lisa DeBruine, Marie Delacre, Zoltan Dienes, Noah van Dongen, Alexander Etz, Ellen Evers, Jaroslav Gottfriend, Seth Green, Christopher Harms, Arianne Herrera-Bennett, Joe Hilgard, Peder Isager, Maximilian Maier, Neil McLatchie, Brian Nosek, Friedrich Pahlke, Pepijn Obels, Amy Orben, Anne Scheel, Janneke Staaks, Leo Tiokhin, Mehmet Tunç, Duygu Uygun Tunç, Gernot Wassmer, contributed substantially ideas open educational resource. also like thank Zeki Akyol, Emrah Er, Lewis Halsey, Kyle Hamilton, David Lane, Jeremiah Lewis, Leong Utek gave comments GitHub Twitter improve textbook. resource created sabbatical Padova University, thanks Advanced Data Analysis Psychological Science students, Gianmarco Altoè Ughetta Moscardino hospitality.find mistakes, suggestions improvement, can submit issue GitHub page open educational resource. work shared CC--NC-SA License. can cite resource :Lakens, D. (2022). Improving Statistical Inferences. Retrieved https://lakens.github.io/statistical_inferences/. https://doi.org/10.5281/zenodo.6409077This work dedicated Kyra, love life.Daniël LakensEindhoven University Technology","code":""},{"path":"pvalue.html","id":"pvalue","chapter":"1 Using p-values to test a hypothesis","heading":"1 Using p-values to test a hypothesis","text":"Scientists can attempt answer wide range questions collecting data. One question interests scientists whether measurements collected different conditions differ, . answer question ordinal claim, researcher states average measurements larger, smaller, , comparing conditions. example, researcher might interested hypothesis students learn better tests, need retrieve information learned (condition ), compared getting tests, spending time studying (condition B). collecting data, observing mean grade higher students spent part time tests, researcher can make ordinal claim student performance better condition compared condition B. Ordinal claims can used state difference conditions. quantify size effect.make ordinal claims, researchers typically rely methodological procedure known hypothesis test. One part hypothesis test consists computing p-value examining whether statistically significant difference. 'Significant' means something worthy attention. hypothesis test used distinguish signal (worth paying attention ) random noise empirical data. worth distinguishing statistical significance, used claim whether observed effect signal noise, practical significance, depends whether size effect large enough worthwhile consequences real life. Researchers use methodological procedure decide whether make ordinal claim safeguard confirmation bias. internal report Guinness brewery use statistical tests applied setting, William Gosset ('Student', developed t-test) already wrote (1904):hand, generally agreed leave rejection experiments entirely discretion experimenter dangerous, likely biassed. Hence proposed adopt criterion depending probability wide error occurring given number observations.Depending desires, scientists might tempted interpret data support hypothesis, even . hypothesis test, used correctly, controls amount time researchers fool make ordinal claims.","code":""},{"path":"pvalue.html","id":"philosophical-approaches-to-p-values","chapter":"1 Using p-values to test a hypothesis","heading":"1.1 Philosophical approaches to p-values","text":"look p-values computed, important examine supposed help us make ordinal claims testing hypotheses. definition p-value probability observing sample data, extreme data, assuming null hypothesis true. definition tell us much interpret p-value.interpretation p-value depends statistical philosophy one subscribes . Ronald Fisher published 'Statistical Methods Research Workers' 1925 popularized use p-values. Fisherian framework p-value interpreted descriptive continuous measure compatibility observed data null hypothesis (Greenland et al., 2016). compatibility observed data null model falls 1 (perfectly compatible) 0 (extremely incompatible), every individual can interpret p-value “statistical thoughtfulness\". According Fisher (1956), p-values \"generally lead probability statement real world, rational well-defined measure reluctance accept hypotheses test\". Fisher tried formalize philosophy approach called 'fiducial inference', received widespread adoption approaches, decision theory, likelihoods, Bayesian inference. Indeed, Zabell (1992) writes \"fiducial argument stands Fisher's one great failure\", although others expressed hope might developed useful approach future (Schweder & Hjort, 2016). Fisherian p-value describes incompatibility data single hypothesis, known significance testing. main reason significance test limited researchers specify null hypothesis (\\(H_0\\)), alternative hypothesis (\\(H_1\\)).Neyman Pearson built insights p-values William Gosset Ronald Fisher, developed approached called statistical hypothesis testing. main difference significance testing approach developed Fisher statistical hypothesis test null hypothesis alternative hypothesis specified. Neyman-Pearson framework, goal statistical tests guide behavior researchers respect two hypotheses. Based results statistical test, without ever knowing whether hypothesis true , researchers choose tentatively act null hypothesis alternative hypothesis true. psychology, researchers often use imperfect hybrid Fisherian Neyman-Pearson frameworks, Neyman-Pearson approach , according Dienes (2008) “logic underlying statistics see professional journals psychology”.Neyman-Pearson hypothesis test performed, observed p-value used check smaller chosen alpha level, matter much smaller . example, alpha level 0.01 used, p = 0.006 p = 0.000001 lead researchers decide act state world best described alternative hypothesis. differs Fisherian approach p-values, lower p-value, greater psychological reluctance researcher accept null hypothesis testing. Neyman-Pearson hypothesis test see goal inference quantifying continuous measure compatibility evidence. Instead, Neyman (1957) writes:content concept inductive behavior recognition purpose every piece serious research provide grounds selection one several contemplated courses action.Intuitively, one might feel decisions act based results single statistical test, point often raised criticism Neyman-Pearson approach statistical inferences. However, criticisms rarely use definition ‘act’ Neyman used. true , example, decision implement new government policy based single study result. However, Neyman considered making scientific claim ‘act’ well, wrote (1957, p. 10) concluding phase study involves:act decision take particular action, perhaps assume particular attitude towards various sets hypothesesCox (1958) writes:might argued making inference 'deciding' make statement certain type populations therefore, provided word decision interpreted narrowly, study statistical decisions embraces inference. point one main general problems statistical inference consists deciding types statement can usefully made exactly mean.Thus, Neyman-Pearson approach, p-values form basis decisions claims make. science, claims underly novel experiments form auxiliary hypotheses, assumptions underlying hypotheses assumed accurate order test work planned (Hempel, 1966). example, important participants can see color planned experiment, assume true Ishihara test successfully identifies participants colorblind.","code":""},{"path":"pvalue.html","id":"creating-a-null-model","chapter":"1 Using p-values to test a hypothesis","heading":"1.2 Creating a null model","text":"Assume ask two groups 10 people much liked extended directors cut Lord Rings (LOTR) trilogy. means total sample size (N) 20, sample size group (n) 10. first group consists friends, second groups consists friends wife. friends rate trilogy score 1 10. can calculate average rating friends, 8.7, average rating wife’s friends, 7.7. can compare scores groups looking raw data, plotting data.\nTable 1.1: Ratings Lord Rings extended trilogy two groups friends.\ncan see groups overlap mean ratings differ 1 whole point. question now faced following: difference two groups just random variation, can claim friends like extended directors cut Lord Rings (LOTR) trilogy wife’s friends?null hypothesis significance test try answer question calculating probability observed difference (case, mean difference 1) extreme difference, assumption real difference much friends wife’s friends like extended directors cut LOTR, just looking random noise. probability called p-value. probability low enough, decide claim difference. probability low enough, refrain making claim difference.null hypothesis assumes ask infinite number friends infinite number wife’s friends much like LOTR, difference huge groups exactly 0. However, sample drawn population, random variation likely lead difference somewhat larger smaller 0. can create null model quantifies expected variation observed data, just due random noise, tell us constitutes reasonable expectation much differences groups can vary difference population.practical create null model terms standardized distribution, makes easier calculate probability specific values occur, regardless scale used collect measurements. One version null model differences t-distribution, can used describe differences expected drawing samples population. null model built assumptions. case t-distribution, assumption scores normally distributed. reality, assumptions upon statistical methods built never met perfectly, statisticians examine impact violations assumptions methodological procedures. Statistical tests still useful practice impact violations statistical inferences small enough.can quantify distribution t-values expected difference population probability density function. plot probability density function t-distribution 18 degrees freedom (df), corresponds example collect data 20 friends (df = N - 2 two independent groups). continuous distribution, probabilities defined infinite number points, probability observing single point (e.g., t = 2.5) always zero. Probabilities measured intervals. reason, p-value computed, defined 'probability observing data', 'probability observing data, extreme data'. creates interval (tail distribution) probability can calculated.","code":""},{"path":"pvalue.html","id":"calculating-a-p-value","chapter":"1 Using p-values to test a hypothesis","heading":"1.3 Calculating a p-value","text":"t-value can computed mean sample, mean population, standard deviation sample, sample size. computing probability observing t-value extreme extreme one observed, get p-value. comparison movie ratings two groups friends , performing two-sided Student's t-test yields t-value 2.5175 p-value 0.02151.can graph t-distribution (df = 18) highlight two tail areas start t-values 2.5175 -2.5175.\nFigure 1.1: t-distribution 18 degrees freedom.\n","code":"\nt.test(df_long$rating ~ df_long$`Friend Group`, var.equal = TRUE)## \n##  Two Sample t-test\n## \n## data:  df_long$rating by df_long$`Friend Group`\n## t = 2.5175, df = 18, p-value = 0.02151\n## alternative hypothesis: true difference in means between group Friends Daniel and group Friends Kyra is not equal to 0\n## 95 percent confidence interval:\n##  0.1654875 1.8345125\n## sample estimates:\n## mean in group Friends Daniel   mean in group Friends Kyra \n##                          8.7                          7.7"},{"path":"pvalue.html","id":"whichpexpect","chapter":"1 Using p-values to test a hypothesis","heading":"1.4 Which p-values can you expect?","text":"educational video 'Dance p-values', Geoff Cumming explains p-values vary experiment experiment. However, reason 'trust p' mentions video. Instead, important clearly understand p-value distributions prevent misconceptions. p-values part frequentist statistics, need examine can expect long run. never experiment hundreds times, limited number studies lifetime, best way learn expect long run computer simulations.Take moment try answer following two questions. p-values can expect observe true effect, repeat study one-hundred thousand times? p-values can expect true effect, repeat study one-hundred thousand times? know answer, worry - learn now. know answer, worth reflecting know answer essential aspect p-values. like , simply never taught . see, essential solid understanding interpret p-values.p-values can expect completely determined statistical power study, probability observe significant effect, true effect. statistical power ranges 0 1. can illustrate simulating independent t-tests. idea simulate IQ scores group people. know standard deviation IQ scores 15. now, set mean IQ score one simulated group 100, simulated group 105. testing people one group IQ differs group (know correct answer ‘yes’, made simulation).simulation, generate n = 71 normally distributed IQ scores means M (100 105 default) standard deviation 15. perform independent t-test, store p-value, generate plot p-value distribution.\nFigure 1.2: Distribution p-values power = 50%.\nx-axis see p-values 0 1 20 bars, y-axis see frequently p-values observed. horizontal red dotted line indicates alpha 5% (located frequency 100.000*0.05 = 5000) – can ignore line now. title graph, statistical power achieved simulated studies given (assuming alpha 0.05): studies 50% power.simulation result illustrates probability density function p-values. probability density function provides probability random variable specific value (Figure 1.1 t-distribution). p-value random variable, can use probability density function plot p-value distribution (Hung et al., 1997; Ulrich & Miller, 2018), Figure 1.3. online Shiny app can vary sample size, effect size, alpha level examine effect p-value distribution. Increasing sample size effect size increase steepness p-value distribution, means probability observe small p-values increases. p-value distribution function statistical power test.\nFigure 1.3: Probability density function p-values two-sided t-test.\ntrue effect, p-values uniformly distributed. means every p-value equally likely observed null hypothesis true. words, true effect, p-value 0.08 just likely p-value 0.98. remember thinking counterintuitive first learned uniform p-value distributions (well completing PhD). makes sense p-values unifromly distributed think goal guarantee \\(H_0\\) true, alpha % p-values fall alpha level. set alpha 0.01, 1% observed p-values fall 0.01, set alpha 0.12, 12% observed p-values fall 0.12. can happen p-values uniformly distributed null hypothesis true.\nFigure 1.4: Distribution p-values null hypothesis true.\n","code":"\np <- numeric(100000) # store all simulated *p*-values\n\nfor (i in 1:100000) { # for each simulated experiment\n  x <- rnorm(n = 71, mean = 100, sd = 15) # Simulate data\n  y <- rnorm(n = 71, mean = 105, sd = 15) # Simulate data\n  p[i] <- t.test(x, y)$p.value # store the *p*-value\n}\n\n(sum(p < 0.05) / 100000) # compute power\n\nhist(p, breaks = 20) # plot a histogram"},{"path":"pvalue.html","id":"lindley","chapter":"1 Using p-values to test a hypothesis","heading":"1.5 Lindley's paradox","text":"statistical power increases, p-values 0.05 (e.g., p = 0.04) can likely effect effect. known Lindley's paradox (Lindley, 1957), sometimes Jeffreys-Lindley paradox (Spanos, 2013). distribution p-values function statistical power (Cumming, 2008), higher power, right-skewed p-value distribution becomes (.e., likely becomes small p-values observed). true effect, p-values uniformly distributed, 1% observed p-values fall 0.04 0.05. statistical power extremely high, p-values fall 0.05, p-values fall 0.01. Figure 1.5 see high power small p-values (e.g., 0.001) likely observed effect effect (e.g., dotted black curve representing 99% power falls grey horizontal line representing uniform distribution null true p-value 0.01).Yet perhaps surprisingly, observing p-value 0.04 likely null hypothesis (\\(H_0\\)) true alternative hypothesis (\\(H_1\\)) true high power, illustrated fact Figure 1.5 density p-value distribution higher null true, test 99% power, 0.04. Lindley's paradox shows p-value example 0.04 can statistically significant, time provides evidence null hypothesis. Neyman-Pearson approach made claim maximum error rate 5%, likelihood Bayesian approach, conclude data provides evidence favor null hypothesis, relative alternative hypothesis. Lindley's paradox illustrates different statistical philosophies reach different conclusions, p-value directly interpreted measure evidence, without taking power test account. Although necessary, researchers might desire prevent situations frequentist rejects null hypothesis based p < 0.05, evidence test favors null hypothesis alternative hypothesis. can achieved lowering alpha level function sample size (Good, 1992; Leamer, 1978; Maier & Lakens, 2022), explained chapter error control.\nFigure 1.5: P-value distribution 0 (grey horizontal line, 50 percent power (black solid curve), 99 percent power (black dotted curve, p-values just 0.05 likely \\(H_0\\) true \\(H_1\\) true).\n","code":""},{"path":"pvalue.html","id":"correctly-reporting-and-interpreting-p-values","chapter":"1 Using p-values to test a hypothesis","heading":"1.6 Correctly reporting and interpreting p-values","text":"Although strict Neyman-Pearson perspective sufficient report p < \\(\\alpha\\) p > \\(\\alpha\\), researchers report exact p-values. facilitates re-use results secondary analyses (Appelbaum et al., 2018), allows researchers compare p-value alpha level preferred use (Lehmann & Romano, 2005). claims made using methodological procedure known maximum error rates, p-value never allows state anything certainty. Even set alpha level 0.000001 single claim can error, Fisher (1935) reminds us, '“one chance million” undoubtedly occur, less appropriate frequency, however surprised may occur us”. uncertainty sometimes reflected academic writing, researchers can seen using words 'prove', 'show', 'known'. slightly longer accurate statement hypothesis test read:claim /meaningful effect, acknowledging scientists make claims using methodological procedure, misled, long run, alpha % beta % time, deem acceptable. foreseeable future, new data information emerges proves us wrong, assume claim correct.Remember Neyman-Pearson framework researchers make claims, necessarily believe truth claims. example, OPERA collaboration reported 2011 observed data seemed suggest neutrinos traveled faster speed light. claim made 0.2---million Type 1 error rate, assuming error purely due random noise. However, none researchers actually believed claim true, theoretically impossible neutrinos move faster speed light. Indeed, later confirmed equipment failures cause anomalous data: fiber optic cable attached improperly, clock oscillator ticking fast. Nevertheless, claim made explicit invitation scientific community provide new data information prove claim wrong.researchers “accept” “reject” hypothesis Neyman-Pearson approach statistical inferences, communicate belief conclusion substantive hypothesis. Instead, utter Popperian basic statement based prespecified decision rule observed data reflect certain state world. Basic statements describe observation made (e.g., \"observed black swan\") event occurred (e.g., \"students performed better exam trained spaced practice, \").claim data observed, theory used make predictions. claim observed data, statistical inference, theory, requires theoretical inference. Data never 'proves' theory true false. basic statement can corroborate prediction derived theory, . many predictions deduced theory corroborated, can become increasingly convinced theory close truth. 'truth-likeness' theories called verisimilitude (Niiniluoto, 1998; Popper, 2002). shorter statement hypothesis test presented therefore read 'p = .xx, corroborates prediction, alpha level y%', 'p = .xx, corroborate prediction, statistical power y% effect size interest'. Often, alpha level statistical power mentioned experimental design section article, repeating results section might remind readers error rates associated claims.Even made correct claims, underlying theory can false. Popper (2002) reminds us “empirical basis objective science thus nothing ‘absolute’ basis ”. argues science built solid bedrock, piles driven swamp notes “simply stop satisfied piles firm enough carry structure, least time .” Hacking (1965) writes: “Rejection refutation. Plenty rejections must tentative.” reject null model, tentatively, aware fact might done error, without necessarily believing null model false, without believing theory used make predictions true. Neyman (1957) inferential behavior : “act behave future (perhaps new experiments performed) particular manner, conforming outcome experiment”. knowledge science provisional.statisticians recommend interpreting p-values measures evidence. example, Bland (2015) teaches p-values can interpreted 'rough ready' guide strength evidence, p > 0.1 indicates 'little evidence', 0.01 < p < 0.05 indicates 'evidence', p < 0.001 'strong evidence'. incorrect (Johansson, 2011; Lakens, 2022a), clear previous discussions Lindley's paradox uniform p-value distributions. want quantify evidence, see chapters likelihoods Bayesian statistics.","code":""},{"path":"pvalue.html","id":"misconceptions","chapter":"1 Using p-values to test a hypothesis","heading":"1.7 Preventing common misconceptions about p-values","text":"p-value probability observed data, extreme data, assumption null hypothesis true. understand means, might especially useful know doesn’t mean. First, need know ‘assumption null hypothesis true’ looks like, data expect null hypothesis true. Although null hypothesis can value, assignment assume null hypothesis specified mean difference 0. example, might interested calculating difference control condition experimental condition dependent variable.useful distinguish null hypothesis (prediction mean difference population exactly 0) null model (model data expect collect data null hypothesis true). null hypothesis point 0, null model distribution. visualized textbooks power analysis software using pictures can see , t-values horizontal axis, critical t-value somewhere 1.96 – 2.00 (depending sample size). done statistical test comparing two groups based t-distribution, p-value statistically significant t-value larger critical t-value.personally find things become lot clearer plot null model mean differences instead t-values. , can see null model mean differences can expect comparing two groups 50 observations true difference two groups 0, standard deviation group 1. standard deviation 1, can also interpret mean differences Cohen’s d effect size. also distribution can expect Cohen's d 0, collecting 50 observations per group independent t-test.\nFigure 1.6: Distribution observed Cohen's d effect sizes collecting 50 observations per group independent t-test.\nfirst thing notice expect mean null model 0. Looking x-axis, see plotted distribution centered 0. even mean difference population 0 imply every sample draw population give mean difference exactly zero. variation around population value, function standard deviation sample size.y-axis graph represents density, provides indication relative likelihood measuring particular value continuous distribution. can see likely mean difference true population value zero, larger differences zero become increasingly less likely. graph two areas colored red. areas represent 2.5% extreme values left tail distribution, 2.5% extreme values right tail distribution. Together, make 5% extreme mean differences expect observe, given number observations, true mean difference exactly 0. mean difference red area observed, corresponding statistical test statistically significant 5% alpha level. words, 5% observed mean differences far enough away 0 considered surprising. null hypothesis true, observing ‘surprising’ mean difference red areas Type 1 error.Let’s assume null model Figure true, observe mean difference 0.5 two groups. observed difference falls red area right tail distribution. means observed mean difference relatively surprising, assumption true mean difference 0. true mean difference 0, probability density functions shows expect mean difference 0.5 often. calculate p-value observation, lower 5%. probability observing mean difference least far away 0 0.5 (either left mean, right, two-tailed test) less 5%.One reason prefer plot null model raw scores instead t-values can see null model changes sample size increases. collect 5000 instead 50 observations, see null model still centered 0 – null model now expect values fall close around 0.\nFigure 1.7: Distribution observed Cohen's d effect sizes collecting 5000 observations per group independent t-test d = 0.\ndistribution much narrower distribution mean differences based standard error difference means. value calculated based standard deviation sample size, follows:\\[\\sqrt{\\frac{\\sigma_{1}^{2}}{n_{1}}+\\frac{\\sigma_{2}^{2}}{n_{2}}}\\]formula shows standard deviations group (σ) squared divided sample size group, added together, square root taken. larger sample size bigger number divide , thus smaller standard error difference means. n = 50 example :\\[\\sqrt{\\frac{1^{2}}{50}+\\frac{1^{2}}{50}}\\]standard error differences means thus 0.2 n = 50 group, n = 5000 0.02. Assuming normal distribution, 95% observations fall 1.96 SE. 50 samples per group, mean differences fall -1.96 * 0.2 = -0.392, +1.96 * 0.2 = 0.392, can see red areas start approximately -0.392 0.392 n = 50. 5000 samples per group, mean differences fall -1.96 * 0.02, +1.96 * 0.02; words -0.0392 0.0392 n = 5000. Due larger sample size n = 5000 observations per group, expect observe mean differences sample closer 0 compared null model 50 observations.collected n = 5000, observe mean difference 0.5, clear difference even surprising collected 50 observations. now almost ready address common misconceptions p-values, can , need introduce model data null true. sampling data model true mean difference 0, alternative model look like? software (G*power, see Figure 1.8) visualize null model (red curve) alternative model (blue curve) output:\nFigure 1.8: Screenshot G*Power software visualizing null model (red distribution) alternative model (blue distribution) critical t-value (1.66055) threshold distinguishing significant non-significant results.\nstudy, rarely already know true mean difference (already knew, study?). let’s assume -knowing entity. Following Paul Meehl, call -knowing entity ‘Omniscient Jones’. collect sample 50 observations, Omniscient Jones already knows true mean difference population 0.5. , expect variation around 0.5 alternative model. figure shows expected data pattern null hypothesis true (now indicated grey line) shows alternative model, assuming true mean difference 0.5 exists population (indicated black line).\nFigure 1.9: Distribution observed Cohen's d effect sizes collecting 50 observations per group independent t-test d = 0.\nOmniscient Jones said true difference much larger. Let’s assume another study, now collect 50 observations, Omniscient Jones tells us true mean difference 1.5. null model change, alternative model now moves right.can play around alternative null models online app: http://shiny.ieis.tue.nl/d_p_power/. app allows specify sample size group independent t-test (2 infinity), mean difference (0 2), alpha level. plot, red areas visualize Type 1 errors. blue area visualizes Type 2 error rate (discuss ). app also tells critical value: vertical line (n = 50 line falls mean difference 0.4) sentence says: “Effects larger 0.4 statistically significant”. Note true effects smaller -0.4, even though second label , app shows situation two-sided independent t-test.can see left vertical line indicates critical mean difference blue area part alternative model. Type 2 error rate (1 - power study). study 80% power, 80% mean differences observe fall right critical value indicated line. alternative model true, observe effect smaller critical value, observed p-value larger 0.05, even true effect. can check app larger sample size, right entire alternative distribution falls, thus higher power. can also see larger sample size, narrower distribution, less distribution fall critical value (long true population mean larger critical value). Finally, larger alpha level, left critical mean difference moves, smaller area alternative distribution falls critical value.app also plots 3 graphs illustrate power curves function different alpha levels, sample sizes, true mean differences. Play around app changing values. Get feel variable impacts null alternative models, mean difference statistically significant, Type 1 Type 2 error rates.far, several aspects null models become clear. First , population value traditional null hypothesis value 0, sample draw, observed difference falls distribution centered 0, thus often slightly larger smaller 0. Second, width distribution depends sample size standard deviation. larger sample size study, narrower distribution around 0. Finally, mean difference observed falls tails null model, can considered surprising. away null value, surprising result . null model true, surprising values happen probability specified alpha level (called Type 1 errors). Remember Type 1 error occurs researcher concludes difference population, true mean difference population zero.now finally ready address common misconceptions p-values. Let’s go list common misconceptions reported scientific literature. examples might sound like semantics. easy first glance think statement communicates right idea, even written version formally correct. However, statement formally correct, wrong. exactly people often misunderstand p-values, worth formally correct interpreted.","code":""},{"path":"pvalue.html","id":"misconception1","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.1 Misunderstanding 1: A non-significant p-value means that the null hypothesis is true.","text":"common version misconception reading sentence ‘p > 0.05 can conclude effect’. Another version sentence ‘difference, (p > 0.05)’.look misconception detail, want remind one fact easy remember, enable recognize many misconceptions p-values: p-values statement probability data, statement probability hypothesis probability theory. Whenever see p-values interpreted probability theory hypothesis, know something right. Examples statements hypothesis ‘null hypothesis true’, ‘alternative hypothesis true’, statements say probability null alternative model true 100%. subtler version statement ‘observed difference due chance’. observed difference ‘due chance’ (instead due presence real difference) null hypothesis true, , statement implies 100% probable null hypothesis true.conclude ‘effect’ ‘difference’ similarly claiming 100% probable null hypothesis true. since p-values statements probability data, refrain making statements probability theory solely based p-value. ’s ok. p-values designed help identify surprising results noisy data generation process (aka real world). designed quantify probability hypothesis true.Let’s take concrete example illustrate non-significant result mean null hypothesis true. figure , Omniscient Jones tells us true mean difference 0.5. can see , alternative distribution visualizes probability mean differences expect alternative hypothesis true centered 0.5. observed mean difference 0.35. value extreme enough statistically different 0. can see , value fall within red area null model (hence, p-value smaller alpha level).Nevertheless, see observing mean difference 0.35 quite likely given true mean difference 0.5, observing mean difference 0.35 much likely alternative model null model. can see comparing height density curve difference 0.35 null model, approximately 0.5, height density curve alternative model, approximately 1.5. See chapter likelihoods details.\nFigure 1.10: Distribution observed Cohen's d effect sizes collecting 50 observations per group independent t-test d = 0 d = 0.5 observing d = 0.35.\np-value tells us mean difference 0.35 extremely surprising, assume null hypothesis true. can many reasons . real world, Omniscient Jones tell us true mean difference, possible true effect, illustrated figure .say instead? solution subtle, important. Let’s revisit two examples incorrect statements made earlier. First, ‘p > 0.05 can conclude effect’ incorrect, might well effect (remember p-values statements data, probability effect effect). Fisher’s interpretation p-value can conclude rare event happened, null hypothesis false (writes literally: “Either exceptionally rare chance occurred, theory random distribution true”). might sound like statement probability theory, really just stating two possible scenarios low p-values occur (made Type 1 error, alternative hypothesis true). true positive false positive remain possible, quantify probability either possible reality (e.g., saying 95% probable null hypothesis false). Neyman-Pearson perspective, p > .05 means act null hypothesis can rejected, without maintaining desired error rate 5%.interested concluding effect absent, null hypothesis testing tool use. null hypothesis test answers question ‘can reject null hypothesis desired error rate?’. , p > 0.05, conclusion can drawn based p-value (remember concept 無 ‘mu’: answer neither yes ). Luckily, statistical approaches developed ask questions absence effect equivalence testing, Bayes factors, Bayesian estimation (see Harms & Lakens (2018), overview).second incorrect statement ‘difference’. statement somewhat easier correct. can instead write ‘statistically significant difference’. Granted, bit tautological, basically saying p-value larger alpha level two different ways, least statement formally correct. difference ‘difference’ ‘statistically significant difference’ might sound like semantics, first case formally saying ‘difference 0’ second saying ‘difference large enough yield p < .05’. Although never seen anyone , informative message might ‘given sample size 50 per group, alpha level 0.05, observed differences extreme 0.4 statistically significant, observed mean difference 0.35 thus reject null hypothesis’. feels like unsatisfactory conclusion, remember null hypothesis test designed draw interesting conclusions absence effects – need learn equivalence tests get satisfactory answers null effects.","code":""},{"path":"pvalue.html","id":"misunderstanding-2-a-significant-p-value-means-that-the-null-hypothesis-is-false.","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.2 Misunderstanding 2: A significant p-value means that the null hypothesis is false.","text":"opposite misconception one discussed previously. Examples incorrect statements based misconception ‘p < .05, therefore effect’, ‘difference two groups, p < .05’. , statements imply 100% probable null model false, alternative model true.simple example extreme statements incorrect, imagine generate series numbers R using following command:command generates 50 random observations distribution mean 0 standard deviation 1 (long run – mean standard deviation vary sample generated). Imagine run command , observe mean 0.5. figure visualizes scenario. can perform one-sample t-test 0, test tells us, p < .05, data observed surprisingly different 0, assuming random number generator R functions generates data true mean 0.\nFigure 1.11: Distribution observed Cohen's d effect sizes collecting 50 observations per group independent t-test d = 0 observing d = 0.5.\nsignificant p-value allow us conclude null hypothesis (“random number generator works”) false. true mean 50 samples generated surprisingly extreme. low p-value simply tells us observation surprising. observe surprising observations low probability null hypothesis true – null true, still happen. Therefore, significant result mean alternative hypothesis true – result can also Type 1 error, example , Omniscient Jones knows case.Let’s revisit incorrect statement ‘p < .05, therefore effect’. correct interpretation significant p-value requires us acknowledge possibility significant result might Type 1 error. Remember Fisher conclude “Either exceptionally rare chance occurred, theory random distribution true”. correct interpretation terms Neyman-Pearson statistics : “can act null hypothesis false, wrong 5% time long run”. Note specific use word ‘act’, imply anything whether specific hypothesis true false, merely states act null hypothesis false time observe p < alpha, make error alpha percent time.formally correct statements bit long. scientific articles, often read shorter statement : ‘can reject null hypothesis’, ‘can accept alternative hypothesis’. statements might made assumption readers add ‘5% probability wrong, long run’. might useful add ‘5% long run error rate’ least first time make statement article remind readers.example strong subjective prior probability random number generator R works. Alternative statistical procedures incorporate prior beliefs Bayesian statistics false positive report probabilities. frequentist statistics, idea need replicate study several times. observe Type 1 error every now , unlikely observe Type 1 error three times row. Alternatively, can lower alpha level single study reduce probability Type 1 error rate.","code":"\nrnorm(n = 50, mean = 0, sd = 1)##  [1]  0.368269723 -0.156377695  0.616474939  0.891580065  2.129257097\n##  [6]  1.452765820 -0.371848028 -0.716311802 -0.572340194  0.220586938\n## [11] -0.237445908  0.021607492 -0.203797691 -1.038001440  1.712214976\n## [16] -0.779853693  1.980755098  0.003352426 -0.574994319  3.016517492\n## [21] -1.372467170 -1.233813944  0.106312219 -0.787739757  0.532553411\n## [26]  1.806532997 -0.537994340 -0.598688811  0.077920524  0.239845314\n## [31]  0.793207078  0.040327198  0.628850579 -1.084505446 -1.045830223\n## [36] -3.026489777 -0.712574809 -0.420644550  0.495136283 -0.044453037\n## [41]  0.301620724  0.130323689 -0.051586584 -1.399402743 -0.629285186\n## [46]  1.348365737 -1.635393955 -0.429652494 -1.197913687 -0.883861927"},{"path":"pvalue.html","id":"misunderstanding-3-a-significant-p-value-means-that-a-practically-important-effect-has-been-discovered.","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.3 Misunderstanding 3: A significant p-value means that a practically important effect has been discovered.","text":"common concern interpreting p-values ‘significant’ normal language implies ‘important’, thus ‘significant’ effect interpreted ‘important’ effect. However, question whether effect important completely orthogonal question whether different zero, even large effect . effects practical impact. smaller effect, less likely effects noticed individuals, effects might still large impact societal level. Therefore, general take home message statistical significance answer question whether effect matters practice, ‘practically important’. answer question whether effect matters, need present cost-benefit analysis.issue practical significance often comes studies large sample size. seen , increasing sample size, width density distribution around null value becomes narrow, values considered surprising fall closer closer zero.plot null model large sample size (e.g., n = 10000 per group) see even small mean differences (differences extreme mean difference 0.04) considered ‘surprising’. still means really difference population, observe differences larger 0.04 less 5% time, long run, 95% observed differences smaller mean difference 0.04. becomes difficult argue practical significance effects. Imagine specific intervention successful changing people’s spending behavior, implementing intervention people save 12 cents per year. difficult argue effect make individual happier. However, money combined, yield 2 million, used treat diseases developing countries, real impact. cost intervention might considered high goal make individuals happier, might consider worthwhile goal raise 2 million charity.effects psychology additive (combine transfer increase happiness 0.04 scale points), often difficult argue importance small effects subjective feelings (Anvari et al., 2021). cost-benefit analysis might show small effects matter lot, whether case inferred p-value.Note nothing problem interpretation p-value per se: p < 0.05 still correctly indicates , null hypothesis true, observed data considered surprising. However, just data surprising, mean need care . mainly verbal label ‘significant’ causes confusion – perhaps less confusing think ‘significant’ effect ‘surprising’ effect, necessarily ‘important’ effect.","code":""},{"path":"pvalue.html","id":"misconception4","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.4 Misunderstanding 4: If you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%.","text":"misinterpretation one possible explanation incorrect statement p-value ‘probability data observed chance.’ Assume collect 20 observations, Omniscient Jones tells us null hypothesis true (example generated random numbers R). means sampling distribution figure .\nFigure 1.12: Distribution observed Cohen's d effect sizes collecting 20 observations per group independent t-test d = 0.\nreality, means 100% time observe significant result, false positive (Type error). Thus, 100% significant results Type 1 errors.important distinguish probabilities collecting data analyzing result, probabilities collecting data analyzing results. Type 1 error rate controls studies perform future null hypothesis true, 5% observed mean differences fall red tail areas. seen data falls tail areas p < alpha, know null hypothesis true, observed significant effects always Type 1 error. read carefully, notice misunderstanding cause differences question asked. \"observed p < .05, probability null hypothesis true?\" different question \"null hypothesis true, probability observing (extreme) data?”. latter question answered p-value. first question answered without making subjective judgment probability null hypothesis true prior collecting data.","code":""},{"path":"pvalue.html","id":"misunderstanding-5-one-minus-the-p-value-is-the-probability-that-the-effect-will-replicate-when-repeated.","chapter":"1 Using p-values to test a hypothesis","heading":"1.7.5 Misunderstanding 5: One minus the p-value is the probability that the effect will replicate when repeated.","text":"impossible calculate probability effect replicate (Miller, 2009). many unknown factors accurately predict replication probability, one main factors true mean difference. Omniscient Jones, knew true mean difference (e.g., difference two groups 0.5 scale points) know statistical power test. statistical power probability find significant result, alternative model true (.e. true effect). example, reading text left bar app, see N = 50 per group, alpha level 0.05, true mean difference 0.5, probability finding significant result (statistical power) 69.69%. observe significant effect scenario (e.g., p = 0.03) true 97% probability exact replication study (sample size) yield significant effect. probability study yields significant effect determined statistical power - p-value previous study.can generally take away last misunderstanding fact probability replication depends presence versus absence true effect. words, stated , true effect exists level statistical power informs us frequently observe significant result (e.g., 80% power means observe significant result 80% time). hand, null hypothesis true (e.g., effect 0) significant results observed frequency approaching chosen alpha level long run (.e. 5% Type 1 error rate alpha 0.05 chosen). Therefore, original study correctly observed effect, probability significant result replication study determined statistical power, original study correctly observed significant effect, probability significant effect replication study determined alpha level. practice, many factors determine effect replicate. way know effect replicate, replicate . want explore difficult predict findings literature replicate can perform test 80.000 Hours.","code":""},{"path":"pvalue.html","id":"test-yourself","chapter":"1 Using p-values to test a hypothesis","heading":"1.8 Test Yourself","text":"","code":""},{"path":"pvalue.html","id":"questions-about-which-p-values-you-can-expect","chapter":"1 Using p-values to test a hypothesis","heading":"1.8.1 Questions about which p-values you can expect","text":"Copy code R run code. can click 'clipboard' icon top right code section copy code clipboard, can easily paste R.x-axis see p-values 0 1 20 bars, y-axis see frequently p-values observed. horizontal red dotted line indicates alpha 5% (located frequency 100.000*0.05 = 5000) – can ignore line now. title graph, statistical power achieved simulated studies given (assuming alpha 0.05): studies 50% power (minor variations simulation).Q1: Since statistical power probability observing statistically significant result, true effect, can also see power figure . ?can calculate number p-values larger 0.5, divide number simulations.can calculate number p-values first bar (contains ‘significant’ p-values 0.00 0.05) divide p-values bar total number simulations.can calculate difference p-values 0.5 minus p-values 0.5, divide number total number simulations.can calculate difference p-values 0.5 minus p-values 0.05, divide number number simulations.Q2: Change sample size n <- 26 n <- 51. Run simulation selecting lines pressing CTRL+Enter. power simulation now increased sample size 26 people 51 people?55%60%80%95%Q3: look distribution p-values, notice?p-value distribution exactly 50% powerThe p-value distribution much steeper 50% powerThe p-value distribution much flatter 50% powerThe p-value distribution much normally distributed 50% powerFeel free increase decrease sample size see happens run simulation. done exploring, make sure n <- 51 .Q4: happen true difference simulated samples average IQ score? situation, probability observe effect, might say ‘0 power’. Formally, power defined true effect. However, can casually refer 0 power. Change mean sample 100 (set m <- 106 m <- 100) now difference mean sample, population value testing one-sample t-test. Run script . notice?p-value distribution exactly 50% powerThe p-value distribution much steeper 50% powerThe p-value distribution basically completely flat (ignoring minor variation due random noise simulation)p-value distribution normally distributedThe question builds simulation true difference groups.Q5: Look leftmost bar plot produced Q4, look frequency p-values bar. formal name bar?power (true positives)true negativesThe Type 1 error (false positives)Type 2 error (false negatives)Let’s take look just p-values 0.05. Bear next steps – worth . Find variable determines many bars , statement bars <- 20. Change bars <- 100. now get 1 bar p-values 0 0.01, one bar p-values 0.01 0.02, 100 bars total. red dotted line now indicate frequency p-values null hypothesis true, every bar contains 1% total number p-values. want look p-values 0.05, cut plot 0.05. Change xlim = c(0, 1) xlim = c(0, 0.05). Instead seeing p-values 0 1, see p-values 0 0.05. Re-run simulation (still m <- 100). see uniform distribution, now every bar contains 1% p-values, p-value distribution flat almost impossible see (zoom y-axis later assignment). red line now clearly gives frequency bar, assuming null hypothesis true.Change mean simulation line 9 m <- 107 (remember n still 51). Re-run simulation. ’s clear high power. p-values left-bar, contains p-values 0.00 0.01.Q6: plot last simulation tells 90.5% power. power use alpha 5%. can also use alpha 1%. statistical power simulated studies use alpha 1%, looking graph? Pick answer closest answer simulations.~90%~75%~50%~5%able look p-values around 0.03 0.04, zoom y-axis well. part code plot draw, change ylim = c(0, nSims) ylim = c(0, 10000). Re-run script.Change mean sample 108 m <- 108), leave sample size 51. Run simulation. Look distribution changed compared graph .Look fifth bar left. bar now contains p-values 0.04 0.05. notice something peculiar. Remember red dotted line indicates frequency bar, assuming null hypothesis true. See bar p-values 0.04 0.05 lower red line. simulated studies 96% power. power high, p-values 0.04 0.05 rare – occur less 1% time (p-values smaller 0.01). null hypothesis true, p-values 0.04 0.05 occur exactly 1% time (p-values uniformly distributed). Now ask : high power, observe p-value 0.04 0.05, likely null hypothesis true, alternative hypothesis true? Given likely observe p-values 0.04 0.05 null hypothesis true, alternative hypothesis true, interpret p-value significant alpha 0.05 likely null hypothesis true, alternative hypothesis true.simulations, know true effect , real world, don’t know. high power, use alpha level 0.05, find p-value p = .045, data surprising, assuming null hypothesis true, even surprising, assuming alternative hypothesis true. shows significant p-value always evidence alternative hypothesis.Q7: know high (e.g., 98%) power smallest effect size care , observe p-value 0.045, correct conclusion?effect significant, provides strong support alternative hypothesis.effect significant, without doubt Type 1 error.high power, use alpha level smaller 0.05, therefore, effect considered significant.effect significant, data likely null hypothesis alternative hypothesis.Q8: Play around sample size (n) mean (m) changing numerical values (thus, vary statistical power simulated studies). Look simulation result bar contains p-values 0.04 0.05. red line indicates many p-values found bar null hypothesis true (always 1%). best, much likely p-value 0.04 0.05 come p-value distribution representing true effect, come p-value distribution effect? can answer question seeing much higher bar p-values 0.04 0.05 can become. best bar simulation five times high red line (bar shows 5% p-values end 0.04 0.05, red line remains 1%), best p-values 0.04 0.05 five times likely true effect true effect.best, p-values 0.04 0.05 equally likely \nalternative hypothesis, null hypothesis.best, p-values 0.04 0.05 approximately 4 times \nlikely alternative hypothesis, null hypothesis.best, p-values 0.04 0.05 ~10 times likely alternative hypothesis, null hypothesis.best, p-values 0.04 0.05 ~30 times likely alternative hypothesis, null hypothesis.reason, statisticians warn p-values just 0.05 (e.g.,\n0.04 0.05) best weak support alternative\nhypothesis. find p-values range, consider replicating \nstudy, ’s possible, interpret result least bit\ncautiously.","code":"\nnsims <- 100000 # number of simulations\n\nm <- 106 # mean sample\nn <- 26 # set sample size\nsd <- 15 # SD of the simulated data\n\np <- numeric(nsims) # set up empty vector\nbars <- 20\n\nfor (i in 1:nsims) { # for each simulated experiment\n  x <- rnorm(n = n, mean = m, sd = sd)\n  z <- t.test(x, mu = 100) # perform the t-test\n  p[i] <- z$p.value # get the p-value\n}\npower <- round((sum(p < 0.05) / nsims), 2) # power\n\n# Plot figure\nhist(p,\n  breaks = bars, xlab = \"P-values\", ylab = \"number of p-values\\n\", \n  axes = FALSE, main = paste(\"P-value Distribution with\", \n                             round(power * 100, digits = 1), \"% Power\"),\n  col = \"grey\", xlim = c(0, 1), ylim = c(0, nsims))\naxis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1))\naxis(side = 2, at = seq(0, nsims, nsims / 4), \n     labels = seq(0, nsims, nsims / 4), las = 2)\nabline(h = nsims / bars, col = \"red\", lty = 3)"},{"path":"pvalue.html","id":"questions-about-p-value-misconceptions","chapter":"1 Using p-values to test a hypothesis","heading":"1.8.2 Questions about p-value misconceptions","text":"Q1: sample size group independent t-test 50\nobservations (see Figure 1.6), statement correct?mean differences observe two groups always 0.mean differences observe two groups always different 0.Observing mean difference +0.5 -0.5 considered surprising, assuming null hypothesis true.Observing mean difference +0.1 -0.1 considered surprising, assuming null hypothesis true.Q2: sense null models figures (Figure 1.6 1.7) similar, sense different?cases, distributions centered zero, critical\nt-value 1.96 2 (two-sided test, depending sample size). larger sample size, closer 0 mean differences fall considered ‘surprising’.cases, t-value 0 likely outcome, critical t-value around 0.4 n = 50, around 0.05 n = 5000.cases, means vary exactly way around 0, Type 1 error rate much smaller n = 5000 n = 50.standard error much larger n = 50 n = 5000, much likely null hypothesis true n = 50.Q3: can play around alternative null models online app: http://shiny.ieis.tue.nl/d_p_power/. app allows specify sample size group independent t-test (2 infinity), mean difference (0 2), alpha level. plot, red areas visualize Type 1 errors. blue area visualizes Type 2 error rate. app also tells critical value: vertical line (n = 50 line falls mean difference 0.4) verbal label says: “Effects larger 0.4 statistically significant”. Note true effects smaller -0.4, even though second label , app shows situation two-sided independent t-test.can see left vertical line indicates critical mean difference blue area part alternative model. Type 2 error rate (1 - power study). study 80% power, 80% mean differences observe fall right critical value indicated line. alternative model true, observe effect smaller critical value, observed p-value larger 0.05, even true effect. can check app larger effect size, right entire alternative distribution falls, thus higher power. can also see larger sample size, narrower distribution, less distribution fall critical value (long true population mean larger critical value). Finally, larger alpha level, left critical mean difference moves, smaller area alternative distribution falls critical value.app also plots 3 graphs illustrate power curves function different alpha levels, sample sizes, true mean differences. Play around app changing values. Get feel variable impacts null alternative models, mean difference statistically significant, Type 1 Type 2 error rates.Open app, make sure set default settings sample size 50 alpha level 0.05. Look distribution null model. Set sample size 2. Set sample size 5000. app allow plot data ‘group’ size 1, n = 2 get pretty good idea range values can expect true effect 0, collect single observations (n = 1). Given experiences app change different parameters, statement true?null hypothesis true standard deviation 1, randomly take 1 observation group calculate difference score, differences fall -0.4 0.4 95% pairs observations draw.null hypothesis true standard deviation 1, n = 50 per group, 95% studies data collected observe long run mean difference -0.4 0.4.study n = 50 per group, even SD unknown known null hypothesis true, rarely observe mean difference extreme -0.4 0.4.sample size increases, expected distribution means become narrower null model, alternative model.Q4: Open app default settings. Set slider alpha level 0.01 (keeping mean difference 0.5 sample size 50). Compared critical value alpha = 0.05, statement true?Compared alpha 0.05, less extreme values considered surprising alpha 0.01 used, differences larger 0.53 scale points (smaller -0.53) now statistically significant.Compared alpha 0.05, less extreme values considered surprising alpha 0.01 used, differences larger 0.33 scale points (smaller -0.33) now statistically significant.Compared alpha 0.05, extreme values considered surprising alpha 0.01 used, differences larger 0.53 scale points (smaller -0.53) statistically significant.Compared alpha 0.05, extreme values considered surprising alpha 0.01 used, differences larger 0.33 scale points (smaller -0.33) now statistically significant.Q5: can’t conclude null hypothesis true, observe statistically non-significant p-value (p > alpha)?calculating p-values always need take prior probability account.need acknowledge probability observed Type 1 error.null hypothesis never true.need acknowledge probability observed Type 2 error.Q6: can’t conclude alternative hypothesis true, observe statistically significant p-value (p < alpha)?calculating p-values always need take prior probability account.need acknowledge probability observed Type 1 error.alternative hypothesis never true.need acknowledge probability observed Type 2 error.Q7: common concern interpreting p-values ‘significant’ normal language implies ‘important’, thus ‘significant’ effect interpreted ‘important’ effect. However, question whether effect important completely orthogonal question whether different zero, even large effect . effects practical impact. smaller effect, less likely effects noticed individuals, effects might still large impact societal level. Therefore, general take home message statistical significance answer question whether effect matters practice, ‘practically important’. answer question whether effect matters, need present cost-benefit analysis.Go app: http://shiny.ieis.tue.nl/d_p_power/. Set sample size 50000, mean difference 0.5, alpha level 0.05. effects , observed, statistically different 0?Effects extreme -0.01 0.01Effects extreme -0.04 0.04Effects extreme -0.05 0.05Effects extreme -0.12 0.12If plot null model large sample size (e.g., n = 10000 per group) see even small mean differences (differences extreme mean difference 0.04) considered ‘surprising’. still means really difference population, observe differences larger 0.04 less 5% time, long run, 95% observed differences smaller mean difference 0.04. becomes difficult argue practical significance effects. Imagine specific intervention successful changing people’s spending behavior, implementing intervention people save 12 cents per year. difficult argue effect make individual happier. However, money combined, yield 2 million, used treat diseases developing countries, real impact. cost intervention might considered high goal make individuals happier, might consider worthwhile goal raise 2 million charity.effects psychology additive (combine transfer increase happiness 0.04 scale points), often difficult argue importance small effects subjective feelings. cost-benefit analysis might show small effects matter lot, whether case inferred p-value. Instead, need report interpret effect size,Q8: Let’s assume random number generator R works, use rnorm(n = 50, mean = 0, sd = 1) generate 50 observations, mean observations 0.5, one-sample t-test effect 0 yields p-value 0.03, smaller alpha level (set 0.05). probability observed significant difference (p < alpha) just chance?3%5%95%100%Q9: statement true?probability replication study yield significant result 1-p.probability replication study yield significant result 1-p multiplied probability null hypothesis true.probability replication study yield significant result equal statistical power replication study (true effect), alpha level (true effect).probability replication study yield significant result equal statistical power replication study + alpha level.question conceptually similar asked Tversky Kahneman (1971) article 'Belief law small numbers':\nFigure 1.13: Screenshot first paragraph Tversky Kahneman, 1971.\nSuppose run experiment 20 subjects, obtained significant result confirms theory (z = 2.23, p < .05, two-tailed). now cause run additional group 10 subjects. think probability results significant, one-tailed test, separately group?Tversky Kahneman argue reasonable answer 48%, correct response correct response question 9, exact probability known (Miller, 2009).Q10: non-significant p-value (.e., p = 0.65) mean null hypothesis true?- result Type 2 error, false negative.Yes, true negative.Yes, p-value larger alpha level null hypothesis true., need least two non-significant p-values conclude null hypothesis true.Q11: correct way present non-significant p-value (e.g., p = 0.34 assuming alpha level 0.05 used independent t-test)?null hypothesis confirmed, p > 0.05There difference two conditions, p > 0.05The observed difference statistically different 0.null hypothesis true.Q12: observing significant p-value (p < .05) mean null hypothesis false?, p < .05 means alternative true, null hypothesis wrong., p-values never statement probability hypothesis theory.Yes, exceptionally rare event occurred.Yes, difference statistically significant.Q13: statistically significant effect always practically important effect?, extremely large samples, extremely small effects can statistically significant, small effects never practically important., alpha level theory set 0.20, case significant effect practically important., important effect depends cost-benefit analysis, surprising data null hypothesis.true.Q14: correct definition p-value?p-value probability null hypothesis true, given data extreme extreme data observed.p-value probability alternative hypothesis true, given data extreme extreme data observed.p-value probability observing data extreme extreme data observed, assuming alternative hypothesis true.p-value probability observing data extreme extreme data observed, assuming null hypothesis true.","code":""},{"path":"pvalue.html","id":"open-questions","chapter":"1 Using p-values to test a hypothesis","heading":"1.8.3 Open Questions","text":"determines shape p-value distribution?determines shape p-value distribution?shape p-value distribution change true effect sample size increases?shape p-value distribution change true effect sample size increases?Lindley’s paradox?Lindley’s paradox?p-values distributed true effect?p-values distributed true effect?correct definition p-value?correct definition p-value?incorrect think non-significant p-value means null hypothesis true?incorrect think non-significant p-value means null hypothesis true?incorrect think significant p-value means null hypothesis false?incorrect think significant p-value means null hypothesis false?incorrect think significant p-value means practically important effect discovered?incorrect think significant p-value means practically important effect discovered?incorrect think observed significant finding, probability made Type 1 error (false positive) 5%?incorrect think observed significant finding, probability made Type 1 error (false positive) 5%?incorrect think 1 – p (e.g., 1 – 0.05 = 0.95) probability effect replicate repeated?incorrect think 1 – p (e.g., 1 – 0.05 = 0.95) probability effect replicate repeated?","code":""},{"path":"errorcontrol.html","id":"errorcontrol","chapter":"2 Error control","heading":"2 Error control","text":"previous chapter p-values learned Neyman-Pearson approach hypothesis testing goal make scientific claims controlling often make fool long run. Benjamini (2016) notes, p-value \"offers first line defense fooled randomness, separating signal noise\". indications banning use p-values increases ability researchers present erroneous claims. Based qualitative analyses scientific articles published null hypothesis significance ban journal Basic Applied Social Psychology Fricker et al. (2019) conclude: “researchers employ descriptive statistics found likely overinterpret /overstate results compared researcher uses hypothesis testing p < 0.05 threshold”. Researchers often control error rates make claims, sometimes intentionally use flexibility data analysis 'p-hack' cherry-pick one many performed analyses shows results wanted see. error-statistical approach statistical inferences, problematic behavior, Mayo (2018) writes:problem cherry picking, hunting significance, host biasing selection effects – main source handwringing behind statistics crisis science – wreak havoc method’s error probabilities. becomes easy arrive findings severely tested.","code":""},{"path":"errorcontrol.html","id":"which-outcome-can-you-expect-if-you-perform-a-study","chapter":"2 Error control","heading":"2.1 Which outcome can you expect if you perform a study?","text":"perform study plan make claim based statistical test plan perform, long run probability making correct claim erroneous claim determined three factors, namely Type 1 error rate, Type 2 error rate, probability null hypothesis true. four possible outcomes statistical test, depending whether result statistically significant , whether null hypothesis true, .False Positive (FP): Concluding true effect, true effect (\\(H_0\\) true). also referred Type 1 error, indicated \\(\\alpha\\).False Negative (FN): Concluding true effect, true effect (\\(H_1\\) true). also referred Type 2 error, indicated \\(\\beta\\).True Negative (TN): Concluding true effect, true effect (\\(H_0\\) true). complement False Positives, thus indicated 1-\\(\\alpha\\).True Positive (TP): Concluding true effect, true effect (\\(H_1\\) true). complement False Negatives, thus indicated 1-\\(\\beta\\).probability observing true positive true effect , long run, equal statistical power study. probability observing false positive null hypothesis true , long run, equal alpha level set, Type 1 error rate.\nFigure 2.1: Difference Type 1 Type 2 errors. Figure made Paul Ellis\n, next study perform, four possible outcomes likely? First, assume set alpha level 5%. Furthermore, assume designed study 80% power (example, assume Omniscient Jones knows indeed exactly 80% power). last thing specify probability null hypothesis true. Let’s assume next study idea null hypothesis true , equally likely null hypothesis true, alternative hypothesis true (probability 50%). can now calculate likely outcome study .perform calculation, take moment think know answer. might designed studies 5% alpha level 80% power, believed equally likely \\(H_0\\) \\(H_1\\) true. Surely, useful reasonable expectations result expect, perform study? Yet experience, many researchers perform without thinking probabilities . often hope observe true positive, even situation described , likely outcome true negative. now calculate probabilities.assume perform 200 studies 5% alpha level, 80% power, 50% probability \\(H_0\\) true. many false positives, true positives, false negatives, true negatives expect long run?table see 2.5% studies false positive (5% Type 1 error rate, multiplied 50% probability \\(H_0\\) true). 40% studies true positive (80% power multiplied 50% probability \\(H_1\\) true). probability false negative 10% (20% Type 2 error rate multiplied 50% probability \\(H_1\\) true). likely outcome true negative, 47.5% (95% probability observing non-significant result, multiplied 50% probability \\(H_0\\) true).might enthusiastic outlook, like perform studies higher probability observing true positive. ? can reduce alpha level, increase power, increase probability \\(H_1\\) true. probability observing true positive depends power, multiplied probability \\(H_1\\) true, design studies values high. Statistical power can increased changes design study (e.g., increasing sample size). probability \\(H_1\\) true depends hypothesis testing. probability \\(H_1\\) true high outset, risk testing hypothesis already established enough certainty. solution, might happen often career, come test hypothesis trivial, explaining peers makes lot sense. words, come idea , explaining , think extremely plausible. creative research ideas likely rare academic career, ever . research needs ground-breaking. also extremely valuable perform replication extension studies relatively likely \\(H_1\\) true, scientific community still benefits knowing findings generalize different circumstances.","code":""},{"path":"errorcontrol.html","id":"ppv","chapter":"2 Error control","heading":"2.2 Positive predictive value","text":"John Ioannides wrote well known article titled \"Published Research Findings False\" (Ioannidis, 2005). time, learned set alpha 5%, Type 1 error rate higher 5% (long run). two statements related? aren’t 95% published research findings true? key understanding difference two different probabilities calculated. Type 1 error rate probability saying effect, effect. Ioannides calculates positive predictive value (PPV), conditional probability study turns show statistically significant result, actually true effect. probability useful understand, people often selectively focus significant results, due publication bias, research areas significant results published.real-life example useful understand concept positive predictive value concerned number vaccinated unvaccinated people admitted hospital COVID symptoms. places, equal numbers patients vaccinated unvaccinated. understand concept positive predictive value, might believe reveals equally likely end hospital, whether vaccinated . incorrect. Figure 2.2 nicely visualizes, probability person vaccinated high, probability vaccinated person ends hospital much lower probability unvaccinated person ends hospital. However, select individuals end hospital, computing probability conditional hospital.\nFigure 2.2: positive predictive value can used explain vaccinated people hospital unvaccinated people.\nuseful understand probability , observed significant result experiment, result actually true positive. words, long run, many true positives can expect, among positive results (true positives false positives)? known Positive Predictive Value (PPV). can also calculate many false positives can expect, among positive results (, true positives false positives). known False Positive Report Probability (Wacholder et al., 2004), sometimes also referred False Positive Risk (Colquhoun, 2019).\\[PPV = \\frac{\\text{True}\\ \\text{Positives}}{(\\text{True}\\ \\text{Positives} +\n                                                \\text{False}\\ \\text{Positives})}\\]\\[FPRP = \\frac{\\text{False}\\ \\text{Positives}}{(\\text{True}\\ \\text{Positives}\n                                                  + \\text{False}\\ \\text{Positives})}\\]PPV FPRP combine classic Frequentist concepts statistical power alpha levels prior probabilities \\(H_0\\) \\(H_1\\) true. depend proportion studies effect (\\(H_1\\) true), effect (\\(H_0\\) true), addition statistical power, alpha level. , can observe false positive null hypothesis true, can observe true positive alternative hypothesis true. Whenever perform study, either operating reality true effect, operating reality effect – don’t know reality .perform studies, aware outcomes studies (significant non-significant findings). read literature, publication bias, often access significant results. thinking PPV (FPRP) becomes important. set alpha level 5%, long run 5% studies \\(H_0\\) true (FP + TN) significant. literature significant results, access true negatives, possible proportion false positives literature much larger 5%.continue example , see 85 positive results (80 + 5) 200 studies. false positive report probability 5/85 = 0.0588. time, alpha 5% guarantees (long run) 5% 100 studies null hypothesis true Type 1 errors: 5%*100 = 5. also true. 200 studies, 0.05*200 = 10 possibly false positives (\\(H_0\\) true experiments). 200 studies performed (\\(H_0\\) true 50% studies), proportion false positives experiments 2.5%. Thus, experiments , proportion false positives , long run, never higher Type error rate set researcher (e.g., 5% \\(H_0\\) true experiments), can lower (\\(H_0\\) true less 100% experiments).\nFigure 2.3: Screenshot output results PPV Shiny app Michael Zehetleitner Felix Schönbrodt \n(Note: FDR FPRP different abbreviations thing.)People often say something like: “Well, know 1 20 results published literature Type 1 errors”. able understand true practice, learning positive predictive value. 100% studies perform, null hypothesis true, studies published, 1 20 studies, long run, false positives (rest correctly reveal statistically significant difference). also explains common p-value misconception \"observed significant finding, probability made Type 1 error (false positive) 5%.\" correct, practice null hypothesis true tests performed (sometimes alternative hypothesis true). Importantly, long publication bias (findings desired results end scientific literature, example non-significant results shared) even researchers use 5% alpha level, quite reasonable assume much 5% significant findings published literature false positives. scientific literature, false positive report probability can quite high, specific circumstances, might even high published research findings false. happen researchers examine mostly studies null hypothesis true, low power, Type 1 error rate inflated due p-hacking types bias.","code":""},{"path":"errorcontrol.html","id":"type-1-error-inflation","chapter":"2 Error control","heading":"2.3 Type 1 error inflation","text":"\nFigure 2.4: Quote 1830 book Babbage Reflections Decline Science England Causes.\nperform multiple comparisons, risk Type 1 error rate inflates. multiple comparisons planned, cases possible control Type 1 error rate lowering alpha level individual analysis. widely known approach control multiple comparisons Bonferroni correction alpha level divided number tests performed (Dunn, 1961). However, researchers also often use informal data analysis strategies inflate Type 1 error rate. Babbage (1830) already complained problematic practices 1830, two centuries later, still common. Barber (1976) provides depth discussion range approaches, eyeballing data decide hypotheses test (sometimes called 'double dipping'), selectively reporting analyses confirm predictions, ignoring non-significant results, collecting many variables performing multitudes tests, performing sub-group analyses planned analysis yields nonsignificant results, nonsignificant prediction derive new hypothesis supported data, test hypothesis data hypothesis derived (sometimes called HARKing, Hypothesizing Results Known (Kerr, 1998)). Many researchers admit used practices inflate error rates (see section questionable research practices chapter research integrity). used practices first scientific article published, fully aware problematic - article published several years later reflect , see Jostmann et al. (2016).paradigms, researchers lot flexibility compute main dependent variable. Elson colleagues examined 130 publications use Competitive Reaction Time Task, participants select duration intensity blasts delivered competitor (Elson et al., 2014). task used measure 'aggressive behavior' ethical manner. compute score, researchers can use duration noise blast, intensity, combination thereof, averaged number trials, several possible transformations data. 130 publications examined reported 157 different quantification strategies total, showing calculations dependent variable unique, used single article. One might wonder authors sometimes used different computations across articles. One possible explanation used flexibility data analysis find statistically significant results.\nFigure 2.5: Plot publications using CRTT (blue) unique quantifications measure (red). Figure FlexibleMeasures.com Malte Elson\n","code":""},{"path":"errorcontrol.html","id":"optionalstopping","chapter":"2 Error control","heading":"2.4 Optional stopping","text":"\nFigure 2.6: Screenshot scientific paper explicitly admitting using optional stopping.\nOne practice inflates Type 1 error rate known optional stopping. optional stopping, researcher repeatedly analyzes data, continues data collection test result statistically significant, stops significant effect observed. quote published article figure example researchers transparently report used optional stopping, commonly people disclose use optional stopping methods sections. recent years, many researchers learned optional stopping problematic. lead general idea collect data, look whether results significant, stop data collection result significant, , continue data collection. correct conclusion, example becoming inflexible. correct approach - collect data batches, called sequential analysis - extensively developed statisticians, used many medical trials. discuss sequential analyses dedicated chapter. main lesson certain research practices can increase flexibility efficiency studies perform, done right, practices can inflate Type 1 error rate done wrong. Let’s therefore try get better understanding inflating Type 1 error rate optional stopping, correctly using sequential analysis.Copy code R run . script simulate ongoing data collection. 10 participants condition, p-value calculated performing independent t-test, t-test repeated every additional participant collected. , p-values plotted function increasing sample size.example, Figure , see p-value plotted y-axis (0 1) sample size plotted x-axis (0 200). simulation, true effect size d = 0, meaning true effect. can thus observe true negatives false positives. sample size increases, p-value slowly moves (remember chapter p-values true effect, p-values uniformly distributed). Figure 2.7, p-value drops grey line (indicating alpha level 0.05) collecting 83 participants condition, drift back upwards larger p-values. figure, becomes clear often look data, larger total sample size, higher probability one analyses yield p < \\(\\alpha\\). resources infinite, Type 1 error rate 1, researcher can always find significant result optional stopping.\nFigure 2.7: Simulated p-values additional observation null true.\ntrue effect, see p-values also vary, eventually drop alpha level. Due variation, just know exactly . perform -priori power analysis, can compute probability looking specific sample size yield significant p-value. Figure 2.8 see simulation, now true small effect d = 0.3. 200 observations per condition, sensitivity power analysis reveals 85% power. analyze data interim analysis (e.g., 150 observations) often already find statistically significant effect (74% power). illustrates benefit sequential analyses, control error rates, can stop early interim analysis. Sequential analyses especially useful large expensive studies uncertainty true effect size.\nFigure 2.8: Simulated p-values additional observation d = 0.3.\nformally examine inflation Type 1 error rate optional stopping simulation study. Copy code R run code. Note 50000 simulations (needed get error rates reasonably accurate) take time run.simulation perform multiple independent t-tests simulated data, looking multiple times maximum sample size reached. first four lines, can set important parameters simulation. First, maximum sample size condition (e.g., 100). , number looks (e.g., 5). best, can look data every participant (e.g., 100 participants, can look 100 times – actually 98 times, need 2 participants condition t-test!). can set number simulations (, clearer pattern , longer simulation takes), alpha level (e.g., 0.05). Since can make Type 1 error true effect, effect size set 0 simulations.perform single test, Type 1 error rate probability finding p-value lower alpha level, effect. optional stopping scenario look data twice, Type 1 error rate probability finding p-value lower alpha level first look, probability finding p-value lower alpha level first look, finding p-value lower alpha level second look. conditional probability, makes error control little bit complex multiple looks completely independent.much optional stopping inflate Type 1 error rate? p-values can expect optional stopping?Start running simulation without changing values, simulating 100 participants condition, looking 5 times data, alpha 0.05. Note 50.000 simulations take ! see something similar Figure 2.9 (based 500.000 simulations make pattern clear).\nFigure 2.9: Simulation 500000 studies performing 5 interim analyses alpha level 5%.\nsee 100 bars, one % (one p-values 0.00 0.01, one p-values 0.01 0.02, etc.). horizontal line indicates p-values fall, uniformly distributed (true effect, explained chapter p-values).distribution p-values peculiar. see compared uniform distributions, bunch results just alpha threshold 0.05 missing, seem pulled just 0.05, much higher frequency outcomes compared data analyzed multiple times comes . Notice relatively high p-values (e.g., p = 0.04) common lower p-values (e.g., 0.01). see chapter bias detection statistical techniques p-curve analysis can pick pattern.using alpha level 5% 5 looks data, overall Type 1 error rate inflated 14%. lower alpha level interim analysis, overall Type 1 error rate can controlled. shape p-value distribution still look peculiar, total number significant test results controlled desired alpha level. well-known Bonferroni correction (.e., using alpha level \\(\\alpha\\) / number looks), Pocock correction slightly efficient. information perform interim analyses controlling error rates, see dedicated chapter sequential analysis.","code":"\nn <- 200 # total number of datapoints (per condition) after initial 10\nd <- 0.0 # effect size d\n\np <- numeric(n) # store p-values\nx <- numeric(n) # store x-values\ny <- numeric(n) # store y-values\n\nn <- n + 10 # add 10 to number of datapoints\n\nfor (i in 10:n) { # for each simulated participants after the first 10\n  x[i] <- rnorm(n = 1, mean = 0, sd = 1)\n  y[i] <- rnorm(n = 1, mean = d, sd = 1)\n  p[i] <- t.test(x[1:i], y[1:i], var.equal = TRUE)$p.value\n}\n\np <- p[10:n] # Remove first 10 empty p-values\n\n# Create the plot\npar(bg = \"#fffafa\")\nplot(0, col = \"red\", lty = 1, lwd = 3, ylim = c(0, 1), xlim = c(10, n), \n     type = \"l\", xlab = \"sample size\", ylab = \"p-value\")\nlines(p, lwd = 2)\nabline(h = 0.05, col = \"darkgrey\", lty = 2, lwd = 2) # draw line at p = 0.05\n\nmin(p) # Return lowest p-value from all looks\ncat(\"The lowest p-value was observed at sample size\", which.min(p) + 10) \ncat(\"The p-value dropped below 0.05 for the first time at sample size:\", \n    ifelse(is.na(which(p < 0.05)[1] + 10), \"NEVER\", which(p < 0.05)[1] + 10)) \nN <- 100 # total datapoints (per condition)\nlooks <- 5 # set number of looks at the data\nnsims <- 50000 # number of simulated studies\nalphalevel <- 0.05 # set alphalevel\n\nif(looks > 1){\n  look_at_n <- ceiling(seq(N / looks, N, (N - (N / looks)) / (looks - 1)))\n}  else {\n  look_at_n <- N\n}\nlook_at_n <- look_at_n[look_at_n > 2] # Remove looks at N of 1 or 2\nlooks<-length(look_at_n) # if looks are removed, update number of looks\n\nmatp <- matrix(NA, nrow = nsims, ncol = looks) # Matrix for p-values l tests\np <- numeric(nsims) # Variable to save pvalues\n\n# Loop data generation for each study, then loop to perform a test for each N\nfor (i in 1:nsims) {\n  x <- rnorm(n = N, mean = 0, sd = 1)\n  y <- rnorm(n = N, mean = 0, sd = 1)\n  for (j in 1:looks) {\n    matp[i, j] <- t.test(x[1:look_at_n[j]], y[1:look_at_n[j]], \n                         var.equal = TRUE)$p.value # perform the t-test, store\n  }\n  cat(\"Loop\", i, \"of\", nsims, \"\\n\")\n}\n\n# Save Type 1 error rate smallest p at all looks\nfor (i in 1:nsims) {\n  p[i] <- ifelse(length(matp[i,which(matp[i,] < alphalevel)]) == 0, \n                 matp[i,looks], matp[i,which(matp[i,] < alphalevel)])\n}\n\nhist(p, breaks = 100, col = \"grey\") # create plot\nabline(h = nsims / 100, col = \"red\", lty = 3)\n\ncat(\"Type 1 error rates for look 1 to\", looks, \":\", \n    colSums(matp < alphalevel) / nsims)\ncat(\"Type 1 error rate when only the lowest p-value for all looks is reported:\", \n    sum(p < alphalevel) / nsims)"},{"path":"errorcontrol.html","id":"justifyerrorrate","chapter":"2 Error control","heading":"2.5 Justifying Error Rates","text":"reject \\(H_0\\) , may reject true; accept \\(H_0\\) , may accepting false, say, really alternative Bt true. two sources error can rarely eliminated completely; cases important avoid first, others second. reminded old problem considered Laplace number votes court judges needed convict prisoner. serious convict innocent man acquit guilty? depend upon consequences error; whether punishment death fine; danger community released criminals; current ethical views punishment. point view mathematical theory, can show risk errors may controlled minimised. use statistical tools given case, determining just balance struck, must left investigator.Even though theory Type 1 Type 2 error rate justified researcher (Neyman Pearson (1933) write ), practice researchers tend imitate others. default use alpha level 0.05 can already found work Gosset t-distribution (Cowles & Davis, 1982; Kennedy-Shaffer, 2019), believed difference two standard deviations (z-score 2) sufficiently rare. default use 80% power (20% Type 2 error rate) similarly based personal preferences Cohen (1988), writes:proposed convention , investigator basis setting desired power value, value .80 used. means beta set .20. value offered several reasons (Cohen, 1965, pp. 98-99). chief among takes consideration implicit convention alpha .05. beta .20 chosen idea general relative seriousness two kinds errors order .20/.05, .e., Type errors order four times serious Type II errors. .80 desired power convention offered hope ignored whenever investigator can find basis substantive concerns specific research investigation choose value ad hoc.see conventions built conventions: norm aim 80% power built norm set alpha level 5%. Although nothing special alpha level 5%, interesting reflect become widely established. Irwin Bross (1971) argues use alpha level functional efficient seen aspect communication networks among scientists, writes “Thus specification critical levels […] proved practice effective method controlling noise communication networks.”\nBross believes 0.05 threshold somewhat, completely arbitrary, asks us imagine happened alpha level 0.001 proposed, alpha level 0.20. cases, believes convention spread – first case many fields sufficient resources make claims low error rate, second case researchers found alpha level satisfactory quantification ‘rare’ events.\nUygun Tunç et al. (2021) argue one possible reason , far conventions go, alpha level 5% might low enough peers take claims made error rate seriously, time high enough peers motivated perform independent replication study increase decrease confidence claim. Although lower error rates establish claims convincingly, also require resources. One might speculate research areas every claim important enough careful justification costs benefits, 5% pragmatic function facilitating conjectures refutations fields otherwise lack coordinated approach knowledge generation, faced limited resources.Nevertheless, researchers proposed move away default use 5% alpha level. example, Johnson (2013) proposes default significance level 0.005 0.001. Others cautioned blanket recommendation additional resources required reduce Type 1 error rate might worth costs (Lakens, Adolfi, et al., 2018). lower alpha lever requires larger sample size achieve statistical power. sample size increased, lower alpha level reduces statistical power, increases Type 2 error rate. Whether desirable evaluated case case basis.two main reasons abandon universal use 5% alpha level. first reason carefully choose alpha level decision-making becomes efficient (Gannon et al., 2019; Mudge et al., 2012). researchers use hypothesis tests make dichotomous decisions methodological falsificationist approach statistical inferences, certain maximum sample size willing able collect, typically possible make decisions efficiently choosing error rates combined cost Type 1 Type 2 errors minimized. aim either minimize balance Type 1 Type 2 error rates given sample size effect size, alpha level set based convention, weighting relative cost types errors (Maier & Lakens, 2022).example, imagine researcher plans collect 64 participants per condition detect d = 0.5 effect, weighs cost Type 1 errors 4 times much Type 2 errors. exactly scenario Cohen (1988) described, 64 participants per condition relative weight Type 1 Type 2 errors yields 5% Type 1 error rate 20% Type 2 error rate. Now imagine researcher realizes resources collect 80 observations instead just 64. interest effect size d = 0.5, relative weight Type 1 Type 2 errors 4 satisfied set alpha level 0.037 Type 2 error rate 0.147. Alternatively, researcher might decided collect 64 observations, balance error rates, set alpha level weighted combined error rate minimized, achieved alpha level set 0.033, visualized Figure 2.10 (information, see Maier & Lakens (2022)).\nFigure 2.10: Weighted combined error rate, minimized alpha = 0.037.\nJustifying error rates can lead situations alpha level increased 0.05, leads optimal decision making. Winer (1962) writes:frequent use .05 .01 levels significance matter convention little scientific logical basis. power tests likely low levels significance, Type 1 Type 2 errors approximately equal importance, .30 .20 levels significance may appropriate .05 .01 levels.”reasoning design 70% power smallest effect size interest balance Type 1 Type 2 error rates sensible manner. course, increase alpha level deemed acceptable authors can justify costs increase Type 1 error rate sufficiently compensated benefit decreased Type 2 error rate. encompass cases (1) study practical implications require decision making, (2) cost-benefit analysis provided gives clear rationale relatively high costs Type 2 error, (3) probability \\(H_1\\) false relatively low, (4) feasible reduce overall error rates collecting data.One also carefully reflect choice alpha level experiment achieves high statistical power effect sizes considered meaningful. study 99% power effect sizes interest, thus 1% Type 2 error rate, uses default 5% alpha level, also suffers lack balance, use lower alpha level lead balanced decision, increase severity test.second reason relevant large data sets, related Lindley's paradox. statistical power increases, p-values 0.05 (e.g., p = 0.04) can likely effect effect. prevent situations frequentist rejects null hypothesis based p < 0.05, evidence test favors null hypothesis alternative hypothesis, recommended lower alpha level function sample size. need discussed Leamer (1978), writes \"rule thumb quite popular now, , setting significance level arbitrarily .05, shown deficient sense every reasonable viewpoint significance level decreasing function sample size.\" idea approach reduce alpha level Bayes factor likelihood computed significant result never evidence null hypothesis (online Shiny app perform calculations, see .","code":""},{"path":"errorcontrol.html","id":"why-you-dont-need-to-adjust-your-alpha-level-for-all-tests-youll-do-in-your-lifetime.","chapter":"2 Error control","heading":"2.6 Why you don't need to adjust your alpha level for all tests you'll do in your lifetime.","text":"researchers criticize corrections multiple comparisons one might well correct tests lifetime (Perneger, 1998). choose use Neyman-Pearson approach statistics reason correct tests perform lifetime work done life tests single theory, use last words decide accept reject theory, long one individual tests performed yielded p < \\(\\alpha\\). Researchers rarely work like .Instead, Neyman-Pearson approach hypothesis testing, goal use data make decisions act. Neyman (1957) calls approach inductive behavior. outcome experiment leads one take different possible actions, can either practical (e.g., implement new procedure, abandon research line) scientific (e.g., claim effect). error-statistical approach (Mayo, 2018) inflated Type 1 error rates mean become likely able claim support hypothesis, even hypothesis wrong. reduces severity test. prevent , need control error rate level claim.useful distinction literature multiple testing union-intersection testing approach, intersection-union testing approach (Dmitrienko & D’Agostino Sr, 2013). union-intersection approach, claim made -least-one test significant. cases, correction multiple comparisons required control error rate. intersection-union approach, claim made performed tests statistically significant, correction multiple comparisons required (indeed, assumptions researchers even increase alpha level intersection-union approach).Let’s assume collect data 100 participants control treatment condition. collect 3 dependent variables (dv1, dv2, dv3). population difference groups three variables (true effect size 0). analyze three dv’s independent t-tests. requires specifying alpha level, thus deciding whether need correct multiple comparisons. reason fully understand, several researchers believe difficult decide need correct multiple comparisons. Bretz, Hothorn, & Westfall (2011) write excellent book “Multiple Comparisons Using R”: “appropriate choice null hypotheses primary interest controversial question. , always clear set hypotheses constitute family H1,…,Hm. topic often dispute general consensus.” one best papers controlling multiple comparisons , Bender & Lange (2001) write: “Unfortunately, simple unique answer appropriate control error rate. Different persons may different nevertheless reasonable opinions. addition problem deciding error rate control, defined first tests study belong one experiment.”never understood confusion, least working within Neyman-Pearson approach hypothesis testing, goal control error rates level statistical claim. control error rates depends claim(s) want make. might want act (claim ) treatment works difference treatment control conditions three variables. means consider prediction corroborated p-value first t-test smaller alpha level, p-value second t-test smaller alpha level, p-value third t-test smaller alpha level. falls union-intersection approach, researcher correct alpha level multiple comparisons.also want make three different predictions. Instead one hypothesis (“something happen”) three different hypotheses, predict effect dv1, dv2, dv3. claims can corroborated, . three tests, inform three claims, multiple comparisons, correction alpha level required.might seem researchers can get corrections multiple comparisons formulating hypothesis every possible test perform. Indeed, can. ten ten correlation matrix, researcher might state testing 45 unique predictions, uncorrected alpha level. However, readers might reasonably question whether 45 tests predicted sensible theory, author just making predictions order correct alpha level. Distinguishing two scenarios statistical question, theoretical question. 45 tests corroborate prediction, meager track record predictions make readers doubt body work used derive predictions anything going .different ways control error rates, easiest Bonferroni correction ever--slightly less conservative Holm-Bonferroni sequential procedure. number statistical tests becomes substantial, sometimes preferable control false discovery rates, instead error rates (Benjamini & Hochberg, 1995).","code":""},{"path":"errorcontrol.html","id":"power-analysis","chapter":"2 Error control","heading":"2.7 Power Analysis","text":"far largely focused Type 1 error control. clear Figure 2.8, true effect p-values eventually become smaller alpha level sample size becomes large enough. designing experiment, one goal might choose sample size provides desired Type 2 error rate effect size interest. can achieved performing -priori power analysis. statistical power test (Type 2 error rate) depends effect size, sample size, alpha level. larger sample size, effect size, alpha level, higher statistical power, lower effect size, sample size, alpha level, lower statistical power.important highlight goal -priori power analysis achieve sufficient power true effect size. true effect size always unknown designing study. goal -priori power analysis achieve sufficient power, given specific assumption effect size researcher wants detect. Just like Type error rate maximum probability making Type error conditional assumption null hypothesis true, -priori power analysis computed assumption specific effect size. unknown assumption correct. researcher can make sure assumptions well justified. Statistical inferences based test Type II error controlled conditional assumption specific effect size. allow inference , assuming true effect size least large used -priori power analysis, maximum Type II error rate study larger desired value.Figure 2.11 see expected distribution observed standardized effect sizes (Cohen's d) independent t-test 50 observations condition. bell-shaped curve left represents expectations null true, red areas tail represent Type 1 errors. bell-shaped curve right represents expectations alternative hypothesis true, d = 0.5. vertical line d = 0.4 represents critical effect size. sample size alpha level 0.05, observed effect sizes smaller d = 0.4 statistically significant. true effect, outcomes Type 2 errors, illustrated blue shaded area. remainder curve reflects true positives, true effect, observed effect sizes statistically significant. power test percentages distribution right larger critical value.\nFigure 2.11: Distribution d = 0 d = 0.5 independent t-test n = 50.\nissue Type 2 error control discussed detail chapter sample size justification. Even though topic Type 2 error control briefly discussed , least important Type 1 error control. informative study high probability observing effect effect. Indeed, default recommendation aim 80% power leaves surprisingly large (20%) probability Type 2 error. researcher cares making decision error, researcher care whether decision error false positive false negative, argument made Type 1 Type 2 errors weighed equally. Therefore, desiging study balanced error rates (e.g., 5% Type 1 error 95% power) make sense.","code":""},{"path":"errorcontrol.html","id":"test-yourself-1","chapter":"2 Error control","heading":"2.8 Test Yourself","text":"","code":""},{"path":"errorcontrol.html","id":"questions-about-the-positive-predictive-value","chapter":"2 Error control","heading":"2.8.1 Questions about the positive predictive value","text":"Q1: example start chapter, see control Type 1 error rate 5% using alpha 0.05. Still, 50% probability \\(H_0\\) true, proportion false positives experiments performed turns much lower, namely 2.5%, 0.025. ?proportion false positives experiments performed variable distribution around true error rate – sometimes ’s higher, sometimes ’s lower, due random variation.proportion false positives experiments performed 5% \\(H_0\\) true 200 studies.proportion false positives experiments performed 5% 50% power – power increases 50%, proportion false positives experiments performed becomes smaller.proportion false positives experiments performed 5% 100% power, becomes smaller power lower 100%.Q2: make biggest difference improving probability find true positive? Check answer shifting sliders online PPV app http://shinyapps.org/apps/PPV/ https://shiny.ieis.tue.nl/PPV/Increase % -priori true hypothesesDecrease % -priori true hypothesesIncrease alpha levelDecrease alpha levelIncrease powerDecrease powerIncreasing power requires bigger sample sizes, studying larger effects. Increasing % -priori true hypotheses can done making better predictions – example building reliable findings, relying strong theories. useful recommendations want increase probability performing studies find statistically significant result.Q3: Set “% priori true hypotheses” slider 50%. Leave ‘\\(\\alpha\\) level’ slider 5%. Leave ‘% p-hacked studies’ slider 0. title Ioannidis’ paper ‘published research findings false’. One reason might studies often low power. value power PPV 50%. words, level power significant result just likely true, false?80%50%20%5%seems low power alone best explanation published findings might false, unlikely power low enough scientific literature. Ioannidis (2005) discusses scenarios becomes likely published research findings false. assume ‘p-hacked studies’, studies show significant result due bias, enter literature. good reasons believe happens, discussed chapter. ‘presets Ioannidis’ dropdown menu, can select situations. Explore , pay close attention ones PPV smaller 50%.Q4: general, published findings false? Interpret ‘low’ ‘high’ answer options relation values first example chapter 50% probability \\(H_1\\) true, 5% alpha, 80% power,\n0% bias.probability examining true hypothesis low, combined either low power substantial bias (e.g., p-hacking).probability examining true hypothesis high, combined either low power substantial bias (e.g., p-hacking).alpha level high, combined either low power substantial bias (e.g., p-hacking).power low p-hacking high (regardless % true hypotheses one examines).Q5: Set “% priori true hypotheses” slider 0%. Set “% p-hacked studies” slider 0%. Set “\\(\\alpha\\) level” slider 5%. Play around power slider. statement true?\nWithout p-hacking, alpha level 5%, 0% hypotheses true, proportion false positives experiments performed 100%.PPV depends power studies.regardless power, PPV equals proportion false positives experiments performed.regardless power, proportion false positives experiments performed 5%, PPV 0% (significant results false positives).","code":""},{"path":"errorcontrol.html","id":"questions-about-optional-stopping","chapter":"2 Error control","heading":"2.8.2 Questions about optional stopping","text":"Questions 1 4, use script :Q1: Run script plots p-value sample size increases 20 times, count often lowest p-value ends 0.05 (calculate long run probability happening extensive simulations later). Remember can click 'clipboard' icon top right code section copy code clipboard, paste RStudio.Q2: true effect, can observe true positive false negative. Change effect size second line script d <- 0.0 d <- 0.3. relatively small true effect, 200 participants condition, 85% power (85% probability finding significant effect). Run script . one possible example trajectory p-values sample size increases. Run script 20 times. Take good look variation p-value trajectory. Remember N = 200, 85% times p-value ended 0.05. script returns sample size p-value lowest (often, always, maximum sample size, true effect) sample size p-value drops 0.05 first time. statement true?p-value drops 0.05, stays 0.05.p-value randomly moves 0 1, every now end 0.05.p-value often drops 0.05 well 200 participants \ncondition. around 50% simulations, already happens N = 100.p-value typically move 0.05 stay time,\ngiven large enough sample, always move back p > 0.05.Q3: Change effect size second line script d <- 0.8, can regarded large effect. Run script 20 times. Take good look variation p-value trajectory. statement true?p-value randomly moves 0 1, every now end 0.05.p-values drop stay 0.05 much earlier true effect size 0.3.p-values meaningful effect sizes large (e.g., d = 0.8), meaningless effect sizes small (e.g., d = 0.3).examine large effect, whenever p-value drops 0.05, always stay 0.05 sample size increases.Q4: Looking Figure 2.9, statement true?Optional stopping impact Type 1 error rate.Optional stopping inflates Type 1 error rate. can see first five bars (p-values 0.00 0.05), substantially higher horizontal line.Optional stopping inflates Type 1 error rate. can see bars just 0.05, dip substantially uniform distribution present true effect.Questions 5 8, use script :Q5: script simulate optional stopping provides written output. One summary gives Type 1 error rate individual look. One summary gives Type 1 error rate optional stopping used. running script default values, statement true?look, Type 1 error rate higher alpha level (0.05).\nusing optional stopping (reporting lowest p-value), Type 1 error rate higher 0.05.look, Type 1 error rate approximately equal alpha level (0.05). using optional stopping (reporting lowest p-value), alpha level also approximately equals alpha level (0.05).look, Type 1 error rate approximately equal alpha level (0.05). using optional stopping, Type 1 error rate higher alpha level (0.05).Q6: Change number looks simulation 2 (change 'looks <- 5' 'looks <- 2'), leave settings . Run simulation . Type 1 error rate using optional stopping 1 interim analysis, rounded 2 digits? (Note due small number simulations, exact alpha level get might differ little bit \nanswer options ).0.050.080.120.18Q7: Wagenmakers (2007) notes: “user NHST always obtain significant result optional stopping (.e., analyzing data accumulate stopping experiment whenever p-value reaches desired significance level)”. correct. ’s true p-value always drop alpha level point time. , need rather large number observations. can calculate maximum Type 1 error rate due optional stopping maximum sample size. example, maximum Type 1 error rate optional stopping used collecting 200 participants condition, looking 200 times (198 times, given can’t perform t-test sample size 1 2 people)? Set number participants 200, number looks 200, number simulations 10000 (simulation take even longer!), alpha 0.05.maximum Type 1 error rate collecting 200 participants \ncondition independent t-test, using optional stopping, rounded 2\ndigits? (Note simulation take , still, due \nrelatively small number simulations, exact alpha level get might\ndiffer little bit answer options – choose answer option\nclosest result).0.050.110.200.41Q8: Wikipedia, look entry Pocock boundary: https://en.wikipedia.org/wiki/Pocock_boundary . ethical reasons look data, data collected. clear medicine, similar arguments can made research areas (see Lakens, 2014). Researchers often want look data multiple times. perfectly fine, long design study number looks advance, control Type 1 error rate.Pocock boundary provides easy way control type 1 error rate sequential analyses. Sequential analysis formal way optional stopping. Researchers use slightly lower alpha level look, make sure overall alpha level (looks) larger 5%.Set number participants 100, number looks 5, number simulations 50000 (back original script). Wikipedia article Pocock boundary, find corrected alpha level 5 looks data. Change alpha level simulation value. Run simulation. following statements true?Type 1 error rate look approximately 0.03, overall alpha level approximately 0.05.Type 1 error rate look approximately 0.03, overall alpha level approximately 0.15.Type 1 error rate look approximately 0.016, overall alpha level approximately 0.05.Type 1 error rate look approximately 0.016, overall alpha level approximately 0.08.Q9: Look graph p-value distribution using Pocock boundary, compare graph got using Pocock boundary. can flip back forth plots generated RStudio using blue arrows plots tab. statement true?Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nlikely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) also \nlikely slightly higher p-values (p = 0.04).Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nlikely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) less likely\nslightly higher p-values (p = 0.04).Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nless likely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) likely\nslightly higher p-values (p = 0.04).Without Pocock’s boundary, small p-values (e.g., p = 0.01) \nless likely slightly higher p-values (p = 0.04). \nPocock’s boundary, small p-values (e.g., p = 0.01) also less\nlikely slightly higher p-values (p = 0.04).","code":"\nn <- 200 # total number of datapoints (per condition) after initial 10\nd <- 0.0 # effect size d\n\np <- numeric(n) # store p-values\nx <- numeric(n) # store x-values\ny <- numeric(n) # store y-values\n\nn <- n + 10 # add 10 to number of datapoints\n\nfor (i in 10:n) { # for each simulated participants after the first 10\n  x[i] <- rnorm(n = 1, mean = 0, sd = 1)\n  y[i] <- rnorm(n = 1, mean = d, sd = 1)\n  p[i] <- t.test(x[1:i], y[1:i], var.equal = TRUE)$p.value\n}\n\np <- p[10:n] # Remove first 10 empty p-values\n\n# Create the plot\npar(bg = \"#fffafa\")\nplot(0, col = \"red\", lty = 1, lwd = 3, ylim = c(0, 1), xlim = c(10, n), \n     type = \"l\", xlab = \"sample size\", ylab = \"p-value\")\nlines(p, lwd = 2)\nabline(h = 0.05, col = \"darkgrey\", lty = 2, lwd = 2) # draw line at p = 0.05\n\nmin(p) # Return lowest p-value from all looks\ncat(\"The lowest p-value was observed at sample size\", which.min(p) + 10) \ncat(\"The p-value dropped below 0.05 for the first time at sample size:\", \n    ifelse(is.na(which(p < 0.05)[1] + 10), \"NEVER\", which(p < 0.05)[1] + 10)) \nN <- 100 # total datapoints (per condition)\nlooks <- 5 # set number of looks at the data\nnsims <- 50000 # number of simulated studies\nalphalevel <- 0.05 # set alphalevel\n\nif(looks > 1){\n  look_at_n <- ceiling(seq(N / looks, N, (N - (N / looks)) / (looks - 1)))\n}  else {\n  look_at_n <- N\n}\nlook_at_n <- look_at_n[look_at_n > 2] # Remove looks at N of 1 or 2\nlooks<-length(look_at_n) # if looks are removed, update number of looks\n\nmatp <- matrix(NA, nrow = nsims, ncol = looks) # Matrix for p-values l tests\np <- numeric(nsims) # Variable to save pvalues\n\n# Loop data generation for each study, then loop to perform a test for each N\nfor (i in 1:nsims) {\n  x <- rnorm(n = N, mean = 0, sd = 1)\n  y <- rnorm(n = N, mean = 0, sd = 1)\n  for (j in 1:looks) {\n    matp[i, j] <- t.test(x[1:look_at_n[j]], y[1:look_at_n[j]], \n                         var.equal = TRUE)$p.value # perform the t-test, store\n  }\n  cat(\"Loop\", i, \"of\", nsims, \"\\n\")\n}\n\n# Save Type 1 error rate smallest p at all looks\nfor (i in 1:nsims) {\n  p[i] <- ifelse(length(matp[i,which(matp[i,] < alphalevel)]) == 0, \n                 matp[i,looks], matp[i,which(matp[i,] < alphalevel)])\n}\n\nhist(p, breaks = 100, col = \"grey\") # create plot\nabline(h = nsims / 100, col = \"red\", lty = 3)\n\ncat(\"Type 1 error rates for look 1 to\", looks, \":\", \n    colSums(matp < alphalevel) / nsims)\ncat(\"Type 1 error rate when only the lowest p-value for all looks is reported:\", \n    sum(p < alphalevel) / nsims)"},{"path":"errorcontrol.html","id":"open-questions-1","chapter":"2 Error control","heading":"2.8.3 Open Questions","text":"definition positive predictive value?definition positive predictive value?definition false positive?definition false positive?definition false negative?definition false negative?definition true positive?definition true positive?definition true negative?definition true negative?perform 200 studies, 50% probability H0 true, 80% power, use 5% Type 1 error rate, likely outcome study?perform 200 studies, 50% probability H0 true, 80% power, use 5% Type 1 error rate, likely outcome study?can increase positive predictive value lines research decide perform?can increase positive predictive value lines research decide perform?incorrect think “1 20 results published literature Type 1 errors”?incorrect think “1 20 results published literature Type 1 errors”?problem optional stopping?problem optional stopping?multiple tests inflate Type 1 error rate, can done correct multiple comparisons?multiple tests inflate Type 1 error rate, can done correct multiple comparisons?replication study, determines probability observe significant effect?replication study, determines probability observe significant effect?approach statistical inferences Neyman-Pearson approach part , main goal Neyman-Pearson approach?approach statistical inferences Neyman-Pearson approach part , main goal Neyman-Pearson approach?error rates (alpha beta) statistical test determined?error rates (alpha beta) statistical test determined?","code":""},{"path":"likelihoods.html","id":"likelihoods","chapter":"3 Likelihoods","heading":"3 Likelihoods","text":"addition frequentist Bayesian approaches statistical inferences, likelihoods provide third approach statistical inferences (Dienes, 2008; Pawitan, 2001). Like Bayesian approaches, discussed next chapter, likelihoodists interested quantifying measure relative evidence comparing two models hypotheses. Unlike Bayesians, likelihoodists enthusiastic idea incorporating prior information statistical inferences. likelihoodists Taper Lele (2011) write:believe Bayes' rule Bayesian mathematics flawed, axiomatic foundational definition probability Bayesianism doomed answer questions irrelevant science. care believe, barely care believe, interested can show.Likelihoodists interested measure relative evidence,. Unlike Fisherian frequentist approach, \\(H_0\\) specified, lower p-values less compatible null model interpreted evidence null, likelihoodists specify null alternative model, quantify relative likelihood data models. Neyman-Pearson approach, \\(H_0\\) \\(H_1\\) specified, aims decisions act, aim quantify evidence. time, likelihood functions important part frequentist Bayesian approaches. Neyman-Pearson approach, likelihoods play important role Neyman-Pearson lemma, shows likelihood ratio test powerful test \\(H_0\\) \\(H_1\\), useful determining critical value used reject hypothesis. Bayesian approaches, likelihood combined prior compute posterior probability distribution.can use likelihood functions make inferences unknown quantities. Let’s imagine flip coin 10 times, turns heads 8 times. true probability (sometimes indicated Greek letter \\(\\theta\\) (theta), use p chapter) coin landing heads?binomial probability observing k successes n studies :\\[\nPr\\left( k;n, p \\right) = \\frac{n!}{k!\\left( n - k \\right)!}p^{k}{(1 - p)}^{n - k}\n\\]p probability success, k observed number successes, n number trials. first term indicates number possible combinations results (e.g., start eight successes, end eight successes, possible combinations), multiplied probability observing one success trials, multiplied probability observing success remaining trials.Let’s assume expect fair coin. binomial probability observing 8 heads 10 coin flips, p = 0.5? answer :\\[\nPr\\left(8;10, 0.5 \\right) = \\frac{10!}{8!\\left( 10 - 8 \\right)!}*0.5^{8}*{(1 - 0.5)}^{10 - 8}\n\\]\nR probability computed :using function:Let’s assume don’t information coin. (might believe coins fair; priors discussed talk Bayesian statistics next chapter). equation Pr(k;n,p) gives probability observing k successes n trials coin’s probability success p.computing probability, assume model known, compute probability observing specific outcome. based data observed, can ask reversed question: value p make observed data likely? computing likelihood, assume data known, make inference likely parameter model. answer question, can plug values k n find value p maximizes function. Ronald Fisher called maximum likelihood estimation (considered one important developments 20th century statistics, Fisher published first paper 1912 third year undergraduate 22 (Aldrich, 1997)). Since p can value 0 1, can plot values known likelihood function, can see maximum easily.\nFigure 3.1: Binomial likelihood function 8 successes 10 trials.\nlikelihood plotted possible values p (0 1). surprising given data observed, likely value true parameter 8 10, p = 0.8, likelihood 0.30 (highest point y-axis). example, p = 0.8 called maximum likelihood estimator. important know likelihood meaning isolation. sense, differs probability. can compare likelihoods function across different values p. can read value p, see given observed data, low values p (e.g., 0.2) likely.subtle difference probability likelihood. colloquial language, can use terms mean thing, statistics terms used different sides coin. Note equation Pr involves information data (k, n) information parameter (p). compute probability, view p fixed (instance, fair coin, plug p = 0.5) estimate probability different outcomes (k, n). resulting function probability mass function. compute likelihood, instead view observed data fixed (e.g., observing 5 heads 10 coin tosses), view Pr function p, estimating value maximizes likelihood particular sample.Likelihoods example statistical inference: observed data, use data draw inference different population parameters. formally, likelihood function (joint) density function evaluated observed data. Likelihood functions can calculated many different models (binomial distributions, normal distributions, see Millar (2011)). approach called likelihoodist statistics, likelihoodism, distinct frequentist Bayesian approaches statistics, directly uses likelihood function make inferences.mix heads tails observed, likelihood curve rises falls, possible coin can come heads tails (, already observed). heads 0 heads observed, likelihood curve peaks far left right x-axis. plot likelihood curves 0 heads 10 coin flips, likelihood curve looks like Figure 3.2.\nFigure 3.2: Binomial likelihood function 0 successes 10 trials.\nLikelihoods can easily combined. Imagine two people flipping coin independently. One person observes 8 heads 10 flips, observes 4 heads 10 flips. might believe give likelihood curve one person flipping coin 20 times, observing 12 heads, indeed, . plot , likelihood curves standardized dividing curve maximum likelihood curve. curves now maximum 1, can easily compare different likelihood curves.\nFigure 3.3: Combining likelihoods.\ncurve left 4 10 heads, one right 8 10 heads. black dotted curve middle 12 20 heads. grey curve, directly beneath 12 20 heads curve, calculated multiplying likelihood curves: \\(L(p_{combined}) = L(p = 0.8) / L(p = 0.4)\\).Figure 3.4 see likelihood curves 10, 100, 1000 coin flips, yield 5, 50, 500 heads, respectively. likelihood curves standardized make easily comparable. sample size increases, curves become narrow (dashed line n = 10, dotted line n = 100, solid line n = 1000). means sample size increases, data become increasingly less likely population parameters removed observed number heads. words, collected increasingly strong evidence p = 0.5, compared possible population parameters.\nFigure 3.4: Likelihood function 5/10, 50/100 500/1000 heads coin flips.\n","code":"\nfactorial(10)/(factorial(8)*(factorial(10-8))) * 0.5^8 * (1 - 0.5)^(10-8)\ndbinom(x = 2, size = 10, prob = 0.5)"},{"path":"likelihoods.html","id":"likelihood-ratios","chapter":"3 Likelihoods","heading":"3.1 Likelihood ratios","text":"can use likelihood function compare possible values p. example, might believe coin flipped fair, even though flipped eight ten heads. fair coin p = 0.5, observed p = 0.8. likelihood function allows us compute relative likelihood different possible parameters. much likely observed data hypothesis unfair coin average turn heads 80% time, compared alternative theory fair coin turn heads 50% time?can calculate likelihood ratio:\\[\n\\frac{L(p = 0.8)}{L(p = 0.5)}\n\\]0.302/0.044 = 6.87. plot, circles show points \nlikelihood curve L(p = 0.5) L(p = 0.8).\nFigure 3.5: Computing likelihood ratio p = 0.5 relative p = 0.8 observing p = 0.8.\ncan subjectively interpret likelihood ratio, tells us observed data 6.87 times likely hypothesis coin unfair turn heads 80% time, hypothesis fair coin. convincing ? Let’s round likelihood ratio 7, imagine two bags marbles. One bag contains 7 blue marbles. second contains 7 marbles, one different color rainbow, violet, indigo, blue, green, yellow, orange, red. Someone randomly picks one two bags, draws marble, shows . marble blue: certain marble came bag blue marbles, compared bag rainbow coloured marbles? strongly likelihood ratio tells us believe data generated unfair coin turns heads 80% time, relative fair coin, given observed 8 heads 10 tosses. explanation, intended make rely much benchmarks, might still useful know Royall (1997) considered likelihood ratios 8 moderately strong evidence, likelihood ratios 32 strong evidence.Note likelihood ratios give us relative evidence one specified hypothesis, another specified hypothesis. likelihood ratio can calculated two hypothesized values. example, Figure 3.6 , likelihood ratio calculated compares hypothesis fair coin (p = 0.5) alternative hypothesis coin comes heads 80% time (p = 0.8), observed 4 heads 10 coin flips. see observed data 0.2050/0.0055=37.25 times likely (ignoring rounding differences – try calculate numbers hand using formula provided earlier) hypothesis fair coin hypothesis coin turns heads 80% time.\nFigure 3.6: Computing likelihood ratio p = 0.5 relative p = 0.8 observing p = 0.4.\nlikelihood ratio 1 means data equally likely hypotheses. Values away 1 indicate data likely one hypothesis . ratio can expressed favor one hypothesis (example L(p = 0.5)/L(p = 0.8) vice versa (L(p = 0.8)/L(p = 0.5). means likelihood ratio 37.25 \\(H_0\\) relative \\(H_1\\) equivalent likelihood ratio 1/37.25 = 0.02685 \\(H_1\\) relative \\(H_0\\). Likelihood ratios range 0 infinity, closer zero infinity, stronger relative evidence one hypothesis . see chapter Bayesian statistics likelihood ratios sense similar (special case ) Bayes Factor.Likelihoods relative evidence. Just data likely one possible value p another value p doesn’t mean data come either two distributions. values might generate even higher likelihood values. example, consider situation flip coin 100 times, observe 50 heads. compare p = 0.3 versus p = 0.8, find likelihood ratio 803462, implying 803461 times evidence data p = 0.3 p = 0.8. might sound pretty conclusive evidence p = 0.3. relative evidence p = 0.3 compared p = 0.8. look likelihood function, clearly see , surprisingly, p = 0.5 value maximizes likelihood function. Just one hypothesis likely another hypothesis, mean third hypothesis even likely.\nFigure 3.7: Computing likelihood ratio p = 0.3 relative p = 0.8 observing p = 0.5 100 coin flips.\n","code":""},{"path":"likelihoods.html","id":"mixedresults","chapter":"3 Likelihoods","heading":"3.2 Likelihood of mixed results in sets of studies","text":"Science cumulative process, evaluate lines research, single studies. One big problem scientific literature nonsignificant results often never published (Fanelli, 2010; Franco et al., 2014). time, statistical power hypothesis tests never 100% (often much lower), mathematical reality unlikely (“good true”) set multiple studies yields exclusively significant results. (Francis, 2014; Schimmack, 2012). can use binomial likelihoods examine likely observe mixed results, understand mixed results nevertheless strong evidence presence effect. following largely based Lakens & Etz (2017).probability observing significant nonsignificant result study depends Type 1 error rate (\\(\\alpha\\)), statistical power test (1-\\(\\beta\\)), probability null hypothesis true (Wacholder et al., 2004). four possible outcomes study: true positive, false positive, true negative, false negative. \\(H_0\\) true, probability observing false positive depends \\(\\alpha\\) level Type 1 error rate (e.g., 5%). \\(H_1\\) true, probability observing true positive depends statistical power performed test (often recommended minimum 80%), turn depends \\(\\alpha\\) level, true effect size, sample size. \\(\\alpha\\) level 5%, \\(H_0\\) true, false positive occur 5% probability (long error rates controlled, e.g., preregistered studies) true negative occur 95% probability. test 80% power, \\(H_1\\) true, true positive probability 80%, false negative probability 20%.perform multiple studies, can calculate binomial probability observe specific number significant non-significant findings (Ioannidis & Trikalinos, 2007). can calculate probability finding exactly two significant results three studies assuming null hypothesis true. \\(H_0\\) true, probability significant results equals \\(\\alpha\\) level, thus \\(\\alpha\\) level carefully controlled (e.g., preregistered studies) probability observing significant result (p) = 0.05. , k = 2, n = 3, p = .05, binomial probability function tells us probability finding exactly two significant results three studies 0.007 (0.05 × 0.05 × 0.95 = 0.002375, three orders two three results can observed, 0.002375 × 3 = 0.007).calculate likelihood assuming \\(H_1\\) true, need make assumption power study. Let’s provisionally assume studies powered 80% thus p = .80. probability observing exactly two significant results three studies, assuming power 0.8, 0.384 (0.8 × 0.8 × 0.2 = 0.128, three orders two three results can significant, 0.128 × 3 = 0.384). words, set perform 3 studies, hypothesis correct, test hypothesis 80% power, 38.4% probability observing 2 3 significant results, 9.6% probability observe 1 3 significant results (extremely unlucky individual, 0.8% probability finding significant results three studies, even though true effect). Unless power extremely high, mixed results expected sets studies.likelihoods p = .05 p = .80 highlighted Figure 3.8 circles dotted vertical lines.can use likelihood data assuming \\(H_0\\) \\(H_1\\) true calculate likelihood ratio, 0.384/0.007 = 53.89, tells us observed outcome exactly two significant results three studies 53.89 times likely \\(H_1\\) true studies 80% power, \\(H_0\\) true studies carefully controlled 5% Type 1 error rate. Likelihood ratios 8 32 proposed benchmarks moderately strong strong evidence, respectively (Royall, 1997), implies finding two significant results three studies considered strong evidence \\(H_1\\), assuming 80% power. Shiny app perform calculations available .\nFigure 3.8: Computing likelihood ratio two three significant results, assuming alpha 5% 80% power.\nsets studies, likelihood ratio favor \\(H_1\\) versus \\(H_0\\) observing mix significant nonsignificant findings can become surprisingly large. Even though evidence appears mixed, actually strong evidence favor true effect. example, researcher performs six studies 80% power 5% alpha level finds three significant outcomes three nonsignificant outcomes, cumulative likelihood ratio convincingly large 38--1 favor \\(H_1\\) consider set studies strong evidence true effect. Intuitively, researchers\nmight feel convinced set studies three six results statistically significant. math, see set studies can strong evidence favor true effect. better understanding probabilities might important step mitigating negative effects publication bias.Hopefully, researchers become inclined submit nonsignificant findings publication better understanding evidential value lines research mixed results. Publishing performed studies lines research reduce publication bias, increase informational value data scientific literature. Expecting studies lines research statistically significant reasonable, important researchers develop realistic expectations draw meaningful inferences lines research. don’t good feeling real patterns studies look like, continuously exposed scientific literature reflect reality. Almost multiple study papers scientific literature present statistically significant results, even though unlikely given power studies, probability study correct predictions (Scheel, Schijen, et al., 2021). Educating researchers binomial probabilities likelihood ratios straightforward way develop realistic expectations research lines contain evidential value favor \\(H_1\\) actually look like.","code":""},{"path":"likelihoods.html","id":"likettest","chapter":"3 Likelihoods","heading":"3.3 Likelihoods for t-tests","text":"far computed likelihoods binomial probabilities, likelihoods can computed statistical model (Glover & Dixon, 2004; Pawitan, 2001). example, can compute relative likelihood observing t-value null alternative hypothesis (Figure 3.9). course, observed data likely assume observed effect equals true effect, examining likelihood reveals many alternative hypotheses relatively likely null hypothesis. also holds observing nonsignificant results, can likely alternative hypothesis interest, null hypothesis. reason incorrect say effect p > \\(\\alpha\\) (see p-value misconception 1).\nFigure 3.9: Likelihood ratio observed t-value \\(H_0\\) \\(H_1\\).\n","code":""},{"path":"likelihoods.html","id":"test-yourself-2","chapter":"3 Likelihoods","heading":"3.4 Test Yourself","text":"","code":""},{"path":"likelihoods.html","id":"questions-about-likelihoods","chapter":"3 Likelihoods","heading":"3.4.1 Questions about likelihoods","text":"Q1: Let’s assume expect fair coin. binomial probability observing 8 heads 10 coin flips, p = 0.5? (can use functions chapter, compute hand).0.0440.050.50.8Q2: likelihood curve rises falls , except extremes, 0 heads heads observed. Copy code (remember can click 'clipboard' icon top right code section copy code clipboard), plot likelihood curves 0 heads (x <- 0) 10 flips (n <- 10) running script. likelihood curve look like?likelihood curve horizontal line.script returns error message: possible plot likelihood curve 0 heads.curve starts highest point p = 0, likelihood decreases p increases.curve starts lowest point p = 0, likelihood increases p increases.Q3: Get coin wallet. Flip 13 times, count number heads. Using code , calculate likelihood observed results hypothesis coin fair, compared hypothesis coin fair. Set number successes (x) number heads observed. Change \\(H_1\\) number heads observed (leave 0 didn’t observe heads !). can just use 4/13, enter 0.3038. Leave \\(H_0\\) 0.5. Run script calculate likelihood ratio. likelihood ratio fair compared non-fair coin (\\(H_0\\)/\\(H_1\\)) flips heads often observed, based observed data? Round answer 2 digits decimal.Q4: Earlier mentioned increasing sample sizes, collected stronger relative evidence. Let’s say want compare L(p = 0.4) L(p = 0.5). likelihood ratio \\(H_1\\) 0.4, \\(H_0\\) 0.5, flip 5 heads 10 trials? two possible ways calculate likelihood ratio (\\(H_1\\)/\\(H_0\\) \\(H_0\\)/\\(H_1\\)), report likelihood ≥ 1, round 2 digits decimal point.Q5: likelihood ratio \\(H_1\\) 0.4, \\(H_0\\) 0.5, flip 50 heads 100 trials? two possible ways calculate likelihood ratio (\\(H_1\\)/\\(H_0\\) \\(H_0\\)/\\(H_1\\)), report likelihood ≥ 1, round 2 digits decimal point.Q6: likelihood ratio \\(H_1\\) 0.4, \\(H_0\\) 0.5, flip 500 heads 1000 trials? two possible ways calculate likelihood ratio (\\(H_1\\)/\\(H_0\\) \\(H_0\\)/\\(H_1\\)), report likelihood ≥ 1, round 2 digits decimal point.Q7: comparing two hypotheses (p = X vs p = Y), likelihood ratio :0.02 means enough evidence data either two hypotheses.5493 means hypothesis p = X supported data.5493 means hypothesis p = X much supported data p = Y.0.02 means hypothesis data 2% likely hypothesis p = X hypothesis p = Y.","code":"\nn <- 10 # set total trials\nx <- 5 # set successes\nH0 <- 0.5 # specify one hypothesis you want to compare\nH1 <- 0.4 # specify another hypothesis you want to compare\ndbinom(x, n, H0) / dbinom(x, n, H1) # Returns the H0/H1 likelihood ratio\ndbinom(x, n, H1) / dbinom(x, n, H0) # Returns the H1/H0 likelihood ratio\n\ntheta <- seq(0, 1, len = 100) # create probability variable from 0 to 1\nlike <- dbinom(x, n, theta)\n\nplot(theta, like, type = \"l\", xlab = \"p\", ylab = \"Likelihood\", lwd = 2)\npoints(H0, dbinom(x, n, H0))\npoints(H1, dbinom(x, n, H1))\nsegments(H0, dbinom(x, n, H0), x / n, dbinom(x, n, H0), lty = 2, lwd = 2)\nsegments(H1, dbinom(x, n, H1), x / n, dbinom(x, n, H1), lty = 2, lwd = 2)\nsegments(x / n, dbinom(x, n, H0), x / n, dbinom(x, n, H1), lwd = 2)\ntitle(paste(\"Likelihood Ratio H0/H1:\", round(dbinom(x, n, H0) / dbinom(x, n, H1), digits = 2), \" Likelihood Ratio H1/H0:\", round(dbinom(x, n, H1) / dbinom(x, n, H0), digits = 2)))"},{"path":"likelihoods.html","id":"questions-about-mixed-results","chapter":"3 Likelihoods","heading":"3.4.2 Questions about mixed results","text":"Q8: statement correct perform 3 studies?\\(H_1\\) true, alpha = 0.05, power = 0.80, almost likely observe one non-significant results (48.8%) observe significant result (51.2%).alpha = 0.05 power = 0.80, extremely rare find 3 significant results (0.0125%), regardless whether \\(H_0\\) true \\(H_1\\) true.alpha = 0.05 power = 0.80, 2 3 statistically significant results likely outcome possible outcomes (0 3, 1 3, 2 3, 3 3), occurs 38.4% time \\(H_1\\) true.alpha = 0.05 power = 0.80, probability finding least one false positive (significant result \\(H_0\\) true) three studies 5%.Q9: Sometimes lines three studies, ’ll find significant effect one study, effect two related studies. Assume two related studies exactly every way (e.g., changed manipulation, procedure, questions). two studies work minor differences effect fully understand yet. single significant result Type 1 error, \\(H_0\\) true three studies. statement correct, assuming 5% Type 1 error rate 80% power?else equal, probability Type 1 error one three studies 5% true effect three studies, probability finding exactly 1 three significant effects, assuming 80% power three studies, 80%, substantially likely.else equal, probability Type 1 error one three studies 13.5% true effect three studies, probability finding exactly 1 three significant effects, assuming 80% power three studies (thus true effect), 9.6%, slightly, substantially less likely.else equal, probability Type 1 error one three studies 85.7% true effect three studies, probability finding exactly 1 three significant effects, assuming 80% power three studies (thus true effect) (thus true effect), 0.8%, substantially less likely.possible know probability observe Type 1 error perform 3 studies.idea studies 80% power slightly optimistic. Examine correct answer previous question across range power values (e.g., 50% power, 30% power).Q10: Several papers suggest reasonable assumption power psychological literature might around 50%. Set number studies 4, number successes also 4, assumed power slider 50%, look table bottom app. likely observe 4 significant results 4 studies, assuming true effect?6.25%12.5%25%37.5%Imagine perform 4 studies, 3 show significant result. Change numbers online app. Leave power 50%. output text tells :observed results equally likely \\(H_0\\) \\(H_1\\), likelihood ratio 1. Benchmarks interpret Likelihood Ratios suggest 1<LR<8 weak evidence, 8<LR<32 moderate evidence, LR>32, strong evidence.data likely alternative hypothesis null hypothesis likelihood ratio 526.32These calculations show , assuming observed three significant results four studies, assuming study 50% power, 526 times likely observed data alternative hypothesis true, null hypothesis true. words, 526 times likely find significant effect three studies 50% power, find three Type 1 errors set four studies.Q11: Maybe don’t think 50% power reasonable assumption. low can power (rounded 2 digits), likelihood remain higher 32 favor \\(H_1\\) observing 3 4 significant results?5% power17% power34% power44% powerThe main take home message calculations understand 1) mixed results supposed happen, 2) mixed results can contain strong evidence true effect, across wide range plausible power values. app also tells much evidence, rough dichotomous way, can expect. useful educational goal. want evaluate results multiple studies, formal way performing meta-analysis.calculations make important assumption: Type 1 error rate controlled 5%. try many different tests study, report result yielded p < 0.05, calculations longer hold.Q12: Go back default settings 2 3 significant results, now set Type 1 error rate 20%, reflect modest amount p-hacking. circumstances, highest likelihood favor \\(H_1\\) can get explore possible values true power?Approximately 1Approximately 4.63Approximately 6.70Approximately 62.37As scenario shows, p-hacking makes studies extremely uninformative.\ninflate error rate, quickly destroy evidence data. can longer determine whether data likely effect, effect. Sometimes researchers complain people worry p-hacking try promote better Type 1 error control missing point, things (better measurement, better theory, etc.) important. fully agree aspects scientific research least important better error control. better measures theories require decades work. Better error control can accomplished today, researchers stop inflating error rates flexibly analyzing data. assignment shows, inflated rates false positives quickly make difficult learn true data collect. relative ease part scientific research can improved, can achieve today (decade) think worth stressing importance error control, publish realistic looking sets studies.Q13: ‘prestigious’ journals (, examined terms scientific quality reproducibility, reporting standards, policies concerning data material sharing, quite low quality despite prestige) publish manuscripts large number studies, statistically significant. assume average power psychology 50%, 3.125% 5 study articles contain exclusively significant results. pick random issue prestigious journal, see 10 articles, reporting 5 studies, manuscripts exclusively significant results, trust reported findings , less, articles reported mixed results? ?Q14: Unless power studies 99.99% rest career (slightly inefficient, great don’t like insecurity), observe mixed results lines research. plan deal mixed results lines research?","code":""},{"path":"likelihoods.html","id":"open-questions-2","chapter":"3 Likelihoods","heading":"3.4.3 Open Questions","text":"difference probability likelihood?difference probability likelihood?important remember likelihood ratio relative evidence?important remember likelihood ratio relative evidence?compare 2 hypotheses, H0 H1, likelihood ratio H1 compared H0 77, mean?compare 2 hypotheses, H0 H1, likelihood ratio H1 compared H0 77, mean?benchmarks medium strong evidence according Royall?benchmarks medium strong evidence according Royall?can likelihood ratio 200, hypotheses incorrect?can likelihood ratio 200, hypotheses incorrect?perform multiple studies find 2 3 studies show significant results, can actually strong evidence H1?perform multiple studies find 2 3 studies show significant results, can actually strong evidence H1?","code":""},{"path":"bayes.html","id":"bayes","chapter":"4 Bayesian statistics","heading":"4 Bayesian statistics","text":"\"Logic!\" said Professor half . \"teach logic schools? three possibilities. Either sister telling lies, mad, telling truth. know tell lies obvious mad. moment unless evidence turns , must assume telling truth.\"Lion, Witch, Wardrobe. Story Children C. S. Lewis.children's book Lion, Witch, Wardrobe, Lucy Edmund go wardrobe country called Narnia. Lucy tells older brother sister, Peter Susan, Narnia, Edmund wants keep secret, tells Peter Susan Lucy just pretending Narnia exists. Peter Susan know believe - Narnia exist, ? Lucy telling truth, Edmund? Thinking probabilities long run help much - unique event, need think probability Narnia exists, , based information available.ask Professor, lives house wardrobe, advice. Professor asks Susan Peter past experience, Lucy Edward truthful, Peter answers \"till now, said Lucy every time.\" , stronger prior belief Lucy telling truth, relative Edward telling truth. Professor replies quote . three possible options, believe Lucy lying, done past, Professor believes clear just talking Lucy mad. Therefore, plausible option Lucy telling truth. new evidence uncovered, beliefs can updated future. approach knowledge generation, prior probability different hypotheses quantified, possible updated light new data, example Bayesian inference.Although frequentist statistics far dominant approach science, important least rudimentary exposure Bayesian statistics statistics training. Bayesian statistics especially useful inferences made cases data investigation unique, frequentist probability defined limit many trials. example, question might often Lucy lies average, whether Lucy lying specific instance existence Narnia. research, often start prior belief hypothesis true. collecting data, can use data update prior beliefs. Bayesian statistics allows update prior beliefs posterior probabilities logically consistent manner. collected data, prior odds Hypothesis 1 (\\(H_1\\)) null-hypothesis (\\(H_0\\)) P(\\(H_1\\))/P(\\(H_0\\)), collected data, posterior odds P(\\(H_1\\)|D)/P(\\(H_0\\)|D), can read probability \\(H_1\\), given data, divided probability \\(H_0\\), given data. different approaches Bayesian statistics. first discuss Bayes factors, Bayesian estimation.","code":""},{"path":"bayes.html","id":"bayes-factors","chapter":"4 Bayesian statistics","heading":"4.1 Bayes factors","text":"One approach Bayesian statistics focuses comparison different models might explain data (referred model comparison). Bayesian statistics, probability data specified model (P|D(\\(H_0\\)) number expressed sometimes referred absolute evidence, formally referred marginal likelihood. marginal likelihood uses prior probabilities average likelihood across parameter space. example, assume simple model M based single parameter, can take two values, X Y, -prior believe probability values p(X) = 0.4 p(Y) = 0.6. collect data, calculate likelihood parameter values, p(D|X) = 0.02 p(D|Y) = 0.08. marginal likelihood model M P(D|M) = 0.4 × 0.02 + 0.6 × 0.08 = 0.056. often, models continuously varying parameters, marginal likelihood formula based integral, idea remains .comparison two models based relative evidence data provides models comparing. relative evidence calculated dividing marginal likelihood one model marginal likelihood another model, ratio relative evidence based marginal likelihoods called Bayes factor. Bayes factors Bayesian equivalent hypothesis tests (Dienes, 2008; Kass & Raftery, 1995). Bayes factor represents much updated beliefs, based observing data. can express Bayes factors indicate much likely \\(H_1\\) given data compared \\(H_0\\) (often indicated B10) much likely \\(H_0\\) become compared \\(H_1\\) (B01), B10 = 1/B01. Similar likelihood ratios 1, Bayes factor 1 change beliefs one model compared model. large Bayes factor \\(H_1\\) \\(H_0\\) increased belief \\(H_1\\), Bayes Factor close \\(H_1\\) \\(H_0\\) 0 increased belief \\(H_0\\). prior belief \\(H_1\\) , low (e.g., belief unicorns) even large Bayes factor supports presence unicorn might yet convince unicorns real – updated belief unicorns, now believe least likely (even still think unicorns unlikely exist). contribution Bayes Factor prior calculating posterior odds clear following formula:\\[\n\\frac{P(H_1|D)}{P(H_0|D)} = \\ \\frac{P(D|H_1)}{P(D|H_0)}\\  \\times \\ \\frac{P(H_1)}{P(H_0)}\n\\]\\[\nPosterior\\ Probability = \\ Bayes\\ Factor\\  \\times \\ Prior\\ Probability\n\\]Bayesian analysis data requires specifying prior. , continue example based binomial probability, coin flip. likelihood example, compared two point hypotheses (e.g., p = 0.5 vs. p = 0.8). Bayesian statistics, parameters considered random variables, uncertainty degree belief respect parameters quantified probability distributions.binomial probability lies 0 1. draw probability density want 0 1, turn prior, good reasons (simplicity, mostly) beta-prior often used binomial probabilities. shape beta-prior depends two parameters, \\(\\alpha\\) \\(\\beta\\). Note Greek letters used Type 1 error rate Type 2 error rate, purely coincidental! \\(\\alpha\\) \\(\\beta\\) binomial probabilities unrelated error rates, use letters mainly due lack creativity among statisticians limited choice alphabet gives us. also help \\(\\beta\\) one parameters Beta distribution. Try keep different Beta’s apart! probability density function :\\[\nf{}^{}{\\left(x;\\ \\alpha,\\ \\beta \\right) = \\ \\frac{1}{B(\\alpha,\\beta)}}x^{\\alpha - 1}{(1 - x)}^{\\beta - 1}\n\\]B(\\(\\alpha\\), \\(\\beta\\)) beta function. Understanding mathematical basis function beyond scope chapter, can read Wikipedia Kruschke's book Bayesian Data Analysis (Kruschke, 2014). beta-prior variety values \\(\\alpha\\) \\(\\beta\\) can seen figure .\nFigure 4.1: Four examples Bayesian priors.\nbeta densities reflect different types priors. Let’s assume approached street merchant tries sell special coin heads tails , flipped, almost always turn heads. \\(\\alpha\\) = 1, \\(\\beta\\) = 1 prior newborn baby prior, without idea expect flip coin, thus every value p equally likely. \\(\\alpha\\) = 1, \\(\\beta\\) = 1/2 prior true believer prior. sales merchant tells coin turn heads almost every time, thus, believe turn heads almost every time. \\(\\alpha\\) = 4, \\(\\beta\\) = 4, \\(\\alpha\\) = 100, \\(\\beta\\) = 100 priors slightly extremely skeptical people. \\(\\alpha\\) = 4, \\(\\beta\\) = 4 prior, expect coin fair, willing believe wide range true values possible (curve centered 0.5, curve wide, allowing high low values p). \\(\\alpha\\) = 100, \\(\\beta\\) = 100 prior really convinced coins fair, believe slight bias, (curve centered 0.5, skeptic believes p lie 0.4 0.6 – much narrower range compared slightly skeptic individual).Let’s assume newborn baby, true believer, slightly skeptic extreme skeptic buy coin, flip n = 20 times, observe x = 10 heads. outcome can plotted binomial distribution 10 heads 20 trials, Beta(11, 11) distribution.newborn baby prior Beta distribution \\(\\alpha\\) = 1 \\(\\beta\\) = 1, equals binomial likelihood distribution 0 heads 0 trials. posterior Beta distribution Beta(\\(\\alpha\\)*, \\(\\beta\\)*), :\\(\\alpha\\)* = \\(\\alpha\\) + x = 1 + 10= 11\\(\\beta\\)* = \\(\\beta\\) + n – x = 1 + 20 – 10 = 11Or calculating values directly \\(\\alpha\\) \\(\\beta\\) prior \nlikelihood:\\(\\alpha\\)* = \\(\\alpha\\)prior + \\(\\alpha\\)likelihood – 1 = 1 + 11 - 1= 11\\(\\beta\\)* = \\(\\beta\\)prior + \\(\\beta\\)likelihood - 1 = 1 + 11 – 1 = 11Thus, posterior distribution newborn Beta(11,11) distribution. equals binomial likelihood function 10 heads 20 trials, Beta(11,11) distribution. words, posterior distribution identical likelihood function uniform prior used.Take look Figure . Given 10 heads 20 coin flips, see prior distribution newborn (horizontal grey line), likelihood (blue dotted line) posterior (black line).\nFigure 4.2: Four examples different priors updated based data posterior.\ntrue believer posterior distribution centered maximum likelihood observed data, just bit direction prior. slightly skeptic strong skeptic end much stronger belief fair coin observing data, mainly already stronger prior coin fair.","code":""},{"path":"bayes.html","id":"updating-our-belief","chapter":"4 Bayesian statistics","heading":"4.2 Updating our belief","text":"Now distribution prior, distribution posterior, can see graphs values p belief increased. Everywhere black line (posterior) higher grey line (prior) belief p increased.\nFigure 4.3: Plot prior, likelihood, posterior.\nBayes Factor used quantify increase relative evidence. Let’s calculate Bayes Factor hypothesis coin fair newborn. Bayes Factor simply value posterior distribution p = 0.5, divided value prior distribution p = 0.5:BF10 = Beta(p = 0.5, 11, 11)/Beta(p = 0.5, 1, 1) = 3.70/1 = 3.70We can calculate plot Bayes Factor, show prior (grey), likelihood (dashed blue) posterior (black). example 20 flips, 10 heads, newborn prior, plot looks like :\nFigure 4.4: Plot prior, likelihood, posterior.\nsee newborn, p = 0.5 become probable, p = 0.4. Now let’s assume strong skeptic, believes coin fair prior Beta(100, 100), buys coin flips 100 times. Surprisingly, coin comes heads 90 100 flips. plot prior, likelihood, posterior now looks much extreme, informed prior, extremely different data. see grey prior distribution, dashed blue likelihood based data, posterior distribution black. Bayes Factor 0 (note value rounded, extremely small, exactly zero) - represents substantial drop belief coin fair – indeed, now seems untenable hypothesis, even strong skeptic. shows data can update belief. newborn now completely believe true p coin somewhere around 0.9, strong skeptic reason believe p around 0.65, due strong prior conviction coin fair. Given enough data, even strong skeptic become convinced coin return heads time well.\nFigure 4.5: Plot prior, likelihood, posterior.\ncan now also see difference likelihood inference approach, Bayesian inference approach. likelihood inference, can compare different values p likelihood curve (e.g., p = 0.5 vs p = 0.8) calculate likelihood ratio. Bayesian inference, can compare difference prior posterior value p, calculate Bayes Factor.never seen Bayes Factors , might find difficult interpret numbers. guideline (e.g., interpreting effect sizes small, medium, large) criticism use benchmarks. hand, start somewhere getting feel Bayes Factors mean. Bayes factor 1 3 considered ‘worth bare mention’, larger 3 (smaller 1/3) considered ‘substantial’, larger 10 (smaller 1/10) considered ‘strong’ (Jeffreys, 1939). labels refer increase much believe specific hypothesis, posterior belief hypothesis. think extra-sensory perception extremely implausible, single study BF = 14 increase belief, now think extra-sensory perception pretty much extremely implausible.Bayes factors often promoted alternative p-values. One stated benefit can provide support alternative, null (Dienes, 2014). However, can achieved frequentist equivalence tests, see chapter equivalence testing, inferences based Bayes factors equivalence tests typically lead conclusions (Lakens et al., 2020). Another reason people give switch Bayes factors instead p-values , saw chapter p-values, p-values often misunderstood. However, surprisingly, Bayes factors least often misunderstood misused (Wong et al., 2022). Statistical inferences hard, thinking probabilities something get right trusting intuition. need train draw correct inferences, switching different statistic prevent misuse.","code":""},{"path":"bayes.html","id":"bayesest","chapter":"4 Bayesian statistics","heading":"4.3 Bayesian Estimation","text":"posterior distribution summarizes belief expected number heads flipping coin seeing data, averaging prior beliefs data (likelihood). mean Beta distribution can calculated \\(\\alpha\\)/(\\(\\alpha\\)+\\(\\beta\\)). can thus easily calculate mean posterior distribution, expected value based prior beliefs data.can also calculate credible interval around mean, Bayesian version confidence interval slightly different interpretation. Instead Frequentist interpretation parameter one (unknown) true value, Bayesian approach considers data fixed, allow parameter vary. Bayesian approaches, probability distributions represent degree belief. calculating credible interval, one saying ‘believe 95% probable (given prior data) true parameter falls within credible interval’. 95% credible interval simply area posterior distribution 0.025 0.975 quantiles.credible interval confidence interval , uniform prior (e.g., Beta(1,1)) used. case, credible interval numerically identical confidence interval. interpretation differs. Whenever informed prior used, credible interval confidence interval differ. chosen prior representative truth, credible interval representative truth, always correct formalization beliefs. single confidence interval, probability contains true population parameter either 0 1. long run 95% confidence intervals contain true population parameter. important differences Bayesian credible intervals Frequentist confidence intervals keep mind.can plot mean posterior 10 heads 20 coin flips observed, given uniform prior.\nFigure 4.6: Plot mean posterior 10 20 heads observed given uniform prior.\ncan also use ‘binom’ package calculate posterior mean, credible interval, highest density interval (HDI). highest density interval alternative credible interval works better posterior beta distribution skewed (identical posterior distribution symmetrical. won’t go calculations HDI .posterior mean identical Frequentist mean, case mean prior equals mean likelihood (Albers et al., 2018). research, likely need calculations binomial example used , lot Bayesian tests now available free open source software package JASP. math priors become complex, basic idea remains . can use Bayesian statistics quantify relative evidence, can inform much believe, update beliefs, theories.chapter showed essence Bayesian inference, decide upon prior distribution, collect data calculate marginal likelihood, use calculate posterior distribution. posterior distribution, can estimate mean 95% credible interval. specific hypothesis, can calculate relative evidence posterior model, compared prior model, Bayes Factor. many different flavors Bayesian statistics. means disagreements among Bayesians best approach statistical inferences , least vehement disagreements frequentists Bayesians. example, many Bayesians dislike Bayes factors (McElreath, 2016). Bayesians dislike subjective priors used subjective Bayesian analysis, instead prefer known objective Bayesian analysis (Berger & Bayarri, 2004). Teaching material Bayesian statistics often present superior frequentist statistics. balanced educational lecture Bayesian vs. frequentist statistics honestly highlights strengths weaknesses approach, see first 50 minutes lecture Michael . Jordan.","code":"\nlibrary(binom)\n\nn <- 20 # set total trials\nx <- 10 # set successes\naprior <- 1 # Set the alpha for the Beta distribution for the prior\nbprior <- 1 # Set the beta for the Beta distribution for the prior\n\nbinom.bayes(x, n, type = \"central\", prior.shape1 = aprior, prior.shape2 = bprior)\nbinom.bayes(x, n, type = \"highest\", prior.shape1 = aprior, prior.shape2 = bprior)"},{"path":"bayes.html","id":"test-yourself-3","chapter":"4 Bayesian statistics","heading":"4.4 Test Yourself","text":"Q1: true believer prior Beta(1,0.5). observing 10 heads 20 coin flips, posterior distribution, given α* = α + x β* = β + n – x?Beta(10, 10)Beta(11, 10.5)Beta(10, 20)Beta(11, 20.5)Q2: strong skeptic prior Beta(100,100). observing 50 heads 100 coin flips, posterior distribution, given α* = α + x β* = β + n – x?Beta(50, 50)Beta(51, 51)Beta(150, 150)Beta(151, 151)Copy R script R. script requires 5 input parameters (identical Bayes Factor calculator website used ). hypothesis want examine (e.g., evaluating whether coin fair, p = 0.5), total number trials (e.g., 20 flips), number successes (e.g., 10 heads), \\(\\alpha\\) \\(\\beta\\) values Beta distribution prior (e.g., \\(\\alpha\\) = 1 \\(\\beta\\) = 1 uniform prior). Run script. calculate Bayes Factor, plot prior (grey), likelihood (dashed blue) posterior (black).see newborn, p = 0.5 become probable, p = 0.4.Q3: Change hypothesis first line 0.5 0.675, run script. testing idea coin returns 67.5% heads, statement true?belief hypothesis, given data, decreased.belief hypothesis, given data, stayed .belief hypothesis, given data, increased.Q4: Change hypothesis first line back 0.5. Let’s look increase belief hypothesis p = 0.5 strong skeptic 10 heads 20 coin flips. Change \\(\\alpha\\) prior line 4 100 \\(\\beta\\) prior line 5 100. Run script. Compare Figure R increase belief newborn (plot previous page). statement true?belief hypothesis p = 0.5, given data, increased strong skeptic, much newborn.belief hypothesis p = 0.5, given data, increased strong skeptic, exactly much newborn.belief hypothesis p = 0.5, given data, increased strong skeptic, much newborn.belief hypothesis p = 0.5, given data, decreased strong skeptic.Copy R script run . script plot mean posterior 10 heads 20 coin flips observed, given uniform prior (4.6) . script also use ‘binom’ package calculate posterior mean, credible interval, highest density interval (HDI). highest density interval alternative credible interval works better posterior beta distribution skewed (identical posterior distribution symmetrical. won’t go calculations HDI .posterior mean identical Frequentist mean, case mean prior equals mean likelihood.Q5: Assume outcome 20 coin flips 18 heads. Change x 18 line 2 run script. Remember mean prior Beta(1,1) distribution α/(α+β), 1/(1+1) = 0.5. Frequentist mean simply x/n, 18/20=0.9. statement true?frequentist mean higher mean posterior, combining prior data, mean posterior closer mean prior distribution.frequentist mean lower mean posterior, combining prior data, mean posterior closer mean prior distribution.frequentist mean higher mean posterior, combining prior data, mean posterior mean prior distribution.frequentist mean lower mean posterior, combining prior data, mean posterior mean prior distribution.Q6: , today, best estimate probability sun rises every day? Assume born uniform Beta(1,1) prior. sun can either rise, . Assume seen sun every day since born, means continuous string successes every day alive. ok estimate days alive just multiplying age 365 days. best estimate probability sun rise?Q7: best estimate Frequentist perspective?Q8: think goal science ? Rozeboom (1960) criticized Neyman-Pearson hypothesis testing stating:primary aim scientific experiment precipitate decisions, make appropriate adjustment degree one accepts, believes, hypothesis hypotheses tested\".Frick (1996) argued Rozeboom, stating:Rozeboom (1960) suggested scientists making decisions claims, calculating updating probability claims. However, seem practical. handful potential claims given area psychology, feasible assign probabilities, constantly updating probabilities, expect experimenters keep track ever-changing probabilities. fact, just number claims psychology overwhelming. probably impossible human beings keep track probability claim, especially probabilities constantly changing. case, scientists assign probabilities claims. Instead, scientists act like goal science collect corpus claims considered established (Giere, 1972).comes philosophy science, right wrong answers. Reflect 250 words thoughts two goals science outlines Rozeboom Frick, relate philosophy science.","code":"\nH0 <- 0.5 # Set the point null hypothesis you want to calculate the Bayes Factor for\nn <- 20 # set total trials\nx <- 10 # set successes\naprior <- 1 # Set the alpha for the Beta distribution for the prior\nbprior <- 1 # Set the beta for the Beta distribution for the prior\n\nalikelihood <- x + 1 # Calculate the alpha for the Beta distribution for the likelihood\nblikelihood <- n - x + 1 # Calculate the beta for the Beta distribution for the likelihood\naposterior <- aprior + alikelihood - 1 # Calculate the alpha for the Beta distribution for the posterior\nbposterior <- bprior + blikelihood - 1 # Calculate the beta for the Beta distribution for the posterior\n\ntheta <- seq(0, 1, 0.001) # create probability range p from 0 to 1\nprior <- dbeta(theta, aprior, bprior)\nlikelihood <- dbeta(theta, alikelihood, blikelihood)\nposterior <- dbeta(theta, aposterior, bposterior)\n\n# Create plot\nplot(theta, posterior, ylim = c(0, 15), type = \"l\", lwd = 3, xlab = \"p\", ylab = \"Density\", las = 1)\nlines(theta, prior, col = \"grey\", lwd = 3)\nlines(theta, likelihood, lty = 2, lwd = 3, col = \"dodgerblue\")\nBF10 <- dbeta(H0, aposterior, bposterior) / dbeta(H0, aprior, bprior)\npoints(H0, dbeta(H0, aposterior, bposterior), pch = 19)\npoints(H0, dbeta(H0, aprior, bprior), pch = 19, col = \"grey\")\nsegments(H0, dbeta(H0, aposterior, bposterior), H0, dbeta(H0, aprior, bprior), lty = 2)\ntitle(paste(\"Bayes Factor:\", round(BF10, digits = 2)))\nn <- 20 # set total trials\nx <- 10 # set successes\naprior <- 1 # Set the alpha for the Beta distribution for the prior\nbprior <- 1 # Set the beta for the Beta distribution for the prior\n\nymax <- 10 # set max y-axis\n\nalikelihood <- x + 1 # Calculate the alpha for the Beta distribution for the likelihood\nblikelihood <- n - x + 1 # Calculate the beta for the Beta distribution for the likelihood\naposterior <- aprior + alikelihood - 1 # Calculate the alpha for the Beta distribution for the posterior\nbposterior <- bprior + blikelihood - 1 # Calculate the beta for the Beta distribution for the posterior\n\ntheta <- seq(0, 1, 0.001) # create probability range p from 0 to 1\nprior <- dbeta(theta, aprior, bprior) # deterine prior distribution\nlikelihood <- dbeta(theta, alikelihood, blikelihood) # determine likelihood distribution\nposterior <- dbeta(theta, aposterior, bposterior) # determine posterior distribution\nplot(theta, posterior, ylim = c(0, ymax), type = \"l\", lwd = 3, xlab = bquote(theta), ylab = \"Density\", las = 1) # draw posterior distribution\nlines(theta, prior, col = \"grey\", lwd = 3) # draw prior distribution\nlines(theta, likelihood, lty = 2, lwd = 3, col = \"dodgerblue\") # draw likelihood distribution\nLL <- qbeta(.025, aposterior, bposterior) # calculate lower limit credible interval\nUL <- qbeta(.975, aposterior, bposterior) # calculate upper limit credible interval\nabline(v = aposterior / (aposterior + bposterior)) # draw line mean\nabline(v = LL, col = \"grey\", lty = 3) # draw line lower limit\nabline(v = UL, col = \"grey\", lty = 3) # draw line upper limit\npolygon(c(theta[theta < LL], rev(theta[theta < LL])), c(posterior[theta < LL], rep(0, sum(theta < LL))), col = \"lightgrey\", border = NA)\npolygon(c(theta[theta > UL], rev(theta[theta > UL])), c(posterior[theta > UL], rep(0, sum(theta > UL))), col = \"lightgrey\", border = NA)\ntitle(paste(\"Mean posterior:\", round((aposterior / (aposterior + bposterior)), digits = 5), \", 95% Credible Interval:\", round(LL, digits = 2), \";\", round(UL, digits = 2)))\nif (!require(binom)) {\n  install.packages(\"binom\")\n}\nlibrary(binom)\nbinom.bayes(x, n, type = \"central\", prior.shape1 = aprior, prior.shape2 = bprior)\nbinom.bayes(x, n, type = \"highest\", prior.shape1 = aprior, prior.shape2 = bprior)"},{"path":"bayes.html","id":"open-questions-3","chapter":"4 Bayesian statistics","heading":"4.4.1 Open Questions","text":"Bayes factor?Bayes factor?difference Bayes factor likelihood ratio?difference Bayes factor likelihood ratio?Bayes factor 1 mean?Bayes factor 1 mean?prior Bayesian inference, possible different people different priors?prior Bayesian inference, possible different people different priors?difference Frequentist confidence interval Bayesian credible interval?difference Frequentist confidence interval Bayesian credible interval?difference uniform informed prior compute posterior distribution?difference uniform informed prior compute posterior distribution?Give definition credible interval.Give definition credible interval.","code":""},{"path":"questions.html","id":"questions","chapter":"5 Asking Statistical Questions","heading":"5 Asking Statistical Questions","text":"core design new study evaluation information quality: potential particular dataset achieving given analysis goal employing data analysis methods considering given utility (Kenett et al., 2016). goal data collection gain information empirical research observations collected analyzed, often statistical models. Three approaches statistical modelling can distinguished Shmueli (2010): Description, explanation, prediction, discussed . utility often depends effects deemed interesting. thorough evaluation information quality study therefore depends clearly specifying goal data collection, statistical modelling approach chosen, usefulness data draw conclusions effects interest chosen analysis method. study low information quality might worth performing, data collected low potential achieve analysis goal.","code":""},{"path":"questions.html","id":"description","chapter":"5 Asking Statistical Questions","heading":"5.1 Description","text":"Description aims answer questions features empirical manifestation phenomenon. Description can involve unique events (e.g., case studies single patients) classes events (e.g., patients certain disease). Examples features interest duration (long), quantity (many), location (), etc.example descriptive question research Kinsey, studied sexual behavior experiences Americans time little scientific research available topic. used interviews provided statistical basis draw conclusions sexuality United States, , time, challenged conventional beliefs sexuality.Descriptive research questions answered estimation statistics. informational value estimation study determined amount observations (observations, higher precision estimates) sampling plan (representative sample, lower sample selection bias, increases ability generalize sample population), reliability measure.Descriptive research questions sometimes seen less exciting explanation prediction questions (Gerring, 2012), essential building block theory formation (Scheel, Tiokhin, et al., 2021). Although estimation question often focus mean score measure, accurate estimates variance measure extremely valuable well. variance measure essential information well-informed sample size justification, planning accuracy, performing -priori power analysis.","code":""},{"path":"questions.html","id":"prediction","chapter":"5 Asking Statistical Questions","heading":"5.2 Prediction","text":"goal predictive modeling apply algorithm statistical model predict future observations (Shmueli, 2010). example, COVID-9 pandemic large number models created combined variables estimate risk people infected COVID, people infected experience negative effects health (Wynants et al., 2020). Ideally, goal develop prediction model accurately captures regularities training data, generalizes well unseen data. bias-variance trade two goals, researchers need decide much bias reduced increases variance, vice-versa (Yarkoni & Westfall, 2017). goal prediction minimize prediction error. common method evaluate prediction errors cross-validation, examined whether model developed training dataset generalizes holdout dataset. development prediction models becoming increasingly popular rise machine learning approaches.","code":""},{"path":"questions.html","id":"explanation","chapter":"5 Asking Statistical Questions","heading":"5.3 Explanation","text":"use statistical models concerns tests explanatory theories. case, statistical models used test causal assumptions, explanations derive theories. Meehl (1990) reminds us important distinction substantive theory, statistical hypothesis, observations. Statistical inference involved drawing conclusions statistical hypothesis. Observations can lead conclusion statistical hypothesis confirmed (), conclusion directly translate corroboration theory. Platt (1964) refers systematic application statistical tests accumulate knowledge strong inference. consists 1) specifying alternative hypotheses, 2) designing experiment can corroborate one hypothesis falsify another, 3) performing experiment. cycle can repeated test number hypotheses one hypothesis can explain observed data remains. Platt notes entertaining multiple alternative hypotheses prevents researchers becoming attached single hypothesis. designing new experiment, researchers ask Platt calls 'Question': \"sir, hypothesis experiment disprove?\".never test theory isolation, always include auxiliary hypotheses measures instruments used study, conditions realized experiment, ceteris paribus clause assumes \"things equal\". best experimental set-can rarely 'deduced' theory, requires premisses tacitly taken granted. Hempel (1966) states: \"Reliance auxiliary hypotheses, shall see, rule rather exception testing scientific hypotheses; important consequence question whether unfavorable test finding, .e., one shows false, can held disprove hypothesis investigation.\" Therefore, never clear failure corroborate theoretical prediction blamed theory auxiliary hypotheses. generate reliable explanatory theories, researchers therefore perform lines research auxiliary hypotheses systematically tested (Uygun Tunç & Tunç, 2022).\nFigure 5.1: Distinction theoretical hypothesis, statistical hypothesis, observations. Figure based Meehl, 1990.\n","code":""},{"path":"questions.html","id":"looseningtightening","chapter":"5 Asking Statistical Questions","heading":"5.4 Loosening and Tightening","text":"three questions , can ask questions description, prediction, explanation loosening phase research, tightening phase (Fiedler, 2004). distinction relative. loosening stage, focus creating variation provides source new ideas. tightening stage, selection takes place goal distinguish useful variants less useful variants. descriptive research, unstructured interview aligned loosening phase, structured interview aligned tightening phase. prediction, building prediction model based training set loosening phase, evaluation prediction error holdout dataset tightening phase. explanation, exploratory experimentation functions generate hypotheses, hypothesis tests function distinguish theories make predictions corroborated theories predictions corroborated.important realize whether goal generate new ideas, test new ideas. Researchers often explicit stage research , runs risk trying test hypotheses prematurely (Scheel, Tiokhin, et al., 2021). Clinical trials research explicit different phases research, distinguishes Phase 1, Phase 2, Phase 3, Phase 4 trials. Phase 1 trial researchers evaluate safety new drug intervention small group non-randomized (often healthy) volunteers, examining much drug safe give, monitoring range possible side effects. phase 2 trial often performed patients participants, can focus detail finding definite dose. goal systematically explore range parameters (e.g., intensity stimulus) identify boundary conditions (Dubin, 1969). phase 3 trial large randomized controlled trial goal test effectiveness new intervention practice. Phase 3 trials require prespecified statistical analysis plan strictly controls error rates. Finally, Phase 4 trial examines long term safety generalizability. Compared Phase 3 trial, loosening, researchers explore possibility interactions drugs, moderating effects certain subgroups population. clinical trials, Phase 3 trial requires huge amount preparation, undertaken lightly.\nFigure 5.2: Four phases clinical research. Source.\n","code":""},{"path":"questions.html","id":"three-statistical-philosophies","chapter":"5 Asking Statistical Questions","heading":"5.5 Three Statistical Philosophies","text":"Royall (1997) distinguishes three questions one can ask:believe, now observation?, now observation?observation tell versus B? (interpret observation evidence regarding versus B?)One useful metaphor thinking differences look Hinduism, three ways reach enlightenment: Bhakti yoga, Path Devotion, Karma yoga, Path Action, Jnana yoga, Path Knowledge. three corresponding statistical paths Bayesian statistics, focuses updating beliefs, Neyman-Pearson statistics, focuses making decisions act, likelihood approaches, focus quantifying evidence knowledge gained data. Just like Hinduism different paths mutually exclusive, emphasis three yoga's differs individuals, scientists differ emphasis preferred approach statistics.three approaches statistical modelling (description, prediction, explanation) can examined three statistical philosophies (e.g., frequentist estimation, maximum likelihood estimation, Bayesian estimation, Neyman-Pearson hypothesis tests, likelihood ratio tests, Bayes factors). Bayesian approaches start specified prior belief, use data update belief. Frequentist procedures focus methodological procedures allow researchers make inferences control probability error long run. Likelihood approaches focus quantifying evidential value observed data. used knowledgeably, approaches often yield similar inferences (Dongen et al., 2019; Lakens et al., 2020; Tendeiro & Kiers, 2019). Jeffreys (1939), developed Bayesian hypothesis test, noted following comparing Bayesian hypothesis test frequentist methods proposed Fisher:fact struck repeatedly work, led general principles solution problem, find Fisher already grasped essentials brilliant piece common sense, results either identical mine differ cases doubtful. matter fact applied significance tests numerous applications also worked Fisher’s, yet found disagreement actual decisions reached.time, approach based different principles, allows specific inferences. example, Neyman-Pearson approach quantify evidence, Bayesian approach can lead conclusions relative support one another hypothesis, given specified priors, ignoring rate conclusion misleading. Understanding basic principles useful, criticisms statistical practices (e.g., computing p-values) always boil disagreement principles different statistical philosophies built . However, survey literature, rarely see viewpoint approaches statistical inferences, including p values, provide answers specific questions researcher might want ask. Instead, statisticians often engage call statistician’s\nfallacy — declaration believe researchers really “want know” without limiting usefulness preferred statistical question specific context (Lakens, 2021). well-known example statistician’s fallacy provided Cohen (1994) discussing null-hypothesis significance testing:’s wrong NHST? Well, among many things, tell us want know, much want know want know , desperation, nevertheless believe ! want know ‘Given data, probability H0 true?’Different statisticians argue actually \"want know\" posterior probability hypothesis, false-positive risk, effect size confidence interval, likelihood, Bayes factor, severity hypothesis tested. However, choose statistical strategy matches question want ask (Hand, 1994).","code":""},{"path":"questions.html","id":"reallytest","chapter":"5 Asking Statistical Questions","heading":"5.6 Do You Really Want to Test a Hypothesis?","text":"hypothesis test specific answer specific question. can use dart game metaphor question hypothesis test aims answer. essence, dart game hypothesis test methodological procedure make directional prediction: better worse B? dart game often compare two players, question whether act player best, player B best. hypothesis test, compare two hypotheses, question whether act null hypothesis true, whether alternative hypothesis true.Historically, researchers often interested testing hypotheses examine whether predictions derived scientific theory hold scrutiny. philosophies science () value theories able make predictions. darter wants convince good player, can make prediction (‘next arrow hit bulls-eye’), throw dart, impress hitting bulls-eye. researcher uses theory make prediction, collects data, observes can claim based predefined methodological procedure results confirm prediction, idea impressed predictive validity theory (de Groot, 1969). test supports idea theory useful starting point generate predictions reality. Philosophers science Popper call ‘verisimilitude’– theory way related truth, ‘truth-likeness’.order impressed prediction confirmed, prediction must able wrong. words, theoretical prediction needs falsifiable. predictions concern presence absence clearly observable entities (e.g., existence black swan) relatively straightforward divide possible states world set predicted theory (e.g., swans white), set predicted theory (e.g., swans can colors white). However, many scientific questions concern probabilistic events single observations contain noise due random variation – rats certain probability develop tumor, people certain probability buy product, particles certain probability appear collision. want forbid certain outcomes test measuring probabilistic events, can divide states world based probability result observed.\nFigure 5.3: fields make black white predictions presence absence observables, many sciences, predictions probabilistic, shades grey.\nJust hypothesis test can performed, mean interesting. hypothesis test useful 1) data generating models decided plausibility, 2) possible apply informative methodological procedure.First, two competing models good players. Just dart game little interest played Michael van Gerwen (world champion time writing) decide better dart player . Since play darts well, game two us interesting watch. Similarly, sometimes completely uninteresting compare two data generating models, one representing state world effect, another representing state world effect, cases absence effect extremely implausible.Second, hypothesis test interesting need designed informative study. designing study, need able make sure methodological rule provides severe test, likely corroborate prediction correct, time fail corroborate prediction wrong (Mayo, 2018). world champion darts stand 20 inches away dart board can just push dart location want end , possible show lack skill. blindfolded throwing darts 100 feet, possible world champion display skill. hypothesis test, statistical severity test determined error rates. Therefore, researcher needs able adequately control error rates perform test hypothesis high informational value.now hopefully clear hypothesis tests specific tool, answer specific question: applying methodological rule observed data, decision make want make incorrect decisions often? desire use methodological procedure decide competing theories, real reason report results hypothesis test. Even though might feel like test hypothesis research, carefully thinking statistical question want ask might reveal alternative statistical approaches, describing data observed, quantifying personal beliefs hypotheses, reporting relative likelihood data different hypotheses might approach answers question really want know.","code":""},{"path":"questions.html","id":"onesided","chapter":"5 Asking Statistical Questions","heading":"5.7 Directional (One-Sided) versus Non-Directional (Two-Sided) Tests","text":"Interestingly, quite disagreement whether statistical question ask study directional (meaning effects predicted direction lead rejection null hypothesis) non-directional (meaning effects either direction lead rejection null-hypothesis). example, Baguley (2012) writes \"one-sided tests typically avoided\" researchers rarely willing claim effect non-predicted direction non-significant, regardless large . time, Jones (1952) stated: “Since test null hypothesis one-sided alternative powerful test directional hypotheses, strongly recommended one-tailed model adopted wherever use appropriate”, Cho & Abe (2013) complain \"widespread overuse two-tailed testing directional research hypotheses tests\". reflect arguments choice perform one-sided test.First, clear directional test provides clear advantage statistical power. Figure 5.4 shows, ratio sample non-directional versus directional test means approximately 80% sample size non-directional test required achieve power directional test (exact benefit depends power effect size, seen figure ).\nFigure 5.4: Ratio required sample size one-sample t-test non-directional/directional test achieve 50%, 80% 95% power.\ndirectional test alpha level used one tail distribution, critical test value lower, else equal, power higher. reduction critical value required declare statistically significant effect criticized leads weaker evidence. example, Schulz & Grimes (2005) write: \"Using one-sided test sample size calculations reduce required sample sizes stretches credulity.\". trivially true: change design study requires smaller sample size reduces strength evidence collect, since strength evidence inherently tied total number observations. However, conflates two types statistical philosophies, namely likelihoodist approach, aims quantify relative evidence, frequentist approach, aims provide procedure make claims maximum error rate. difference designing study yields certain level evidence, study adequately controls error rates performing hypothesis test. desire specific level evidence, design study provides desired level evidence. desire control error rate claims, error rate 5% long alpha level 5%, regardless whether one-sided two-sided test performed.Note subtle distinction directional one-sided test (Baguley, 2012). Although two terms overlap performing t-test, overlap F-test. F-value t-value related: \\(t^2 = F\\). holds long df1 = 1 (e.g., F(1, 100), words long two groups compared. can see Figure 5.5 two distributions touch t = 1 (1^2 = 1), F-test negative values due squared nature distribution. critical t-value, squared, non-directional t-test 5% error rate equals critical F-value F-test, always one-sided, 5% error rate. Due 'squared' nature F-test, F-test always non-directional. can logically halve p-value F-test perform 'one-sided' test, directional F-test. comparing two groups, can use t-test instead F-test, can directional.\nFigure 5.5: Distribution rejection areas two-sided t-test corresponding F-test df1 = 1 df2 = 100.\nfinal concern raised one-sided tests surprising findings opposite direction might meaningful, ignored. agree, argument one-sided testing. goal hypothesis testing , surprisingly, test hypothesis. directional hypothesis, result opposite direction can never confirm hypothesis. can lead one create new hypothesis, new hypothesis tested new dataset (de Groot, 1969).\nmakes sense describe unexpected effect opposite direction prediction, difference describing data, testing hypothesis. one-sided hypothesis test prohibit researchers describing unexpected data patterns. really want test effect either direction, simply preregister two-sided test.","code":""},{"path":"questions.html","id":"crud","chapter":"5 Asking Statistical Questions","heading":"5.8 Systematic Noise, or the Crud Factor","text":"Meehl (1978) believes “almost universal reliance merely refuting null hypothesis standard method corroborating substantive theories soft areas terrible mistake, basically unsound, poor scientific strategy, one worst things ever happened history psychology”. time, also wrote: “rat psychologist, unabashedly employed significance testing latent-learning experiments; looking back see reason fault done light present methodological views” (Meehl, 1990). asks ‘ever correct use null-hypothesis significance tests?’ answer :course . say significance testing never appropriate helpful; several contexts incline criticize researcher failed test significance.Meehl opinion null hypothesis significance tests useful , question difference zero exists sometimes interesting question ask. Crucially, Meehl especially worried widespread use null hypothesis significance tests room systematic noise, crud factor data analyzed. presence systematic noise data means extremely unlikely null hypothesis true, combined large enough dataset, question whether null hypothesis can rejected uninteresting.Systematic noise can excluded ideal experiment. ideal experiment, one single factor can lead effect, perfect randomized controlled trial. Perfection notoriously difficult achieve practice. perfect experiment, can tiny causal factors , although main goal experiment, lead differences experimental control condition. Participants experimental condition might read words, answer questions, need time, think deeply, process novel information. things slightly move true effect size away zero – without related independent variable researchers aimed manipulate. difference reliable, caused anything researcher theoretically interested . real life, experiments even close perfect. Consequently, always room systematic noise, although way know large systematic noise specific study.Systematic noise especially problem studies randomization, correlational studies. example correlational data, think research examines differences women men. study subjects randomly assigned condition. non-experimental studies, possible ‘everything correlated everything’. slightly formally, crud can defined epistemological concept , correlational research, variables connected multivariate causal structures result real non-zero correlations variables given dataset (Orben & Lakens, 2020). example, men average taller women, consequence men asked strangers pick object high shelf supermarket bit often tham women. ask men women ‘often help strangers’ average difference height tiny systematic effect responses, even though researcher might theoretically interested differences unrelated height. specific case, systematic noise moves mean difference zero slightly higher value men – unknown number sources systematic noise play, interact, leading unknown final true population difference unlikely exactly zero.consequence, scientific fields find tests correlations relatively uninteresting. Researchers fields might find interesting estimate size correlations, might find worthwhile perform null hypothesis significance test correlation, large enough dataset, statistical significance practically guaranteed. increasingly true, bigger dataset. anecdote, working paper sequential analysis, asked collaborator Prof. Wassmer rpact package module tests correlations. replied enough interest null hypothesis significance tests correlations biopharmaceutical statistics, everything correlates everything anyway, anyone want test ?perform nil null hypothesis test, justify nil null hypothesis interesting hypothesis test . always self-evident, sometimes nil null hypothesis simply interesting. plausible nil null hypothesis true? , interesting perform minimal effect test. concrete example determine presence crud warrants use minimal effect tests literature, see Ferguson & Heene (2021).Several Many Lab Registered Replication Reports psychology, randomized experiments large sample sizes performed revisit published findings, shown practical purposes, given sample sizes psychologists able collect, proven surprisingly difficult find significant effects. multilab replication study examining action-sentence compatibility effect showed average effect logarithm lift-times close 0 [-0.006, 0.004] 903 native English speakers (Morey et al., 2021). Registered Replication Report examining effect priming participants either professor hooligan related concepts yielded non-significant difference number general knowledge questions answered difference 0.14% [−0.71%, 1.00%] sample 4493 participants (O’Donnell et al., 2018). Registered Replication Report examining effect recalling ten commandments 10 books read highschool often people cheated problem-solving task showed non-significant difference 0.11 [-0.09; 0.31] matrices sample 4674 participants (Verschuere et al., 2018). Registered Replication Report testing facial feedback hypothesis showed non-significant effect funniness ratings conditions participants manipulated move muscles related smiling pouting 0.03 [−0.11; 0.16] scale units sample 1894 participants (Wagenmakers et al., 2016). multi-lab replication study ego-depletion effect (feature prominently chapter bias) observed effect d = 0.04 [−0.07, 0.15] sample 2141 participants (Hagger et al., 2016). studies suggest sometimes nil null hypothesis plausible model test , even sample sizes much larger typically collected psychological research, nil null surprisingly difficult reject.multi-lab studies provide indications tiny true effects, due crud factor. Colling et al. (2020) observed congruency effects attentional SNARC effect four inter-stimulus interval conditions (250, 500, 750, 1000 ms) -0.05 ms [-0.82l; 0.71], 1.06 ms [0.34; 1.78], 0.19 ms [-0.53; 0.90], 0.18 ms [-0.51; 0.88] sample size 1105 participants. statistically significant effect 500 ms ISI condition (might crud) conclude: \"view difference 1 ms, even “real,” small neurally psychologically plausible mechanism—particularly one constrained operate within narrow time window 500 ms stimulus.\" McCarthy et al. (2018) observed difference 0.08 [0.004; 0.16] hostile ambiguous behavior vignette rated priming task less words related hostility, conclude \"results suggest procedures used replication study unlikely produce assimilative priming effect researchers practically routinely detect.\" instances, null-hypothesis can rejected, observed effect size deemed small matter. discussed chapter equivalence testing interval hypotheses, solution problem specify smallest effect size interest.","code":""},{"path":"questions.html","id":"inconsistencies","chapter":"5 Asking Statistical Questions","heading":"5.9 Dealing with Inconsistencies in Science","text":"might prefer clear answers scientific research, practice often presented inconsistent results scientific literature. 'even scientists agree'?According Karl Popper ability scientists reach consensus basic statements key criteria science:day longer possible scientific observers reach agreement basic statements amount failure language means universal communication. amount new ‘Babel Tongues’: scientific discovery reduced absurdity. new Babel, soaring edifice science soon lie ruins.philosophers science, Thomas Kuhn, viewed different paradigms science incommensurable. research Kuhn believed paradigms change dramatically time (calls scientific revolutions) advocates competing theories can directly compare discuss theories (Kuhn, 1962). Kuhn acknowledges scientists reach consensus within particular research tradition (calls 'normal science'):Men whose research based shared paradigms committed rules standards scientific practice. commitment apparent consensus produces prerequisites normal science, .e., genesis continuation particular research tradition.Harry Laudan aims resolve different views whether scientists can can reach consensus distinguishing disagreements three levels (Laudan, 1986). first level involves claims theoretical observable entities, scientists can factual disagreements factual consensus. can resolved methodological rules. However, scientists can also disagreements methods procedures used. disagreements methodological level can resolve discussing aims science, methods use optimal techniques achieve aims science. Laudan calls axiological level. According Laudan, mutual justification process three levels, even though different aims, methods, theories, scientists need able justify approach coherent.\nFigure 5.6: interrelationship methdological level, theories explain factual observation, aims science according Laudan's reticulated model scientific rationality.\nFactual inconsistencies can emerge different ways. First, support specific scientific claim can mixed, studies show statistically significant results (p < .05), studies (p > 0.05). seen mixed results expected sets studies. possible (sometimes likely) statistical power studies low. 60% studies yield p < .05 40% studies yield p > .05 might seem inconsistent, reality pattern results perfectly consistent expected long run Type 2 error rates set studies low statistical power. see later combining studies meta-analysis can yield clarity individual studies low statistical power.Popper (2002) writes: \"stray basic statements contradicting theory hardly induce us reject falsified. shall take falsified discover reproducible effect refutes theory.\" Remember claim rejecting accepting hypothesis done certain error rate, close replication studies way distinguish erroneous correct dichotomous claims statistical hypothesis tests. null hypothesis true, alpha level determines many false positive results observed. , errors occur often Type 1 error rate study designed . two-sided test performed alpha level 5%, 2.5% studies lead claim effect positive direction, 2.5% studies lead claim effect negative direction (reality null hypothesis true). Seeing statistically significant effects literature positive negative direction might seem inconsistent, findings Type 1 errors, occur exactly often expected based chosen alpha level.Even true effect, just random variation possible rarely observe statistically significant effect opposite direction, called 'error third kind' (Kaiser, 1960) Type S error (Altoè et al., 2020; Gelman & Carlin, 2014). Although results rare, much likely hear newspaper article reads 'opposed researchers believed last decades, new study suggests spending less time studying might lead better exam results' makes attention-grabbing headline. real risk counter-intuitive findings actually just flukes, good science journalists spent time reporting meta-analyses, less time reporting surprising novel findings.research results transparently reported, multiple studies quickly indicate whether dealing relatively rare Type 1 error, true finding. However, see chapter bias research findings shared. explained section positive predictive value can lead literature many Type 1 errors published, makes difficult determine true effect . combination random variation bias scientific literature can make easy find single study can used support viewpoint argument. prevent confirmation bias, actively search studies contradict point want make, evaluate evidence across multiple studies. larger literature shows inconsistencies, bias detection tests might provide first indication cause inconsistency biased literature. resolve inconsistencies due bias literature new studies performed - preferably Registered Reports preregistered statistical analysis plan published regardless whether results significant .second type inconsistency occurs two conflicting claims supported unbiased literature. case, different researchers might argue one claim true, likely false, true specific conditions. One might argue research fields, like psychology, always conditions claim true, conditions claim false. Indeed, one wanted summarize knowledge generated psychologists two words, \"depends\". McGuire (2004) refers ‘perspectivism’, proposed fruitful approach theorizing: “hypotheses, even pairs contraries, true (least perspective).” Thinking advance prediction might hold good approach theorize boundary conditions types moderators. two conflicting claims received reliable support, presence moderator means statistical relationship two variables depends third variable. Figure 5.7 see effect X Y depends level Z (Z impacts relationship X Y). example, effect winning lottery happy depends whether friends family happy (call condition Z = 0), whether arguments money ruin personal relationships (Z = 1). effect (indicated b) might positive one condition Z, absent even negative another condition Z. many possible moderators, studying moderation effects typically requires resources studying main effects, possible relatively little empirical research examines moderators, case inconsistencies remain unresolved.\nFigure 5.7: Path model moderation effect effect X Y depends Z, effect sizes b differ depending level Z.\nresearchers strongly believe failures replicate published findings can explained presence hitherto unidentified, 'hidden', moderators (Stroebe & Strack, 2014). least one example researchers able provide modest support idea previous failure replicate finding due personally relevant message study (Luttrell et al., 2017). difficult reliably identify moderator variables explain failures replicate published findings, easy raise explanation replication studies observe effect original study. Especially social sciences easy point moderators practically impossible test, fact society changed time, effects work one culture might replicate different cultures. age-old problem, already identified Galileo Assayer, one first books scientific method. book, Galileo discusses claim Babylonians cooked eggs whirling sling, impossible replicate, writes:‘achieve effect others formerly achieved, must lack something operation cause effect succeeding, lack one thing , alone can true cause. Now lack eggs, slings, sturdy fellows whirl , still cook, rather cool faster hot. since lack nothing except Babylonians, Babylonian cause egg hardening.’Resolving inconsistencies science effortful process can facilitated engaging adversarial collaboration, two teams join forces resolve inconsistencies (Mellers et al., 2001). requires first establishing reliable empirical basis reducing probability Type 1 Type 2 errors bias, systematically testing hypotheses proposed explain inconsistencies (Uygun Tunç & Tunç, 2022).","code":""},{"path":"effectsize.html","id":"effectsize","chapter":"6 Effect Sizes","heading":"6 Effect Sizes","text":"Effect sizes important statistical outcome empirical studies. Researchers want know whether intervention experimental manipulation effect greater zero, (obvious effect exists) big effect . Researchers often reminded report effect sizes, useful three reasons. First, allow researchers present magnitude reported effects, allow researchers reflect practical significance effects report, addition statistical significance. Second, effect sizes allow researchers draw meta-analytic conclusions comparing standardized effect sizes across studies. Third, effect sizes previous studies can used planning new study -priori power analysis.measure effect size quantitative description strength phenomenon. expressed number scale. unstandardized effect sizes, effect size expressed scale measure collected . useful whenever people able intuitively interpret differences measurement scale. example, children grow average 6 centimeters year age 2 puberty. can interpret 6 centimeters year effect size, many people world intuitive understanding large 6 cm . p-value used make claim whether effect, whether might just looking random variation data, effect size used answer question large effect . makes effect size estimate important complement p-values studies. p-value tells us can claim children grow age; effect sizes tell us size clothes can expect children wear certain age, long take new clothes small.people parts world use metric system, might difficult understand difference 6 cm . facilitate comparison effect sizes across situations different measurement scales used, researchers can report standardized effect sizes. standardized effect size, Cohen's d, computed dividing difference raw scale standard deviation, thus scaled terms variability sample taken. effect d = 0.5 means difference size half standard deviation measure. means effect sizes determined size effect size standard deviation, difference standardized effect size can caused difference size unstandardized effect, difference standard deviation.Standardized effect sizes common variables measured scale people familiar , measured different scales within research area. ask people happy , answer ‘5’ mean something different asked people answer scale 1 5 asked answer scale 1 9. Standardized effect sizes can understood compared regardless scale used measure dependent variable. Despite ease use standardized effect size measures, good arguments report interpret unstandardized effect sizes wherever possible (Baguley, 2009).Standardized effect sizes can grouped two families (Rosenthal, 1994): d family (consisting standardized mean differences) r family (measures strength association). Conceptually, d family effect sizes based difference observations, divided standard deviation observations. r family effect sizes describe proportion variance explained group membership. example, correlation (\\(r\\)) 0.5 indicates 25% variance (\\(r^2\\)) explained difference groups. effect sizes calculated sum squares (difference individual observations mean group, squared, summed) effect divided sums squares factors design.","code":""},{"path":"effectsize.html","id":"effect-sizes","chapter":"6 Effect Sizes","heading":"6.1 Effect sizes","text":"important outcome empirical study? might tempted say ’s p-value statistical test, given practically always reported articles, determines whether call something ‘significant’ . However, Cohen (1990) writes 'Things ’ve learned (far)':learned taught primary product research inquiry one measures effect size, p-values.Although want learn data different every study, rarely single thing always want know, effect sizes important part information gain data collection.measure effect size \"quantitative reflection magnitude phenomenon used purpose addressing question interest\" (Kelley & Preacher, 2012). expressed number scale, scale used depends effect size measure used. unstandardized effect sizes, can use scale people familiar . example, children grow average 6 centimeters year age 2 puberty. can interpret 6 centimeters year effect size. obvious effect size many benefits p-value. p-value gives indication unlikely children stay size become older – effect sizes tell us size clothes can expect children wear certain age, long take new clothes small.One reason report effect sizes facilitate future research. possible perform meta-analysis power analysis based unstandardized effect sizes standard deviation, easier work standardized effect sizes, especially variation measures researchers use. main goal reporting effect sizes reflect question whether observed effect size meaningful. example, might able reliably measure average 19 years olds grow 1 centimeter next year. difference statistically significant large enough sample, go shopping clothes 19 years old, something need care . look two examples studies looking effect size, addition statistical significance, improved statistical inferences.","code":""},{"path":"effectsize.html","id":"the-facebook-experiment","chapter":"6 Effect Sizes","heading":"6.2 The Facebook experiment","text":"summer 2014 concerns experiment Facebook performed users examine ‘emotional mood contagion’, idea people’s moods can influenced mood people around . can read article . starters, substantial concern ethical aspects study, primarily researchers performed study asked informed consent participants study (), ask permission institutional review board (ethics committee) university.One criticisms study dangerous influence people’s mood. Nancy J. Smyth, dean University Buffalo’s School Social Work wrote Social Work blog: “might even increased self-harm episodes, control anger, dare say , suicide attempts suicides resulted experimental manipulation. experiment create harm? problem , never know, protections human subjects never put place”.Facebook experiment strong effect people’s mood made people commit suicide otherwise committed suicide, obviously problematic. let us look effects manipulation Facebook used people bit closely.article, let’s see researchers manipulated:Two parallel experiments conducted positive negative emotion: One exposure friends’ positive emotional content News Feed reduced, one exposure negative emotional content News Feed reduced. conditions, person loaded News Feed, posts contained emotional content relevant emotional valence, emotional post 10% 90% chance (based User ID) omitted News Feed specific viewing.measured:experiment, two dependent variables examined pertaining emotionality expressed people’s status updates: percentage words produced given person either positive negative experimental period. total, 3 million posts analyzed, containing 122 million words, 4 million positive (3.6%) 1.8 million negative (1.6%).found:positive posts reduced News Feed, percentage positive words people’s status updates decreased B = −0.1% compared control [t(310,044) = −5.63, P < 0.001, Cohen’s d = 0.02], whereas percentage words negative increased B = 0.04% (t = 2.71, P = 0.007, d = 0.001). Conversely, negative posts reduced, percent words negative decreased B = −0.07% [t(310,541) = −5.51, P < 0.001, d = 0.02] percentage words positive, conversely, increased B = 0.06% (t = 2.19, P < 0.003, d = 0.008)., focus negative effects Facebook study (specifically, increase negative words people used) get idea whether risk increase suicide rates. Even though apparently negative effect, easy get understanding size effect numbers mentioned text. Moreover, number posts researchers analyzed really large. large sample, becomes important check size effect finding substantially interesting, large sample sizes even\nminute differences turn statistically significant (look detail ). , need better understanding “effect sizes”.","code":""},{"path":"effectsize.html","id":"the-hungry-judges-study","chapter":"6 Effect Sizes","heading":"6.3 The Hungry Judges study","text":"\nFigure 6.1: Proportion rulings favor prisoners ordinal position. Circled points indicate first decision three decision sessions; tick marks x axis denote every third case; dotted line denotes food break. Danziger, S., Levav, J., Avnaim-Pesso, L. (2011). Extraneous factors judicial decisions. Proceedings National Academy Sciences, 108(17), 6889–6892. https://doi.org/10.1073/PNAS.1018033108\nsee graphical representation proportion favorable parole decisions real-life judges making function number cases process across day Figure 6.1. study mentioned many popular science books example finding shows people always make rational decisions, \"judicial rulings can swayed extraneous variables bearing legal decisions\" (Danziger et al., 2011). see early day, judges start giving 65% people parole, basically means, \"right, can go back society.\" quickly, number favorable decisions decreases basically zero. quick break , authors say, \"may replenish mental resources providing rest, improving mood, increasing glucose levels body\" parole decisions back 65%, quickly drop basically zero. take another break, percentage positive decisions back 65%, drop course day.calculate effect size drop break, next break (Glöckner, 2016), effect represents Cohen's d approximately two, incredibly large. hardly effects psychology large, let alone effects mood rest decision making. surprisingly large effect occurs just , three times course day. mental depletion actually huge real-life impact, society basically fall complete chaos just lunch break every day. least, society organized around incredibly strong effect mental depletion. Just like manufacturers take size differences men women account producing items golf clubs watches, stop teaching time lunch, doctors schedule surgery, driving lunch illegal. psychological effect big, don’t need discover publish scientific journal - already know exists.can look meta-meta-analysis (paper meta-analyzes large number meta-analyses literature) Richard, Bond, & Stokes-Zoota (2003) see effect sizes law psychology close Cohen’s d 2. report two meta-analyzed effects slightly smaller. first effect jury’s final verdict likely verdict majority initially favored, 13 studies show effect size r = 0.63, d = 1.62. second jury initially split verdict, final verdict likely lenient, 13 studies show effect size r = .63 well. entire database, effect sizes come close d = 2 finding personality traits stable time (r = 0.66, d = 1.76), people deviate group rejected group (r = .6, d = 1.5), leaders charisma (r = .62, d = 1.58). might notice almost tautological nature effects. , supposedly, effect size passing time (subsequently eating lunch) parole hearing sentencings.see examining size effect can lead us identify findings caused proposed mechanisms. effect reported hungry judges study must therefore due confound. Indeed, confounds identified, turns ordering cases random, likely cases deserve parole handled first, cases deserve parole handled later (Chatziathanasiou, 2022; Weinshall-Margel & Shapard, 2011). additional use effect sizes identify effect sizes large plausible. Hilgard (2021) proposes build 'maximum positive controls', experimental conditions show largest possible effect provides upper limit plausible effect size measures.","code":""},{"path":"effectsize.html","id":"cohend","chapter":"6 Effect Sizes","heading":"6.4 Standardised Mean Differences","text":"Effect sizes can grouped two families (Rosenthal et al., 2000): d family (based standardized mean differences) r family (based measures strength association). Conceptually, d family effect sizes based comparison difference observations, divided standard deviation observations. means Cohen’s d = 1 means standardized difference two groups equals one standard deviation. size effect Facebook study quantified Cohen’s d. Cohen’s d (d italicized) used describe standardized mean difference effect. value can used compare effects across studies, even dependent variables measured different scales, example one study uses 7-point scales measure dependent variables, study uses 9-point scales. can even compare effect sizes across completely different measures construct, one study uses self-report measure, another study uses physiological measure. Although can compare effect sizes across different measurements, mean comparable, discuss detail section heterogeneity chapter meta-analysis.Cohen’s d ranges minus infinity infinity (although practice, mean difference positive negative direction can observed never infinite), value 0 indicating effect. Cohen (1988) uses subscripts distinguish different versions Cohen’s d, practice follow prevents confusion (without specification, Cohen’s d denotes entire family effect sizes). Cohen refers standardized mean difference two groups independent observations sample \\(d_s\\). get statistical details, let’s first visualize Cohen’s d 0.001 (found Facebook study) means. use visualization http://rpsychologist.com/d3/cohend/, website made Kristoffer Magnusson, allows visualize differences two measurements (increase negative words used Facebook user number positive words timeline reduced). visualization actually shows two distributions, one dark blue one light blue, overlap much tiny difference distributions visible (click settings button change slider settings, set step size 0.001 reproduce figure online app).\nFigure 6.2: vizualization 2 groups (although difference hardly visible) representing d = 0.001.\nfour numbers distribution express effect size different ways facilitate interpretation. example, probability superiority expresses probability randomly picked observation one group larger score randomly picked observation group. effect small, probability 50.03% - means people experimental write almost number positive negative words people control condition. number needed treat index illustrates Facebook study person needs type 3570 words observe one additional negative word, compared control condition. know often type many words Facebook, think can agree effect noticeable individual level.understand Cohen’s d two independent groups calculated, let’s first look formula t-statistic:\\[\nt = \\frac{{\\overline{M}}_{1}{- \\overline{M}}_{2}}{\\text{SD}_{\\text{pooled}} \\times \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}}\n\\]\\({\\overline{M}}_{1}{- \\overline{M}}_{2}\\) difference means, \\(\\text{SD}_{\\text{pooled}}\\) pooled standard deviation (Lakens, 2013), n1 n2 sample sizes two groups compared. t-value used determine whether difference two groups t-test statistically significant (explained chapter p-values. formula Cohen’s d similar:\\[d_s = \\frac{{\\overline{M}}_{1}{-\\overline{M}}_{2}}{\\text{SD}_{\\text{pooled}}}\\]can see, sample size group (\\(n_1\\) \\(n_2\\)) part formula t-value, part formula Cohen’s d (pooled standard deviation computed weighing standard deviation group sample size, cancels groups equal size). distinction useful know, practice means t-value (consequently, p-value) function sample size, Cohen's d independent sample size. true effect (e.g., non-zero effect size population) t-value null hypothesis test effect zero average become larger (p-value become smaller) sample size increases. effect size, however, increase decrease, become accurate, standard error decreases sample size increases. also reason p-values used make statement whether effect practically significant, effect size estimates often important complement p-values making statistical inferences.can calculate Cohen’s d independent groups independent samples t-value (can often convenient result section paper reading report effect sizes) :\\[d_s = t ⨯ \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}\\]d = 0.001 extremely tiny effect, explore effect size bit representative read literature. meta-meta-analysis mentioned earlier, median effect size published studies included meta-analyses psychological literature d = 0.43 (Richard et al., 2003). get feeling effect size, use online app set effect size d = 0.43.\nFigure 6.3: vizualization 2 groups representing d = 0.43.\nOne example meta-analytic effect size meta-meta-analysis exactly \\(d_s\\) = 0.43 finding people group work less hard achieve goal people work individually, called social loafing. effect large enough notice daily life. Yet, look overlap two distributions, see amount effort people put overlaps considerably two conditions (case social loafing, working individually versus working group). see Figure 6.3 probability superiority, probability randomly draw one person group condition one person individual condition, person working group puts less effort, 61.9%. interpretation differences groups also called common language effect size (McGraw & Wong, 1992).\nFigure 6.4: vizualization 2 groups representing d = 2.\nBased data, difference height 21-year old men women Netherlands approximately 13 centimeters (unstandardized effect size), standardized effect size \\(d_s\\) = 2. pick random man random woman walking street hometown Rotterdam, likely man taller woman? see quite likely, probability superiority 92.1%. even huge effect, still considerable overlap two distributions. conclude height people one group greater height people another group, mean everyone one group taller everyone group.Sometimes try explain scientific findings birthday party, skeptical aunt uncle might remark 'well believe true never experience '. probabilistic observations, distribution observed effects. example social loafing, average people put less effort achieve goal working group working . individual population, effect might larger, smaller, absent, even opposite direction. skeptical aunt uncle never experiences finding, contradict claim effect exists average population. Indeed, even expected effect people population, least time. Although might exceptions (e.g., almost every individual experience Stroop effect), many effects smaller, sufficient variation, effect present every single individual population.Conceptually, calculating Cohen’s d within-subjects comparisons based idea independent groups, differences two observations divided standard deviation within groups observations. However, case correlated samples common standardizer standard deviation difference scores. Testing whether two correlated means significantly different paired samples t-test testing whether difference scores correlated means significantly different 0 one-sample t-test. Similarly, calculating effect size difference two correlated means similar effect size calculated one sample t-test. standardized mean difference effect size within-subjects designs referred Cohen’s \\(d_z\\), z alludes fact unit analysis longer x y, difference, z, can calculated :\\[d_z = \\frac{M_{dif}}{\\sqrt{\\frac{\\sum{({X_{dif}-M_{dif})}}^2}{N-1}}}\\]\neffect size estimate Cohen’s \\(d_z\\) can also calculated directly t-value number participants using formula:\\[d_z = \\frac{t}{\\sqrt{n}}\\]Given direct relationship t-value paired-samples t-test Cohen’s \\(d_z\\), surprising software performs power analyses within-subjects designs (e.g., G*Power) relies Cohen’s \\(d_z\\) input.Maxwell & Delaney (2004) remark: ‘major goal developing effect size measures provide standard metric meta-analysts others can interpret across studies vary dependent variables well types designs.’ Cohen's \\(d_z\\) takes correlation dependent measures account, directly compared Cohen's \\(d_s\\). researchers prefer use average standard deviation groups observations standardizer (ignores correlation observations), allows direct comparison Cohen’s \\(d_s\\). effect size referred Cohen’s \\(d_{av}\\) (Cumming, 2013), simply:\\[d_{av} = \\frac{M_{dif}}{\\frac{SD_1+SD_2}{2}}\\]","code":""},{"path":"effectsize.html","id":"interpreting-effect-sizes","chapter":"6 Effect Sizes","heading":"6.5 Interpreting effect sizes","text":"commonly used interpretation Cohen’s d refer effect sizes small (d = 0.2), medium (d = 0.5), large (d = 0.8) based benchmarks suggested Cohen (1988). However, values arbitrary used. practice, see used form circular reasoning: effect small, d = 0.2, d = 0.2 small. see using benchmarks adds nothing, beyond covering fact actually interpret size effect. Furthermore, benchmarks 'medium' 'large' effect even correspond Cohen's d r (explained McGrath & Meyer (2006), see 'Test ' Q12). verbal classification based benchmarks ignores fact effect can practically meaningful, intervention leads reliable reduction suicide rates effect size d = 0.1. cases, effect size d = 0.1 might consequence , example effect smaller just noticeable difference, therefore small noticed individuals real world.Publication bias flexibility data analysis inflate effect size estimates. Innovations Registered Reports (Chambers & Tzavella, 2022; Nosek & Lakens, 2014) increasingly lead availability unbiased effect size estimates scientific literature. Registered Reports scientific publications reviewed data collected based introduction, method, proposed statistical analysis plan, published regardless whether results statistically significant . One consequence longer selectively publishing significant studies many effect sizes turn smaller researchers thought. example, 100 replication studies performed Reproducibility Project: Psychology, observed effect sizes replication studies average half size observed original studies (Open Science Collaboration, 2015).just report interpret effect size, nothing gained common practice finding corresponding verbal label 'small', 'medium', 'large'. Instead, researchers want argue effect meaningful need provide empirical falsifiable arguments meaningfulness effects (Anvari et al., 2021; Primbs et al., 2022). One approach argue effect sizes meaningful explicitly specifying smallest effect size interest] (Götz et al., 2022), example based cost-benefit analysis. Alternatively, researchers can interpret effect sizes relative effects literature (Baguley, 2009; Funder & Ozer, 2019).","code":""},{"path":"effectsize.html","id":"correlations-and-variance-explained","chapter":"6 Effect Sizes","heading":"6.6 Correlations and Variance Explained","text":"r family effect sizes based proportion variance explained group membership (e.g., correlation r = 0.5 indicates 25% variance (\\(r^2\\) explained difference groups). might remember r used refer correlation. correlation two continuous variables can range 0 (completely unrelated) 1 (perfect positive relationship) -1 (perfect negative relationship). get better feel correlations, play game guess correlation see scatterplot, guess correlation variables (see Figure 6.5).\nFigure 6.5: Screenshot Guess Correlation game (correct answer r = 0.24).\nr family effect sizes calculated sum squares (difference individual observations mean group, squared, summed) effect divided sums squares factors design. Earlier, mentioned median effect size psychology \\(d_s\\) = 0.43. However, authors actually report results correlation, r = 0.21. can convert Cohen's d r (take care applies \\(d_s\\), \\(d_z\\)):\\[r = \\frac{d_s}{\\sqrt{{d_s^{2}}^{+}\\frac{N^{2} - 2N}{n_{1} \\times n_{2}}}}\\]N total sample size groups, whereas \\(n_1\\) \\(n_2\\) sample sizes individual groups comparing (common use capital \\(N\\) total sample size, lowercase n sample sizes per group). can go http://rpsychologist.com/d3/correlation/ look good visualization proportion variance explained group membership, relationship r \\(r^2\\). amount variance explained often quite small number, see Figure 6.6 correlation 0.21 (median meta-meta-analysis Richard colleagues) see proportion variance explained 4.4%. Funder Ozer (2019) warn misinterpreting small values variance explained indication effect meaningful (even consider practice squaring correlation \"actively misleading\").\nFigure 6.6: Screenshot correlation effect size vizualization Kristoffer Magnusson r = 0.21.\nseen , can useful interpret effect sizes identify effects practically insignificant, effects implausibly large. Let’s take look study examines number suicides function amount country music played radio. can find paper won Ig Nobel prize studies first make laugh, think, although case, study make think country music, importance interpreting effect sizes.authors predicted following:contend themes found country music foster suicidal mood among people already risk suicide thereby associated high suicide rate.collected data:sample comprised 49 large metropolitan areas data music available. Exposure country music measured proportion radio airtime devoted country music. Suicide data extracted annual Mortality Tapes, obtained Inter-University Consortium Political Social Research (ICPSR) University Michigan. dependent variable number suicides per 100,000 population.concluded:significant zero-order correlation found white suicide rates country music (r = .54, p < .05). greater airtime given country music, greater white suicide rate.can compare size effect known effects psychology. database Richard colleagues, effects large, examples : leaders effective charisma (r = 0.54), good leader–subordinate relations promote subordinate satisfaction (r = 0.53), people can recognize emotions across cultures (r = 0.53). effects large obvious, raise doubts whether relationship listening country music suicides can size. country music really bad? search literature, find researchers able reproduce analysis original authors. likely results spurious, Type 1 error.Eta squared \\(\\eta^2\\) (part r family effect sizes, extension r can used two sets observations) measures proportion variation Y associated membership different groups defined X, sum squares effect divided total sum squares:\\[\\eta^{2} = \\frac{\\text{SS}_{\\text{effect}}}{\\text{SS}_{\\text{total}}}\\]\\(\\eta^2\\) .13 means 13% total variance can accounted group membership. Although \\(\\eta^2\\) efficient way compare sizes effects within study (given every effect interpreted relation total variance, \\(\\eta^2\\) single study sum 100%), eta squared easily compared studies, total variability study (\\(SS_{total}\\)) depends design study, increases additional variables manipulated (e.g., independent variables added). Keppel (Keppel, 1991) recommended partial eta squared (\\(\\eta_{p}^{2}\\)) improve comparability effect sizes studies. \\(\\eta_{p}^{2}\\) expresses sum squares effect relation sum squares effect plus sum squares error associated effect. Partial eta squared calculated :\\[\\eta_{p}^{2} = \\frac{\\text{SS}_{\\text{effect}}}{\\text{SS}_{\\text{effect}} + \\text{SS}_{\\text{error}}}\\]designs fixed factors (manipulated factors, factors exhaust levels independent variable, alive vs. dead), designs measured factors covariates, partial eta squared can computed F-value degrees freedom (Cohen, 1988):\\[\\eta_{p}^{2} = \\frac{F \\times \\text{df}_{\\text{effect}}}{{F \\times \\text{df}}_{\\text{effect}} + \\text{df}_{\\text{error}}}\\]example, F(1, 38) = 7.21, \\(\\eta_{p}^{2}\\) = 7.21 ⨯ 1/(7.21 ⨯ 1 +\n38) = 0.16.Eta squared can transformed Cohen’s d:d = 2\\(\\times f\\) \\(f^{2} = \\eta^{2}/(1 - \\eta^{2})\\)","code":""},{"path":"effectsize.html","id":"correcting-for-bias","chapter":"6 Effect Sizes","heading":"6.7 Correcting for Bias","text":"Population effect sizes almost always estimated basis samples, measure population effect size estimate based sample averages, Cohen’s d slightly overestimates true population effect. Cohen’s d refers population, Greek letter δ typically used. Therefore, corrections bias used (even though corrections always lead completely unbiased effect size estimate). d family effect sizes, correction bias population effect size estimate Cohen’s d known Hedges’ g (although different people use different names – \\(d_{unbiased}\\) also used). correction bias noticeable small sample sizes, since often use software calculate effect sizes anyway, makes sense always report Hedge’s g instead Cohen’s d (Thompson, 2007).Cohen’s d, \\(\\eta^2\\) biased estimate true effect size population. Two less biased effect size estimates proposed, epsilon squared \\(\\varepsilon^{2}\\) omega squared \\(\\omega^{2}\\). practical purposes, two effect sizes correct bias equally well (Albers & Lakens, 2018; Okada, 2013), preferred \\(\\eta^2\\). Partial epsilon squared (\\(\\varepsilon_{p}^{2}\\)) partial omega squared (\\(\\omega_{p}^{2}\\)) can calculated based F-value degrees freedom.\\[\n\\omega_{p}^{2} = \\frac{F - 1}{F + \\ \\frac{\\text{df}_{\\text{error}} + 1}{\\text{df}_{\\text{effect}}}}\n\\]\\[\n\\varepsilon_{p}^{2} = \\frac{F - 1}{F + \\ \\frac{\\text{df}_{\\text{error}}}{\\text{df}_{\\text{effect}}}}\n\\]\nPartial effect sizes \\(\\eta_{p}^{2}\\), \\(\\varepsilon_{p}^{2}\\) \\(\\omega_{p}^{2}\\) generalized across different designs. reason, generalized eta-squared (\\(\\eta_{G}^{2}\\)) generalized omega-squared (\\(\\omega_{G}^{2}\\)) proposed (Olejnik & Algina, 2003), although popular. part, might summarizing effect size ANOVA design single index limitations, perhaps makes sense describe pattern results, see section .","code":""},{"path":"effectsize.html","id":"effect-sizes-for-interactions","chapter":"6 Effect Sizes","heading":"6.8 Effect Sizes for Interactions","text":"effect size used power analyses ANOVA designs Cohen's f. two independent groups, Cohen's \\(f\\) = 0.5 * Cohen's d. two groups, Cohen's f can converted eta-squared back \\(f = \\frac{\\eta^2}{(1 - \\eta^2)}\\) \\(\\eta^2 = \\frac{f^2}{(1 + f^2)}\\). predicting interaction effects ANOVA designs, planning study based expected effect size \\(\\eta_{p}^{2}\\) Cohen's f might intuitive approach.start effect size simple two group comparison. assume predict mean difference 1, know standard deviation measure 2. means standardized effect size d = 0.5. independent t-test mathematically identical F-test two groups. F-test, effect size used power analyses Cohen's f, calculated based standard deviation population means divided population standard deviation (know measure 2), :\\[\\begin{equation}\nf = \\frac{\\sigma _{ m }}{\\sigma}\n\\end{equation}\\]\nequal sample sizes\n\\[\\begin{equation}\n\\sigma _{ m } = \\sqrt { \\frac { \\sum_ { = 1 } ^ { k } ( m _ { } - m ) ^ { 2 } } { k } }.\n\\end{equation}\\]formula m grand mean, k number means, m_i mean group. formula might look bit daunting, calculating Cohen's f difficult two groups.take expected means 0 1, standard deviation 2, grand mean (m formula ) (0 + 1)/2 = 0.5. formula says subtract grand mean mean group, square value, sum . (0 - 0.5)^2 (1 - 0.5)^2, 0.25. sum values (0.25 + 0.25 = 0.5), divide number groups (0.5/2 = 0.25) take square root, find \\(\\sigma_{ m }\\) = 0.5. can now calculate Cohen's f (using \\(\\sigma\\) = 2 measure):\\[\\begin{equation}\nf = \\frac{\\sigma _{ m }}{\\sigma} = \\frac{0.5}{2} = 0.25\n\\end{equation}\\]confirm two groups Cohen's f half large Cohen's d.Now basis look interaction effects. Different patterns means ANOVA can Cohen's f. two types interactions, visualized Figure 6.7. ordinal interaction, mean one group (\"B1\") always higher mean group (\"B2\"). Disordinal interactions also known 'cross-' interactions, occur group larger mean switches . difference important, since disordinal interaction Figure 6.7 larger effect size ordinal interaction.\nFigure 6.7: Schematic illustration disordinal (cross-) ordinal interaction.\nMathematically interaction effect computed cell mean minus sum grand mean, marginal mean condition one factor minus grand mean, marginal mean condition factor minus grand mean (Maxwell & Delaney, 2004).consider two cases, one perfect disordinal interaction (means 0 1 flip around condition, 1 0) ordinal interaction (effect present one condition, means 0 1, disappears condition, means 0 0, see Figure 6.8).\nFigure 6.8: Disordinal (cross-) ordinal interaction means 0 1, n = 50 per group, sd 2.\ncan calculate interaction effect follows (go steps detail). First, look disordinal interaction. grand mean (1 + 0 + 0 + 1) / 4 = 0.5.can compute marginal means A1, A2, B1, B2, simply averaging per row column, gets us A1 row (1+0)/2=0.5. perfect disordinal interaction, marginal means 0.5. means main effects. main effect factor (marginal means A1 A2 exactly 0.5), main effect B.can also calculate interaction effect. cell take value cell (e.g., a1b1 1) compute difference cell mean additive effect two factors :1 - (grand mean 0.5 + (marginal mean a1 minus grand mean, 0.5 - 0.5 = 0) + (marginal mean b1 minus grand mean, 0.5 - 0.5 = 0)). Thus, cell get:a1b1: 1 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = 0.5a1b2: 0 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = -0.5a2b1: 0 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = -0.5a2b2: 1 - (0.5 + (0.5 -0.5) + (0.5 -0.5)) = 0.5Cohen's \\(f\\) \\(f = \\frac { \\sqrt { \\frac { 0.5^2 +-0.5^2 + -0.5^2 + 0.5^2 } { 4 } }}{ 2 } = 0.25\\)ordinal interaction grand mean (1 + 0 + 0 + 0) / 4, 0.25. marginal means a1: 0.5, a2: 0, b1: 0.5, b2: 0.Completing calculation four cells ordinal interaction gives:a1b1: 1 - (0.25 + (0.5 -0.25) + (0.5 -0.25)) = 0.25a1b2: 0 - (0.25 + (0.5 -0.25) + (0.0 -0.25)) = -0.25a2b1: 0 - (0.25 + (0.0 -0.25) + (0.5 -0.25)) = -0.25a2b2: 0 - (0.25 + (0.0 -0.25) + (0.0 -0.25)) = 0.25Cohen's \\(f\\) \\(f = \\frac { \\sqrt { \\frac { 0.25^2 +-0.25^2 + -0.25^2 + 0.25^2 } { 4 } }}{ 2 } = 0.125\\).see effect size cross-interaction (f = 0.25) twice large effect size ordinal interaction (f = 0.125). make sense think interaction test contrasts. disordinal interaction comparing cells a1b1 a2b2 a1b2 a2b1, (1+1)/2 vs. (0+0)/2. Thus, see t-test contrast, see mean difference 1. ordinal interaction, (1+0)/2 vs. (0+0)/2, mean difference halved, namely 0.5. obviously matters statistical power examine interaction effects experiments.Just stating expect 'medium' Cohen's f effect size interaction effect power analysis best approach. Instead, start thinking pattern means standard deviations (within factors, correlation dependent variables) compute effect size data pattern. prefer hand, can use Superpower (Lakens & Caldwell, 2021). also holds complex designs, multilevel models. cases, often case power analyses easier based simulation-based approaches, based plugging single effect size power analysis software (DeBruine & Barr, 2021).","code":""},{"path":"effectsize.html","id":"test-yourself-4","chapter":"6 Effect Sizes","heading":"6.9 Test Yourself","text":"Q1: One largest effect sizes meta-meta analysis Richard colleagues 2003 people likely perform action feel positively action believe common. effect (respect researchers contributed research meta-analysis) somewhat trivial. Even , correlation r = .66, equals Cohen’s d 1.76. according online app https://rpsychologist.com/cohend/ probability superiority effect size?70.45%88.12%89.33%92.14%Q2: Cohen’s d ______ eta-squared ________r; epsilon-squaredHedges’ g; omega-squaredCohen’s \\(d_s\\); generalized eta-squaredQ3: correlation r = 1.2 :ImpossibleImplausibly large effect size social sciencesIn line median effect size psychologyQ4: Let’s assume difference two means observe 1, pooled standard deviation also 1. , average, happens t-value Cohen’s d, simulate studies, function sample size simulations?Given mean difference standard deviation, sample size becomes bigger, t-value become larger, Cohen’s d becomes larger.Given mean difference standard deviation, sample size becomes bigger, t-value gets closer true value, Cohen’s d becomes larger.Given mean difference standard deviation, sample size becomes bigger, t-value become larger, Cohen’s d gets closer true value.Given mean difference standard deviation, sample size becomes bigger, t-value gets closer true value, Cohen’s d gets closer true value.Q5: Go http://rpsychologist.com/d3/correlation/ look good visualization proportion variance explained group membership, relationship r \\(r^2\\). Look scatterplot shared variance effect size r = .21 (Richard et al., 2003). Given r = 0.21 estimate median effect size psychological research (corrected bias), much variance data average explain?2.1%21%4.4%44%Q6: default, sample size online correlation visualization linked 50. Click cogwheel access settings, change sample size 500, click button 'New Sample'. happens?proportion explained variance 5 times large.proportion explained variance 5 times small.proportion explained variance 52 times large.proportion explained variance stays .Q7: old paper find statistical result reported t(36) = 2.14 p < 0.05 independent t-test without reported effect size. Using online MOTE app https://doomlab.shinyapps.io/mote/ (choose Independent t -t Mean Differences dropdown menu) MOTE R function d.ind.t.t, effect size Cohen’s d effect, given 38 participants (e.g., 18 group, leading N – 2 = 36 degrees freedom) alpha level 0.05?d = 0.38d = 0.41d = 0.71d = 0.75Q8: old paper find statistical result 2x3 subjects ANOVA reported F(2, 122) = 4.13, p < 0.05, without reported effect size. Using online MOTE app https://doomlab.shinyapps.io/mote/ (choose Eta – F Variance Overlap dropdown menu) MOTE R function eta.F, effect size partial eta-squared?\\(\\eta_p^2\\) = 0.063\\(\\eta_p^2\\) = 0.996\\(\\eta_p^2\\) = 0.032\\(\\eta_p^2\\) = 0.049Q9: realize computing omega-squared corrects bias eta-squared. old paper F(2, 122) = 4.13, p < 0.05, using online MOTE app https://doomlab.shinyapps.io/mote/ (choose Omega – F Variance Overlap dropdown menu) MOTE R function omega.F, effect size partial omega-squared? HINT: total sample size df error + k, k number groups (6 2x3 ANOVA).\\(\\eta_p^2\\) = 0.047\\(\\eta_p^2\\) = 0.749\\(\\eta_p^2\\) = 0.032\\(\\eta_p^2\\) = 0.024Q10: Several times chapter effect size Cohen’s d converted r, vice versa. can use effectsize R package (can also used compute effect sizes analyze data R) convert median r = 0.21 observed Richard colleagues’ meta-meta-analysis d: effectsize::r_to_d(0.21) (assuming equal sample sizes per condition) yields d = 0.43 (conversion assumes equal sample sizes group). Cohen’s d corresponds r = 0.1?d = 0.05d = 0.10d = 0.20d = 0.30Q11: can useful convert effect sizes r performing meta-analysis effect sizes included based mean differences. Using d_to_r() function effectsize package, d = 0.8 correspond (assuming equal sample sizes per condition)?r = 0.30r = 0.37r = 0.50r = 0.57Q12: questions 10 11 might noticed something peculiar. benchmarks typically used ‘small’, ‘medium’, ‘large’ effects Cohen’s d d = 0.2, d = 0.5, d = 0.8, correlation r = 0.1, r = 0.3, r = 0.5. Using d_to_r() function effectsize package, check see whether benchmark ‘large’ effect size correspond d r.McGrath & Meyer (2006) write: “Many users Cohen’s (1988) benchmarks seem unaware correlation coefficient d strictly equivalent, Cohen’s generally cited benchmarks correlation intended infrequently used biserial correlation rather point biserial.”Download paper McGrath Meyer, 2006 (can find links pdf ), page 390, right column, read solution authors prefer.Just stop using silly benchmarks., honestly, just really stop using silly benchmarks.benchmarks d need changed 0.20, 0.67, 1.15The benchmarks correlations need changed .10, .24, .37","code":""},{"path":"effectsize.html","id":"open-questions-4","chapter":"6 Effect Sizes","heading":"6.9.1 Open Questions","text":"difference standardized unstandardized effect sizes?difference standardized unstandardized effect sizes?Give definition 'effect size'.Give definition 'effect size'.main uses effect sizes?main uses effect sizes?can effect sizes improve statistical inferences, beyond looking p-value?can effect sizes improve statistical inferences, beyond looking p-value?effect size r?effect size r?effect size d?effect size d?unbiased effect sizes correspond d eta-squared called?unbiased effect sizes correspond d eta-squared called?Give example small effects meaningless, .Give example small effects meaningless, .values can Cohen’s d take?values can Cohen’s d take?values can correlation (r) take?values can correlation (r) take?","code":""},{"path":"confint.html","id":"confint","chapter":"7 Confidence Intervals","heading":"7 Confidence Intervals","text":"report point estimates, acknowledge quantify uncertainty estimates. Confidence intervals provide way quantify precision estimate. reporting estimate confidence interval, results reported within range values contain true value parameter desired percentage. example, report effect size estimate 95% confidence interval, expectation interval wide enough 95% time range values around estimate contains true parameter value (test assumptions met).","code":""},{"path":"confint.html","id":"population-vs.-sample","chapter":"7 Confidence Intervals","heading":"7.1 Population vs. Sample","text":"statistics, differentiate population sample. population everyone interested , people world, elderly depressed, people buy innovative products. sample everyone able measure population interested . similarly distinguish parameter statistic. parameter characteristic population, statistic characteristic sample. Sometimes, data entire population. example, measured height people ever walked moon. can calculate average height twelve individuals, know true parameter. need inferential statistics. However, know average height people ever walked earth. Therefore, need estimate parameter, using statistic based sample. Although rare study includes entire population, impossible, illustrated Figure 7.1.\nFigure 7.1: Example registry-based study entire population included study. https://doi.org/10.1093/ije/dyab066\nentire population measured need perform hypothesis test. , population generalize (although possible argue still making inference, even entire population observed, observed metaphorical population one many possible worlds, see D. Spiegelhalter (2019)). data entire population collected, population effect size known confidence interval compute. total population size known, measured completely, confidence interval width shrink zero closer study gets measuring entire population. known finite population correction factor variance estimator (Kish, 1965). variance sample mean \\(\\sigma^2/n\\), finite populations multiplied finite population correction factor standard error:\n\\[FPC = \\sqrt{\\frac{(N - n)}{(N-1)}}\\]\nN size population, n size sample. N much larger n, correction factor close 1 (therefore correction typically ignored populations large, even populations finite), noticeable effect variance. total population measured correction factor 0, variance becomes 0 well. example, total population consists 100 top athletes, data collected sample 35 athletes, finite population correction \\(\\sqrt{(100 - 35)/(100-1)}\\) = 0.81. superb R package can compute population corrected confidence intervals (Cousineau & Chiasson, 2019).","code":""},{"path":"confint.html","id":"what-is-a-confidence-interval","chapter":"7 Confidence Intervals","heading":"7.2 What is a Confidence Interval?","text":"Confidence intervals statement percentage confidence intervals contain true parameter value. behavior confidence intervals nicely visualized website Kristoffer Magnusson: http://rpsychologist.com/d3/CI/. Figure 7.2 see blue dots represent means sample, fall around red dashed vertical line, represents true value parameter population. Due variation sample, estimates fall red dashed line. horizontal lines around blue dots confidence intervals. default, visualization shows 95% confidence intervals. lines black (means confidence interval overlaps orange dashed line indicating true population value), red (indicating capture true population value). long run, 95% horizontal bars black, 5% red.\nFigure 7.2: Series simulated point estimates confidence intervals.\ncan now see meant sentence “Confidence intervals statement percentage confidence intervals contain true parameter value“. long run, 95% samples, orange dashed line (population parameter) contained within 95% confidence interval around sample mean, 5% confidence intervals true. see turn formula confidence intervals, width confidence interval depends sample size standard deviation. larger sample size, smaller confidence intervals.","code":""},{"path":"confint.html","id":"singleCI","chapter":"7 Confidence Intervals","heading":"7.3 Interpreting a single confidence interval","text":"Whenever compute encounter single confidence interval important realize someone else performing exactly experiment , purely due random variation, observed different confidence interval, effect size, p-value. random variation single confidence interval difficult interpret. Misinterpretations common. example, Cumming (2014) writes \"can 95% confident interval includes μ can think lower upper limits likely lower upper bounds μ.\" statements incorrect (Morey et al., 2016). incorrect claim can 95% confident interval includes true population mean, study whether friend can predict whether coin comes heads tails 100 flips, correctly predict coin flip 61 100 flips 95% confidence interval 0.507 0.706, perfectly reasonable use Bayesian reasoning assume (remaining 5% confidence) just fluke, true success rate guessing outcome coin flips 50%. also incorrect believe lower upper limits likely lower upper bounds μ, anyone else performing experiment observed different confidence interval, different upper lower bound, analyzing single sample drawn population. lot data collected (say thousands observations) problem practically disappears, remaining uncertainty small matter.One useful way think confidence interval indication resolution effect estimated. resolution low, difficult get clear picture, resolution extremely high, picture clear enough practical use cases. estimated effect narrow range, say M = 0.52, 95% CI [0.49; 0.55], feel warranted assume one cares differences less 0.05 measure, confidence interval communicates data estimated sufficient precision. Similarly, sample size small, confidence wide, say M = 0.52, 95% CI [0.09; 0.95], feel warranted assume differences almost 1 measure matter situations estimate used, confidence interval communicates effect size estimate precise enough. evaluation resolution estimate can useful, missing p-value effect size reported. reason, recommended report confidence intervals around estimates. Confidence intervals often reported within brackets, interesting alternative (especially tables) use subscripts: \\(_{0.09}0.52_{0.95}\\) (Louis & Zeger, 2009).tempting use Bayesian interpretation single confidence interval, one say \"believe 95% probability interval contains true population parameter\". Bayesian interpretation lost frequentist error control means depending prior belief might misguided much 5% studies. something Bayesian worries , focus inferences limiting errors long run, quantifying beliefs. frequentist can make probability claims single observations. data collected, frequentist can state current confidence interval either contains true population parameter, . long run, \\(\\alpha\\)% confidence intervals include true population parameter, single confidence interval one flukes. Even though frequentist Bayesian confidence interval can identical certain priors (Albers et al., 2018), different definitions probability lead different interpretations single confidence interval. frequentist can easily interpret confidence procedure, easy interpret single confidence interval (Morey et al., 2016). surprise us, difficult interpret single study (need perform replication studies). confidence intervals interpreted long-run procedure, directly related p-values.","code":""},{"path":"confint.html","id":"relatCIp","chapter":"7 Confidence Intervals","heading":"7.4 The relation between confidence intervals and p-values","text":"direct relationship CI around effect size statistical significance null-hypothesis significance test. example, effect statistically significant (p < 0.05) two-sided independent t-test alpha .05, 95% CI mean difference two groups include zero. Confidence intervals sometimes said informative p-values, provide information whether effect statistically significant (.e., confidence interval overlap value representing null hypothesis), also communicate precision effect size estimate. true, mentioned chapter p-values still recommended add exact p-values, facilitates re-use results secondary analyses (Appelbaum et al., 2018), allows researchers compare p-value alpha level preferred use (Lehmann & Romano, 2005).order maintain direct relationship confidence interval p-value necessary adjust confidence interval level whenever alpha level adjusted. example, alpha level 5% corrected three comparisons 0.05/3 - 0.0167, corresponding confidence interval 1 - 0.0167 = 0.9833 confidence interval. Similarly, p-value computed one-sided t-test, upper lower limit interval, end interval ranges −∞ ∞.maintain direct relationship F-test confidence interval, 90% CI effect sizes F-test provided. reason explained Karl Wuensch. Cohen’s d can take positive negative values, r² η² squared, can therefore take positive values. related fact F-tests (commonly used ANOVA) one-sided. calculate 95% CI, can get situations confidence interval includes 0, test reveals statistical difference p < .05 (mathematical explanation, see Steiger (2004)). means 95% CI around Cohen's d independent t-test equals 90% CI around η² exactly test performed ANOVA. final detail, eta-squared smaller zero, lower bound confidence interval smaller 0. means confidence interval effect statistically different 0 start 0. report CI 90% CI [.00; .XX] XX upper limit CI.Confidence intervals often used forest plots communicate results meta-analysis. plot , see 4 rows. row shows effect size estimate one study (Hedges’ g). example, study 1 yielded effect size estimate 0.44, confidence interval around effect size 0.08 0.8. horizontal black line, similarly visualization played around , width confidence interval. touch effect size 0 (indicated black vertical dotted line) effect statistically significant.\nFigure 7.3: Meta-analysis 4 studies.\ncan see, based fact confidence intervals overlap 0, studies 1 3 statistically significant. diamond shape named FE model (Fixed Effect model) meta-analytic effect size. Instead using black horizontal line, upper limit lower limit confidence interval indicated left right points diamond, center diamond meta-analytic effect size estimate. meta-analysis calculates effect size combining weighing studies. confidence interval meta-analytic effect size estimate always narrower single study, combined sample size studies included meta-analysis.preceding section, focused examining whether confidence interval overlapped 0. confidence interval approach null-hypothesis significance test. Even though computing p-value, can directly see confidence interval whether p < \\(\\alpha\\). confidence interval approach hypothesis testing makes quite intuitive think performing tests non-zero null hypotheses (Bauer & Kieser, 1996). example, test whether can reject effect 0.5 examining 95% confidence interval overlap 0.5. can test whether effect smaller 0.5 examining 95% confidence interval falls completely 0.5. see leads logical extension null-hypothesis testing , instead testing reject effect 0, can test whether can reject effects interest range predictions equivalence tests.","code":""},{"path":"confint.html","id":"the-standard-error-and-95-confidence-intervals","chapter":"7 Confidence Intervals","heading":"7.5 The Standard Error and 95% Confidence Intervals","text":"calculate confidence interval, need standard error. standard error (SE) estimates variability sample means obtained taking several measurements population. easy confuse standard deviation, degree individuals within sample differ sample mean. Formally, statisticians distinguish σ \\(\\widehat{\\sigma}\\), hat means value estimated sample, lack hat means population value – ’ll leave hat, even ’ll mostly talk estimated values based sample formulas . Mathematically (σ standard\ndeviation),\\[\nStandard \\ Error \\ (SE) = \\sigma/\\sqrt n\n\\]standard error sample tend zero increasing sample size, estimate population mean become accurate. standard deviation sample become similar population standard deviation sample size increases, become smaller. standard deviation statistic descriptive sample, standard error describes bounds random sampling process.standard error used construct confidence intervals (CI) around sample estimates, mean, differences means, whatever statistics might interested . calculate confidence interval around mean (indicated Greek letter mu: μ), use t distribution corresponding degrees freedom (df : one-sample t-test, degrees freedom n-1):\\[\n\\mu \\pm t_{df, 1-(\\alpha/2)} SE\n\\]95% confidence interval, \\(\\alpha\\) = 0.05, thus critical t-value degrees freedom 1- \\(\\alpha\\) /2, 0.975th quantile calculated. Remember t-distribution slightly thicker tails Z-distribution. 0.975th quantile Z-distribution 1.96, value t-distribution example df = 19 2.093. value multiplied standard error, added (upper limit confidence interval) subtracted (lower limit confidence interval) mean.","code":""},{"path":"confint.html","id":"overlapping-confidence-intervals","chapter":"7 Confidence Intervals","heading":"7.6 Overlapping Confidence Intervals","text":"Confidence intervals often used plots. Figure 7.4 , three estimates visualized (dots), surrounded three lines (95% confidence intervals). left two dots (X Y) represent means independent groups X Y scale 0 8 (see axis 0-8 left side plot). dotted lines two confidence intervals visualize overlap confidence intervals around means. two confidence intervals around means X Y commonly shown figure scientific article. third dot, slightly larger, mean difference X Y, slightly thicker line visualizes confidence interval mean difference. difference score expressed using axis right (-3 5). plot , mean group X 3, mean group Y 5.6, difference 2.6. plot based 50 observations per group, confidence interval around mean difference ranges 0.49 4.68, quite wide.\nFigure 7.4: Means 95% confidence intervals two independent groups mean difference two groups 95% confidence interval.\nmentioned earlier, 95% confidence interval contain 0, effect statistically different 0. Figure 7.4 , mean difference 95% confidence interval around indicated 'difference' label. 95% confidence interval contain 0, t-test significant alpha 0.05. p-value indicated plot 0.016. Even though two means differ statistically significantly , confidence interval around mean overlap. One might intuitively believe effect statistically significant confidence interval around individual means overlap, true. significance test related confidence interval around mean difference.","code":""},{"path":"confint.html","id":"prediction-intervals","chapter":"7 Confidence Intervals","heading":"7.7 Prediction Intervals","text":"Even though 95% confidence intervals contain true parameter long run, 95% confidence interval contain 95% future individual observations (95% future means; discussed next section). Sometimes, researchers want predict interval within single value fall. called prediction interval. always much wider confidence interval. reason individual observations can vary substantially, means future samples (fall within normal confidence interval 95% time) vary much less.Figure 7.5, orange background illustrates 95% confidence interval around mean, yellow background illustrates 95% prediction interval (PI).\nFigure 7.5: comparison 95% confidence interval (orange) 95% prediction interval (yellow).\ncalculate prediction interval, need slightly different formula standard error used confidence interval, namely:\\[\nStandard \\ Error \\ (SE) = \\sigma/\\sqrt(1+1/n)\n\\]rewrite formula used confidence interval \\(\\sigma/\\sqrt(1/N)\\), see difference confidence interval prediction interval “1+” always leads wider intervals. Prediction intervals wider, constructed contain single future value 95% time, instead mean. fact prediction intervals wide good reminder difficult predict happen single individual.","code":""},{"path":"confint.html","id":"capture-percentages","chapter":"7 Confidence Intervals","heading":"7.8 Capture Percentages","text":"can difficult understand 95% confidence interval provide us interval 95% future means fall. percentage means falls within single confidence interval called capture percentage. capture percentage something ever use make inferences data, useful learn capture percentages prevent misinterpreting confidence intervals. Figure 7.6 see two randomly simulated studies sample size population. true effect size studies 0, see 95% confidence intervals studies contain true population value 0. However, two confidence intervals cover quite different ranges effect sizes, confidence interval Study 1 ranging -0.07 0.48, confidence interval Study 2 ranging -0.50 0.06. true future, expect 95% effect sizes fall -0.07 0.48 95% effect sizes fall -0.50 0.06.\nFigure 7.6: Meta-analysis two simulated studies population.\nsituation 95% confidence interval happens also 95% capture percentage observed effect size sample happens exactly true population parameter. Figure 7.6, means need observe effect exactly 0. However, can’t know whether observed effect size happens exactly population effect size. sample estimate identical true population value (almost always case) less 95% future effect sizes fall within CI current sample. observed two studies observed effect sizes bit removed true effect size, find effect size estimates future studies fall outside observed 95% confidence interval quite often. , percentage future means fall within single confidence interval depends upon single confidence interval happened observe. Based simulation studies possible show average, long run, 95% CI 83.4% capture probability (Cumming & Maillardet, 2006).","code":""},{"path":"confint.html","id":"calculating-confidence-intervals-around-standard-deviations.","chapter":"7 Confidence Intervals","heading":"7.9 Calculating Confidence Intervals around Standard Deviations.","text":"calculate standard deviation (SD) sample, value \nestimate true value population. small samples, estimate can quite far . due law large numbers, sample size increases, measuring standard deviation accurately. Since sample standard deviation estimate uncertainty, can calculate 95% confidence interval around .Keeping uncertainty standard deviations mind can important. researchers perform -priori power analysis based effect size interest expressed raw scale, need accurate estimates standard deviation. Sometimes researchers use pilot data get estimate standard deviation. Since estimate population standard deviation based pilot study uncertainty (pilot studies usually relatively small sample size), -priori power analysis inherit uncertainty (see 'Test ' questions ). circumvent , use validated existing measures accurate estimates standard deviation population interest available. keep mind estimates sample uncertainty.","code":""},{"path":"confint.html","id":"computing-confidence-intervals-around-effect-sizes","chapter":"7 Confidence Intervals","heading":"7.10 Computing Confidence Intervals around Effect Sizes","text":"1994, Cohen (1994) reflected reason confidence intervals rarely reported: \"suspect main reason reported embarrassingly large!\" might , another reason might statistical software rarely provided confidence intervals around effect sizes time Cohen wrote article. become increasingly easy report confidence intervals popularity free software packages R, even though packages might provide solutions statistical tests yet. Journal Article Reporting Standards recommend report \"effect-size estimates confidence intervals estimates correspond inferential test conducted, possible\".One easy solution calculating effect sizes confidence intervals MOTE made Dr. Erin Buchanan lab. website comes full collection tutorials, comparisons software packages, demonstration videos giving accessible overviews compute effect sizes confidence intervals wide range tests based summary statistics. means whichever software use perform statistical tests, can enter sample sizes means, standard deviations, test statistics compute effect sizes confidence intervals. example, video gives overview compute confidence interval around Cohen's d independent t-test.MOTE also available R package (Buchanan et al., 2017). Although many solutions exists compute Cohen's d, MOTE sets apart allowing researchers compute effect sizes confidence intervals many additional effect sizes, (partial) omega squared subjects ANOVA (\\(\\omega^{2}\\) \\(\\omega^{2}_p\\)), generalized omega squared ANOVA (\\(\\omega^{2}_G\\)), epsilon squared ANOVA (\\(\\varepsilon^{2}\\)) (partial) generalized eta squared ANOVA (\\(\\eta^{2}_G\\)).MBESS another R package range options compute effect sizes confidence intervals (Kelley, 2007). code reproduces example MOTE .feel comfortable analyzing data R, effectsize package offers complete set convenient solutions compute effect sizes confidence intervals (Ben-Shachar et al., 2020a).personally impressed way effectsize package incorporates state art (although might bit biased). example, recommendation , default, use Welch's t-test instead students t-test (Delacre et al., 2017), based recent simulation study recommended report Hedges’ \\(g_s^*\\) effect size Welch's t-test (Delacre et al., 2021), effectsize package first incorporate .Free statistical software jamovi JASP strong alternatives SPSS (unlike SPSS) allows users compute Cohen's d confidence interval independent dependent t-tests.jamovi, ESCI module allows users compute effect sizes confidence intervals, accompanied educational material focuses estimation less testing (Cumming & Calin-Jageman, 2016).\nFigure 7.7: Output ESCI module jamovi.\nJASP offers wide range frequentist Bayesian analyses, addition Cohen's d also allows users compute omega squared \\(\\omega^{2}\\), less biased version \\(\\eta^{2}\\) (Albers & Lakens, 2018; Okada, 2013).\nFigure 7.8: JASP menu option allows select Cohen's d CI around .\n\nFigure 7.9: JASP output returns Cohen's d confidence interval around .\n","code":"\nMOTE::d.ind.t(m1 = 1.7, m2 = 2.1, sd1 = 1.01, sd2 = 0.96, n1 = 77, n2 = 78, a = .05)$estimate## [1] \"$d_s$ = -0.41, 95\\\\% CI [-0.72, -0.09]\"\nMBESS::smd(Mean.1 = 1.7, Mean.2 = 2.1, s.1 = 1.01, s.2 = 0.96, n.1 = 77, n.2 = 78)## [1] -0.406028\nset.seed(33)\nx <- rnorm(n = 20, mean = 0, sd = 2.5) # create sample from normal distribution\ny <- rnorm(n = 200, mean = 1.5, sd = 3.5) # create sample from normal distribution\n\neffectsize::cohens_d(x, y)\neffectsize::cohens_d(x, y, pooled_sd = FALSE)"},{"path":"confint.html","id":"test-yourself-5","chapter":"7 Confidence Intervals","heading":"7.11 Test Yourself","text":"Q1: Go online app Kristoffer Magnusson:\nhttp://rpsychologist.com/d3/CI/. might want confidence intervals contain true population parameter 95%. Drag ‘Slide ’ button far right, see simulation 99% confidence intervals. statement true?confidence intervals larger, sample means fall closer true mean.confidence intervals smaller, sample means fall closer true mean.confidence intervals larger, sample means fall close true mean 95% confidence interval.confidence intervals smaller, sample means fall close true mean 95% confidence interval.Q2: see formulas confidence intervals, sample means confidence intervals depend sample size. can change sample size online app (see setting underneath vizualization). default, sample size set 5. Change sample size 50 (can type ). statement true?larger sample size, larger confidence intervals. sample size influence sample means vary around true population mean.larger sample size, smaller confidence intervals. sample size influence sample means vary around true population mean.larger sample size, larger confidence intervals, closer sample means true population mean.larger sample size, smaller confidence intervals, closer sample means true population mean.Q3: forest plot , see effect size (indicated square) confidence interval effect size (indicated line around effect). studies 1 4 forest plot statistically significant?\nFigure 7.10: Meta-analysis 4 studies.\nStudies 1, 2, 3, 4Only study 3None four studiesStudies 1, 2 4Q4: light black diamond bottom row fixed effects meta-analytic effect size estimate. Instead using black horizontal line, upper limit lower limit confidence interval indicated left right points diamond. center diamond meta-analytic effect size estimate. meta-analysis calculates effect size combining weighing studies. statement true?confidence interval fixed effect meta-analytic effect size estimate always wider single study, additional variation studies.confidence interval fixed effect meta-analytic effect size estimate always narrow single study, combined sample size studies included meta-analysis.confidence interval fixed effect meta-analytic effect size estimate become wider narrow compared confidence interval single study, just becomes closer true population parameter.Q5: Let’s assume researcher calculates mean 7.5, standard deviation 6.3, sample 20 people. critical value t-distribution df = 19 2.093. Calculate upper limit confidence interval around mean using formula . :\\[\n\\mu \\pm t_{df, 1-(\\alpha/2)} SE\n\\]1.402.958.9110.45Copy code R run code. generate plots like one Figure 7.4. Run entire script often want (notice variability p-values due relatively low power test!), answer following question. p-value plot tell difference statistically significant, p-value . Run simulation find p-value close p = 0.05.Q6: much two 95% confidence intervals around individual means independent groups overlap mean difference two means just statistically significant (p ≈ 0.05 alpha 0.05)?95% confidence interval around one mean contain mean group, groups always differ significantly .95% confidence interval around one mean overlap 95% confidence interval mean group, groups always differ significantly .overlap two confidence intervals around mean overlap little bit (upper bound CI overlaps lower quarter confidence interval around mean) groups differ significantly approximately p = 0.05.relationship overlap 95% confidence intervals around two independent means, p-value difference groups.Note visual overlap rule can used comparison made independent groups, dependent groups! 95% confidence interval around effect sizes therefore typically easily interpretable relation significance test.Let’s experience simulation. simulation R script generates large number additional samples, initial one plotted. simulation returns number CI contains mean (95% long run). simulation also returns % means future studies fall within 95% original study, capture percentage. differs (often lower, sometimes higher, ) confidence interval.Q7: Run simulations multiple times. Look output get R console. example: “95.077 % 95% confidence intervals contained true mean” “capture percentage plotted study, % values within observed confidence interval 88.17208 103.1506 : 82.377 %”. running simulations multiple times, look confidence interval around sample mean, relate capture percentage. Run simulation seen range means closer away true mean simulation (100). statement true?farther sample mean true population mean, lower capture percentage.farther sample mean true population mean, higher capture percentage.Q8: Simulations R randomly generated, can make specific simulation reproducible setting seed random generation process. Copy-paste “set.seed(1000)” first line R script, run simulation. sample mean 94. capture percentage? (Don’t forget remove set.seed command want generate random simulations!).95%42.1%84.3%89.2%Capture percentages rarely directly used make statistical inferences. main reason discuss really prevent common misunderstanding 95% future means fall within single confidence interval: Capture percentages clearly show true. Prediction intervals also rarely used psychology, common data science.Q9: run lines first lines code , see alpha level 0.05, 100 observations, true standard deviation 1, 95% CI [0.88; 1.16]. Change assumed population standard deviation 1 2 (st_dev <- 2). Keep settings . 95% CI around standard deviation 2 100 observations?95% CI [1.38; 3.65]95% CI [1.76; 2.32]95% CI [1.82; 2.22]95% CI [1.84; 2.20]Q10: Change assumed population standard deviation back 2 1. Lower sample size 100 20 (n <- 20). inform us width confidence interval standard deviation run pilot study 20 observations. Keep settings . 95% CI around standard deviation 1 20 observations?95% CI [0.91; 1.11]95% CI [0.82; 1.28]95% CI [0.76; 1.46]95% CI [1.52; 2.92]Q11: want 95% CI around standard deviation 1 0.05 away assumed population standard deviation, large number observations ? Note means want 95% CI fall within 0.95 1.05. notice calculations distribution sample standard deviations symmetrical. Standard deviations can’t smaller 0 (square rooted variance). practice question : smallest number observations upper 95% CI smaller 1.05? Replace n values answer options.n = 489n = 498n = 849n = 948Let’s explore consequences inaccurate estimate population standard deviation -priori power analyses. Let’s imagine want perform -priori power analysis smallest effect size interest half scale point (scale 1-5) measure (unknown) true population standard deviation 1.2.Q12: Change number observations 50. Change assumed population standard deviation 1.2. Keep effect 0.5. 95% confidence interval standard deviation based sample 50 observation ranges 1.002 1.495. perform -priori power analysis need calculate Cohen’s d, difference divided standard deviation. example, want least observe difference 0.5. Cohen’s d (effect/SD) lower bound 95% confidence interval (SD = 1.002) upper bound (SD = 1.495)?d = 0.33 d = 0.50d = 0.40 d = 0.60d = 0.43 d = 0.57d = 0.29 d = 0.55If draw sample 50 observations can happen observe value , due random variation, much smaller much larger true population value. can examine effect number observations think required perform -priori power analysis.Q13: -priori power analysis performed uses estimate Cohen’s d based lower 95% CI standard deviation. statement true?lower bound 95% CI smaller true population SD, Cohen’s d smaller, -priori power analysis yield sample size smaller sample size really need.lower bound 95% CI smaller true population SD, Cohen’s d larger, -priori power analysis yield sample size larger sample size really need.lower bound 95% CI smaller true population SD, Cohen’s d smaller, -priori power analysis yield sample size larger sample size really need.lower bound 95% CI smaller true population SD, Cohen’s d larger, -priori power analysis yield sample size smaller sample size really need.Q14: Let’s check answer previous question correct. still alpha level 0.05, n = 50, standard deviation 1.2, effect interest 0.5. Run power analyses using pwr package. first power analysis uses Cohen’s d based lower bound 95% confidence interval. second power analysis uses upper bound 95% confidence interval. (also third power analysis based (real-life situations unknown) true standard deviation, just comparison). statement true (note sample size power analysis rounded , collect partial observation)?sample size per group 68 calculating effect size based lower bound 95% CI around standard deviation, 86 using upper bound 95% CI around standard deviation.sample size per group 68 calculating effect size based lower bound 95% CI around standard deviation, 123 using upper bound 95% CI around standard deviation.sample size per group 86 calculating effect size based lower bound 95% CI around standard deviation, 123 using upper bound 95% CI around standard deviation.sample size per group 86 calculating effect size based lower bound 95% CI around standard deviation, 189 using upper bound 95% CI around standard deviation.","code":"\nx <- rnorm(n = 50, mean = 3, sd = 5) # get sample group 1\ny <- rnorm(n = 50, mean = 5, sd = 5) # get sample group 2\n\nd <- data.frame(\n  labels = c(\"X\", \"Y\", \"Difference\"),\n  mean = c(mean(x), mean(y), mean(y) - mean(x)),\n  lower = c(t.test(x)[[4]][1], t.test(y)[[4]][1], t.test(y, x)[[4]][1]),\n  upper = c(t.test(x)[[4]][2], t.test(y)[[4]][2], t.test(y, x)[[4]][2])\n)\n\nplot(NA, xlim = c(.5, 3.5), ylim = c(0, max(d$upper[1:2] + 1)), bty = \"l\", \n     xaxt = \"n\", xlab = \"\", ylab = \"Mean\")\npoints(d$mean[1:2], pch = 19)\nsegments(1, d$mean[1], 5, d$mean[1], lty = 2)\nsegments(2, d$mean[2], 5, d$mean[2], lty = 2)\naxis(1, 1:3, d$labels)\nsegments(1:2, d$lower[1:2], 1:2, d$upper[1:2])\naxis(4, seq((d$mean[1] - 3), (d$mean[1] + 5), by = 1), seq(-3, 5, by = 1))\npoints(3, d$mean[1] + d$mean[3], pch = 19, cex = 1.5)\nsegments(3, d$mean[1] + d$lower[3], 3, d$mean[1] + d$upper[3], lwd = 2)\nmtext(\"Difference\", side = 4, at = d$mean[1], line = 3)\nsegments(1:1, d$upper[1:1], 1:2, d$upper[1:1], lty = 3)\nsegments(1:1, d$lower[1:2], 1:2, d$lower[1:2], lty = 3)\ntext(3, 1, paste(\"P-value\", round(t.test(x, y)$p.value, digits = 3)))\nlibrary(ggplot2)\n\nn <- 20 # set sample size\nnsims <- 100000 # set number of simulations\n\nx <- rnorm(n = n, mean = 100, sd = 15) # create sample from normal distribution\n\n# 95% Confidence Interval\nciu <- mean(x) + qt(0.975, df = n - 1) * sd(x) * sqrt(1 / n)\ncil <- mean(x) - qt(0.975, df = n - 1) * sd(x) * sqrt(1 / n)\n\n# 95% Prediction Interval\npiu <- mean(x) + qt(0.975, df = n - 1) * sd(x) * sqrt(1 + 1 / n)\npil <- mean(x) - qt(0.975, df = n - 1) * sd(x) * sqrt(1 + 1 / n)\n\nggplot(as.data.frame(x), aes(x)) + # plot data\n  geom_rect(aes(xmin = pil, xmax = piu, ymin = 0, ymax = Inf),\n            fill = \"gold\") + # draw yellow PI area\n  geom_rect(aes(xmin = cil, xmax = ciu, ymin = 0, ymax = Inf),\n            fill = \"#E69F00\") + # draw orange CI area\n  geom_histogram(colour = \"black\", fill = \"grey\", aes(y = ..density..), bins = 20) +\n  xlab(\"Score\") +\n  ylab(\"frequency\") +\n  theme_bw(base_size = 20) +\n  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(),\n        panel.grid.minor.x = element_blank()) + \n  geom_vline(xintercept = mean(x), linetype = \"dashed\", size = 1) +\n  coord_cartesian(xlim = c(50, 150)) +\n  scale_x_continuous(breaks = c(seq(50, 150, 10))) +\n  annotate(\"text\", x = mean(x), y = 0.02, label = paste(\n    \"Mean = \", round(mean(x)), \"\\n\",\n    \"SD = \", round(sd(x)), sep = \"\"), size = 6.5)\n\n# Simulate Confidence Intervals\nciu_sim <- numeric(nsims)\ncil_sim <- numeric(nsims)\nmean_sim <- numeric(nsims)\n\nfor (i in 1:nsims) { # for each simulated experiment\n  x <- rnorm(n = n, mean = 100, sd = 15) # create sample from normal distribution\n  ciu_sim[i] <- mean(x) + qt(0.975, df = n - 1) * sd(x) * sqrt(1 / n)\n  cil_sim[i] <- mean(x) - qt(0.975, df = n - 1) * sd(x) * sqrt(1 / n)\n  mean_sim[i] <- mean(x) # store means of each sample\n}\n\n# Save only those simulations where the true value was inside the 95% CI\nciu_sim <- ciu_sim[ciu_sim < 100]\ncil_sim <- cil_sim[cil_sim > 100]\n\n# Calculate how many times the observed mean fell within the 95% CI of the original study\nmean_sim <- mean_sim[mean_sim > cil & mean_sim < ciu]\n\ncat((100 * (1 - (length(ciu_sim) / nsims + length(cil_sim) / nsims))),\n    \"% of the 95% confidence intervals contained the true mean\")\ncat(\"The capture percentage for the plotted study, or the % of values within\n    the observed confidence interval from\", cil, \"to\", ciu,\n    \"is:\", 100 * length(mean_sim) / nsims, \"%\")\nalpha_level <- 0.05 # set alpha level\nn <- 100 # set number of observations\nst_dev <- 1 # set true standard deviation\neffect <- 0.5 # set effect size (raw mean difference)\n\n# calculate lower and upper critical values c_l and c_u\nc_l <- sqrt((n - 1)/qchisq(alpha_level/2, n - 1, lower.tail = FALSE))\nc_u <- sqrt((n - 1)/qchisq(alpha_level/2, n - 1, lower.tail = TRUE))\n\n# calculate lower and upper confidence interval for sd\nst_dev * c_l\nst_dev * c_u\n\n# d based on lower bound of the 95CI around the SD\neffect/(st_dev * c_l)\n# d based on upper bound of the 95CI around the SD\neffect/(st_dev * c_u)\n\npwr::pwr.t.test(d = effect/(st_dev * c_l), power = 0.9, sig.level = 0.05)\npwr::pwr.t.test(d = effect/(st_dev * c_u), power = 0.9, sig.level = 0.05)\n\n# Power analysis for true standard deviation for comparison\npwr::pwr.t.test(d = effect/st_dev, power = 0.9, sig.level = 0.05)"},{"path":"confint.html","id":"open-questions-5","chapter":"7 Confidence Intervals","heading":"7.11.1 Open Questions","text":"definition confidence interval?definition confidence interval?confidence interval related statistical significance?confidence interval related statistical significance?happens confidence interval sample size increases?happens confidence interval sample size increases?difference confidence interval capture percentage?difference confidence interval capture percentage?prediction interval?prediction interval?data entire population, need calculate confidence interval?data entire population, need calculate confidence interval?confidence intervals statement ?confidence intervals statement ?mean say collected data, confidence interval either contains true parameter, doesn’t?mean say collected data, confidence interval either contains true parameter, doesn’t?difference estimates small vs. large samples?difference estimates small vs. large samples?","code":""},{"path":"power.html","id":"power","chapter":"8 Sample Size Justification","heading":"8 Sample Size Justification","text":"can listen audio recording chapter .Scientists perform empirical studies collect data helps answer research question. data collected, informative study respect inferential goals. sample size justification consider informative data given inferential goal, estimating effect size, testing hypothesis. Even though sample size justification sometimes requested manuscript submission guidelines, submitting grant funder, submitting proposal ethical review board, number observations often simply stated, justified. makes difficult evaluate informative study . prevent concerns emerging late (e.g., non-significant hypothesis test observed), researchers carefully justify sample size data collected. chapter, largely identical Lakens (2022b), explore detail justify sample size.Table 8.1: Overview possible justifications sample size study.","code":""},{"path":"power.html","id":"six-approaches-to-justify-sample-sizes","chapter":"8 Sample Size Justification","heading":"8.1 Six Approaches to Justify Sample Sizes","text":"Researchers often find difficult justify sample size (.e., number participants, observations, combination thereof). review article six possible approaches discussed can used justify sample size quantitative study (see Table 8.1). exhaustive overview, includes common applicable approaches single studies. topic power analysis meta-analyses outside scope chapter, see Hedges & Pigott (2001) Valentine et al. (2010). first justification data (almost) entire population collected. second justification centers resource constraints, almost always present, rarely explicitly evaluated. third fourth justifications based desired statistical power desired accuracy. fifth justification relies heuristics, finally, researchers can choose sample size without justification. justifications can stronger weaker depending conclusions researchers want draw data plan collect.approaches justification sample sizes, even 'justification' approach, give others insight reasons led decision sample size study. surprising 'heuristics' 'justification' approaches often unlikely impress peers. However, important note value information collected depends extent final sample size allows researcher achieve inferential goals, sample size justification chosen.extent approaches make researchers judge data collected informative depends details question researcher aimed answer parameters chose determining sample size study. example, badly performed -priori power analysis can quickly lead study low informational value. six justifications mutually exclusive, multiple approaches can considered designing study.","code":""},{"path":"power.html","id":"six-ways-to-evaluate-which-effect-sizes-are-interesting","chapter":"8 Sample Size Justification","heading":"8.2 Six Ways to Evaluate Which Effect Sizes are Interesting","text":"informativeness data collected depends inferential goals researcher , cases, inferential goals scientific peers . shared feature different inferential goals considered review article question effect sizes researcher considers meaningful distinguish. implies researchers need evaluate effect sizes consider interesting. evaluations rely combination statistical properties domain knowledge. Table 8.2 six possibly useful considerations provided. intended exhaustive overview, presents common useful approaches can applied practice. evaluations equally relevant types sample size justifications. online Shiny app accompanying Lakens (2022b) provides researchers interactive form guides researchers considerations sample size justification. considerations often rely information (e.g., effect sizes, number observations, standard deviation, etc.) six considerations seen set complementary approaches can used evaluate effect sizes interest.start, researchers consider smallest effect size interest . Second, although relevant performing hypothesis test, researchers consider effect sizes statistically significant given choice alpha level sample size. Third, important consider (range ) effect sizes expected. requires careful consideration source expectation presence possible biases expectations. Fourth, useful consider width confidence interval around possible values effect size population, whether can expect confidence interval reject effects considered -priori plausible. Fifth, worth evaluating power test across wide range possible effect sizes sensitivity power analysis. Sixth, researcher can consider effect size distribution related studies literature.\nTable 8.2: Overview possible ways evaluate effect sizes interesting.\n","code":""},{"path":"power.html","id":"the-value-of-information","chapter":"8 Sample Size Justification","heading":"8.3 The Value of Information","text":"Since scientists faced resource limitations, need balance cost collecting additional datapoint increase information datapoint provides. referred value information (Eckermann et al., 2010). Calculating value information notoriously difficult (Detsky, 1990). Researchers need specify cost collecting data, weigh costs data collection increase utility access data provides. value information perspective every data point can collected equally valuable (J. Halpern et al., 2001; Wilson, 2015). Whenever additional observations change inferences meaningful way, costs data collection can outweigh benefits.value additional information cases non-monotonic function, especially depends multiple inferential goals. researcher might interested comparing effect previously observed large effect literature, theoretically predicted medium effect, smallest effect practically relevant. situation expected value sampling information lead different optimal sample sizes inferential goal. valuable collect informative data large effect, additional data less (even negative) marginal utility, point data becomes increasingly informative medium effect size, value sampling additional information decreasing study becomes increasingly informative presence absence smallest effect interest.difficulty quantifying value information, scientists typically use less formal approaches justify amount data set collect study. Even though cost-benefit analysis always made explicit reported sample size justifications, value information perspective almost always implicitly underlying framework sample size justifications based . Throughout subsequent discussion sample size justifications, importance considering value information given inferential goals repeatedly highlighted.","code":""},{"path":"power.html","id":"measuring-almost-the-entire-population","chapter":"8 Sample Size Justification","heading":"8.4 Measuring (Almost) the Entire Population","text":"instances might possible collect data (almost) entire population investigation. example, researchers might use census data, able collect data employees firm study small population top athletes. Whenever possible measure entire population, sample size justification becomes straightforward: researcher used data available.","code":""},{"path":"power.html","id":"resource-constraints","chapter":"8 Sample Size Justification","heading":"8.5 Resource Constraints","text":"common reason number observations study resource constraints limit amount data can collected reasonable cost (Lenth, 2001). practice, sample sizes always limited resources available. Researchers practically always resource limitations, therefore even resource constraints primary justification sample size study, always secondary justification.Despite omnipresence resource limitations, topic often receives little attention texts experimental design (example exception, see Bulus & Dong (2021)). might make feel like acknowledging resource constraints appropriate, opposite true: resource limitations always play role, responsible scientist carefully evaluates resource constraints designing study. Resource constraint justifications based trade-costs data collection, value access information data provides. Even researchers explicitly quantify trade-, revealed actions. example, researchers rarely spend resources single study. Given resource constraints, researchers confronted optimization problem spend resources across multiple research questions.Time money two resource limitations scientists face. PhD student certain time complete PhD thesis, typically expected complete multiple research lines time. addition time limitations, researchers limited financial resources often directly influence much data can collected. third limitation research lines might simply small number individuals data can collected, studying patients rare disease. resource constraint justification puts limited resources center justification sample size collected, starts resources scientist available. resources translated expected number observations (N) researcher expects able collect amount money given time. challenge evaluate whether collecting N observations worthwhile. decide study informative, conclude data collection worthwhile?evaluating whether resource constraints make data collection uninformative, researchers need explicitly consider inferential goals collecting data (Parker & Berman, 2003). data always provides knowledge research question data, absolute sense, data collected value. However, possible benefits collecting data outweighed costs data collection.straightforward evaluate whether data collection value know certain someone make decision, without data. situations additional data reduce error rates well-calibrated decision process, even ever slightly. example, without data perform better coin flip guess two conditions higher true mean score measure. data, can perform better coin flip picking condition highest mean. small amount data still likely make mistake, error rate smaller without data. cases, value information might positive, long reduction error rates beneficial cost data collection.Another way small dataset can valuable existence eventually makes possible perform meta-analysis (Maxwell & Kelley, 2011). argument favor collecting small dataset requires 1) researchers share data way future meta-analyst can find , 2) decent probability someone perform high-quality meta-analysis include data future (S. D. Halpern et al., 2002). uncertainty whether ever meta-analysis weighed costs data collection.One way increase probability future meta-analysis researchers commit performing meta-analysis , combining several studies performed small-scale meta-analysis (Cumming, 2014). example, researcher might plan repeat study next 12 years class teach, expectation 12 years meta-analysis 12 studies sufficient draw informative inferences (see ter Schure & Grünwald (2019)). plausible researcher collect required data , can attempt set collaboration fellow researchers field commit collecting similar data identical measures. likely sufficient data emerge time reach inferential goals, might value collecting data.Even researcher believes worth collecting data future meta-analysis performed, likely perform statistical test data. make sure expectations results test well-calibrated, important consider effect sizes interest, perform sensitivity power analysis evaluate probability Type II error effects interest. six ways evaluate effect sizes interesting discussed second part review, useful consider smallest effect size can statistically significant, expected width confidence interval around effect size, effects can expected specific research area, evaluate power effect sizes sensitivity power analysis. decision claim made, compromise power analysis worthwhile consider deciding upon error rates planning study. reporting resource constraints sample size justification recommended address five considerations Table 8.3. Addressing points explicitly facilitates evaluating data worthwhile collect. make easier address relevant points explicitly, interactive form implement recommendations chapter can found https://shiny.ieis.tue.nl/sample_size_justification/.\nTable 8.3: Overview recommendations reporting sample size justification based resource constraints.\n","code":""},{"path":"power.html","id":"aprioripower","chapter":"8 Sample Size Justification","heading":"8.6 A-priori Power Analysis","text":"designing study goal test whether statistically significant effect present, researchers often want make sure sample size large enough prevent erroneous conclusions range effect sizes care . approach justifying sample size, value information collect observations point probability erroneous inference , long run, larger desired value. researcher performs hypothesis test, four possible outcomes:false positive (Type error), determined \\(\\alpha\\) level. test yields significant result, even though null hypothesis true.false negative (Type II error), determined \\(\\beta\\), 1 - power. test yields non-significant result, even though alternative hypothesis true.true negative, determined 1-\\(\\alpha\\). test yields non-significant result null hypothesis true.true positive, determined 1-\\(\\beta\\). test yields significant result alternative hypothesis true.Given specified effect size, alpha level, power, -priori power analysis can used calculate number observations required achieve desired error rates, given effect size. Power analyses can performed based standardized effect sizes effect sizes expressed original scale. important know standard deviation effect (see 'Know Measure' section) find slightly convenient talk standardized effects context sample size justifications. Figure 8.1 illustrates statistical power increases number observations (per group) increases independent t test two-sided alpha level 0.05. interested detecting effect d = 0.5, sample size 90 per condition give us 90% power. Statistical power can computed determine number participants, number items (Westfall et al., 2014) can also performed single case studies (Ferron & Onghena, 1996; McIntosh & Rittmo, 2021).Although common set Type error rate 5% aim 80% power, error rates justified (Lakens, Adolfi, et al., 2018). explained section compromise power analysis, default recommendation aim 80% power lacks solid justification. general, lower error rates (thus higher power), informative study , resources required. Researchers carefully weigh costs increasing sample size benefits lower error rates, probably make studies designed achieve power 90% 95% common articles reporting single study. additional consideration whether researcher plans publish article consisting set replication extension studies, case probability observing multiple Type errors low, probability observing mixed results even true effect increases (Lakens & Etz, 2017), also reason aim studies low Type II error rates, perhaps even slightly increasing alpha level individual study.\nFigure 8.1: Power curve independent t test effect d = 0.5 α = 0.05 function sample size.\nFigure 8.2 visualizes two distributions. left distribution (dashed line) centered 0. model null hypothesis. null hypothesis true statistically significant result observed effect size extreme enough (two-sided test either positive negative direction), significant result Type error (dark grey areas curve). true effect, formally statistical power null hypothesis significance test undefined. significant effects observed null hypothesis true Type errors, false positives, occur chosen alpha level. right distribution (solid line) centered effect d = 0.5. specified model alternative hypothesis study, illustrating expectation effect d = 0.5 alternative hypothesis true. Even though true effect, studies always find statistically significant result. happens , due random variation, observed effect size close 0 statistically significant. results false negatives (light grey area curve right). increase power, can collect larger sample size. sample size increases, distributions become narrow, reducing probability Type II error. figures can reproduced adapted online shiny app: http://shiny.ieis.tue.nl/d_p_power/.\nFigure 8.2: Null (d = 0, grey dashed line) alternative (d = 0.5, solid black line) hypothesis, α = 0.05 n = 80 per group.\nimportant highlight goal -priori power analysis achieve sufficient power true effect size. true effect size unknown. goal -priori power analysis achieve sufficient power, given specific assumption effect size researcher wants detect. Just like Type error rate maximum probability making Type error conditional assumption null hypothesis true, -priori power analysis computed assumption specific effect size. unknown assumption correct. researcher can make sure assumptions well justified. Statistical inferences based test Type II error rate controlled conditional assumption specific effect size. allow inference , assuming true effect size least large used -priori power analysis, maximum Type II error rate study larger desired value.point perhaps best illustrated consider study -priori power analysis performed test presence effect, test absence effect. designing study, essential consider possibility effect (e.g., mean difference zero). -priori power analysis can performed null hypothesis significance test, test absence meaningful effect, equivalence test can statistically provide support null hypothesis rejecting presence effects large enough matter (Lakens, 2017; Meyners, 2012; J. L. Rogers et al., 1993). multiple primary tests performed based sample, analysis requires dedicated sample size justification. possible, sample size collected guarantees tests informative, means collected sample size based largest sample size returned -priori power analyses.example, goal study detect reject effect size d = 0.4 90% power, alpha level set 0.05 two-sided independent t test, researcher need collect 133 participants condition informative null hypothesis test, 136 participants condition informative equivalence test. Therefore, researcher aim collect 272 (, 136 participants condition) participants total informative result tests planned. guarantee study sufficient power true effect size (can never known), guarantees study sufficient power given assumption effect researcher interested detecting rejecting. Therefore, -priori power analysis useful, long researcher can justify effect sizes interested .researchers correct alpha level testing multiple hypotheses, -priori power analysis based corrected alpha level. example, four tests performed, overall Type error rate 5% desired, Bonferroni correction used, -priori power analysis based corrected alpha level .0125.-priori power analysis can performed analytically performing computer simulations. Analytic solutions faster less flexible. common challenge researchers face attempting perform power analyses complex uncommon tests available software offer analytic solutions. cases simulations can provide flexible solution perform power analyses test (Morris et al., 2019). following code example power analysis R based 10000 simulations one-sample t test zero sample size 20, assuming true effect d = 0.5. simulations consist first randomly generating data based assumptions data generating mechanism (e.g., normal distribution mean 0.5 standard deviation 1), followed test performed data. computing percentage significant results, power can computed design.wide range tools available perform power analyses. Whichever tool researcher decides use, take time learn use software correctly perform meaningful -priori power analysis. Resources educate psychologists power analysis consist book-length treatments (Aberson, 2019; Cohen, 1988; Julious, 2004; Murphy et al., 2014), general introductions (Baguley, 2004; Brysbaert, 2019; Faul et al., 2007; Maxwell et al., 2008; Perugini et al., 2018), increasing number applied tutorials specific tests (Brysbaert & Stevens, 2018; DeBruine & Barr, 2021; P. Green & MacLeod, 2016; Kruschke, 2013; Lakens & Caldwell, 2021; Schoemann et al., 2017; Westfall et al., 2014). important trained basics power analysis, can extremely beneficial learn perform simulation-based power analyses. time, often recommended enlist help expert, especially researcher lacks experience power analysis specific test.reporting -priori power analysis, make sure power analysis completely reproducible. power analyses performed R possible share analysis script information version package. many software packages possible export power analysis performed PDF file. example, G*Power analyses can exported 'protocol power analysis' tab. software package provides way export analysis, add screenshot power analysis supplementary files.\nFigure 8.3: details power analysis performed can exported G*Power.\nreproducible report needs accompanied justifications choices made respect values used power analysis. effect size used power analysis based previous research, factors presented Table 8.5 (effect size based meta-analysis) Table 8.6 (effect size based single study) discussed. effect size estimate based existing literature, provide full citation, preferably direct quote article effect size estimate reported. effect size based smallest effect size interest, value just stated, justified (e.g., based theoretical predictions practical implications, see Lakens, Scheel, et al. (2018)). overview aspects reported describing -priori power analysis, see Table 8.4.\nTable 8.4: Overview recommendations reporting -priori power analysis.\n","code":"\np <- numeric(10000)   # to store p-values\nfor (i in 1:10000) {  # simulate 10k tests\n  x <- rnorm(n = 20, mean = 0.5, sd = 1)\n  p[i] <- t.test(x)$p.value # store p-value\n}\nsum(p < 0.05) / 10000 # Compute power"},{"path":"power.html","id":"planprecision","chapter":"8 Sample Size Justification","heading":"8.7 Planning for Precision","text":"researchers suggested justify sample sizes based desired level precision estimate (Cumming & Calin-Jageman, 2016; Kruschke, 2018; Maxwell et al., 2008). goal justifying sample size based precision collect data achieve desired width confidence interval around parameter estimate. width confidence interval around parameter estimate depends standard deviation number observations. aspect researcher needs justify sample size justification based accuracy desired width confidence interval respect inferential goal, assumption population standard deviation measure.researcher determined desired accuracy, good estimate true standard deviation measure, straightforward calculate sample size needed desired level accuracy. example, measuring IQ group individuals researcher might desire estimate IQ score within error range 2 IQ points 95% observed means, long run. required sample size achieve desired level accuracy (assuming normally distributed data) can computed :\\[N = \\left(\\frac{z \\cdot sd}{error}\\right)^2\\]N number observations, z critical value related desired confidence interval, sd standard deviation IQ scores population, error width confidence interval within mean fall, desired error rate. example, (1.96 × 15 / 2)^2 = 216.1 observations. researcher desires 95% means fall within 2 IQ point range around true population mean, 217 observations collected. desired accuracy non-zero mean difference computed, accuracy based non-central t-distribution. calculations, expected effect size estimate needs provided, relatively little influence required sample size (Maxwell et al., 2008). also possible incorporate uncertainty observed effect size sample size calculation, known assurance (Kelley & Rausch, 2006). MBESS package R provides functions compute sample sizes wide range tests (Kelley, 2007).less straightforward justify desired level accuracy related inferential goals. literature helps researchers choose desired width confidence interval. Morey (2020) convincingly argues practical use-cases planning precision involve inferential goal distinguishing observed effect effect sizes (Bayesian perspective, see Kruschke (2018)). example, researcher might expect effect size r = 0.4 treat observed correlations differ 0.2 (.e., 0.2 < r < 0.6) differently, effects r = 0.6 larger considered large caused assumed underlying mechanism (Hilgard, 2021), effects smaller r = 0.2 considered small support theoretical prediction. goal indeed get effect size estimate precise enough two effects can differentiated high probability, inferential goal actually hypothesis test, requires designing study sufficient power reject effects (e.g., testing range prediction correlations 0.2 0.6).researchers want test hypothesis, example prefer estimation approach testing approach, absence clear guidelines help researchers justify desired level precision, one solution might rely generally accepted norm precision. norm based ideas certain resolution measurements research area longer lead noticeably different inferences. Just researchers normatively use alpha level 0.05, plan studies achieve desired confidence interval width around observed effect determined norm. Future work needed help researchers choose confidence interval width planning accuracy (see also section confidence interval use Bayesian tests range predictions).","code":""},{"path":"power.html","id":"heuristics","chapter":"8 Sample Size Justification","heading":"8.8 Heuristics","text":"researcher uses heuristic, able justify sample size , trust sample size recommended authority. started PhD student 2005 common collect 15 participants subject condition. asked common practice, one really sure, people trusted justification somewhere literature. Now, realize justification heuristics used. Berkeley (1735) already observed: \"Men learn elements science others: every learner hath deference less authority, especially young learners, kind caring dwell long upon principles, inclining rather take upon trust: things early admitted repetition become familiar: familiarity length passeth evidence.\"papers provide researchers simple rules thumb sample size collected. papers clearly fill need, cited lot, even advice articles flawed. example, Wilson VanVoorhis & Morgan (2007) translate absolute minimum 50+8 observations regression analyses suggested rule thumb examined S. B. Green (1991) recommendation collect ~50 observations. Green actually concludes article \"summary, specific minimum number subjects minimum ratio subjects--predictors supported\". discuss general rule thumb N = 50 + 8 provided accurate minimum number observations 'typical' study social sciences 'medium' effect size, Green claims citing Cohen (1988). Cohen actually claim typical study social sciences 'medium' effect size, instead said (1988, p. 13): \"Many effects sought personality, social, clinical-psychological research likely small effects defined\". see string mis-citations eventually leads misleading rule thumb.Rules thumb seem primarily emerge due mis-citations /overly simplistic recommendations. Simonsohn, Nelson, Simmons (2011) recommended \"Authors must collect least 20 observations per cell\". later recommendation authors presented conference suggested use n > 50, unless study large effects (Simmons et al., 2013). Regrettably, advice now often mis-cited justification collect 50 observations per condition without considering expected effect size. authors justify specific sample size (e.g., n = 50) based general recommendation another paper, either mis-citing paper, paper citing flawed.Another common heuristic collect number observations collected previous study. strategy recommended scientific disciplines widespread publication bias, /novel surprising findings largely exploratory single studies published. Using sample size previous study valid approach sample size justification previous study also applies current study. Instead stating intend collect sample size earlier study, repeat sample size justification, update light new information (effect size earlier study, see Table 8.6).Peer reviewers editors carefully scrutinize rules thumb sample size justifications, can make seem like study high informational value inferential goal even study yield uninformative results. Whenever one encounters sample size justification based heuristic, ask : 'heuristic used?' important know logic behind heuristic determine whether heuristic valid specific situation. cases, heuristics based weak logic, widely applicable. said, might possible fields develop valid heuristics sample size justifications. example, possible research area reaches widespread agreement effects smaller d = 0.3 small interest, studies field use sequential designs (see ) 90% power detect d = 0.3. Alternatively, possible field agrees data collected desired level accuracy, irrespective true effect size. cases, valid heuristics exist based generally agreed goals data collection. example, Simonsohn (2015) suggests design replication studies 2.5 times large sample sizes original study, provides 80% power equivalence test equivalence bound set effect original study 33% power detect, assuming true effect size 0. original authors typically specify effect size falsify hypothesis, heuristic underlying 'small telescopes' approach good starting point replication study inferential goal reject presence effect large described earlier publication. responsibility researchers gain knowledge distinguish valid heuristics mindless heuristics, able evaluate whether heuristic yield informative result given inferential goal researchers specific study, .","code":""},{"path":"power.html","id":"no-justification","chapter":"8 Sample Size Justification","heading":"8.9 No Justification","text":"might sound like contradictio terminis, useful distinguish final category researchers explicitly state justification sample size. Perhaps resources available collect data, used. researcher performed power analysis, planned precision, . cases, instead pretending justification sample size, honesty requires state sample size justification. necessarily bad. still possible discuss smallest effect size interest, minimal statistically detectable effect, width confidence interval around effect size, plot sensitivity power analysis, relation sample size collected. researcher truly specific inferential goals collecting data, evaluation can perhaps performed based reasonable inferential goals peers learn existence collected data.try spin story looks like study highly informative . Instead, transparently evaluate informative study given effect sizes interest, make sure conclusions follow data. lack sample size justification might problematic, might mean study informative effect sizes interest, makes especially difficult interpret non-significant effects, estimates large uncertainty.","code":""},{"path":"power.html","id":"what-is-your-inferential-goal","chapter":"8 Sample Size Justification","heading":"8.10 What is Your Inferential Goal?","text":"inferential goal data collection often way related size effect. Therefore, design informative study, researchers want think effect sizes interesting. First, useful consider three effect sizes determining sample size. first smallest effect size researcher interested , second smallest effect size can statistically significant (studies significance test performed), third effect size expected. Beyond considering three effect sizes, can useful evaluate ranges effect sizes. can done computing width expected confidence interval around effect size interest (example, effect size zero), examine effects rejected. Similarly, can useful plot sensitivity curve evaluate range effect sizes design decent power detect, well consider range effects design low power. Finally, situations useful consider range effects likely observed specific research area.","code":""},{"path":"power.html","id":"what-is-the-smallest-effect-size-of-interest","chapter":"8 Sample Size Justification","heading":"8.11 What is the Smallest Effect Size of Interest?","text":"strongest possible sample size justification based explicit statement smallest effect size considered interesting. smallest effect size interest can based theoretical predictions practical considerations. review approaches can used determine smallest effect size interest randomized controlled trials, see J. Cook et al. (2014) Keefe et al. (2013), reviews different methods determine smallest effect size interest, see King (2011) Copay et al. (2007), discussion focused psychological research, see Lakens, Scheel, et al. (2018).can challenging determine smallest effect size interest whenever theories developed, research question far removed practical applications, still worth thinking effects small matter. first step forward discuss effect sizes considered meaningful specific research line peers. Researchers differ effect sizes consider large enough worthwhile (Murphy et al., 2014). Just every scientist find every research question interesting enough study, every scientist consider effect sizes interesting enough study, different stakeholders differ effect sizes considered meaningful (Kelley & Preacher, 2012).Even though might challenging, important benefits able specify smallest effect size interest. population effect size always uncertain (indeed, estimating typically one goals study), therefore whenever study powered expected effect size, considerable uncertainty whether statistical power high enough detect true effect population. However, smallest effect size interest can specified agreed upon careful deliberation, becomes possible design study sufficient power (given inferential goal detect reject smallest effect size interest certain error rate). Put differently, although smallest effect interest may subjective (one researcher might find effect sizes smaller d = 0.3 meaningless, another researcher might still interested effects smaller d = 0.1), might uncertainty parameters required specify smallest effect size interest (e.g., performing cost-benefit analysis), researchers determine smallest effect size interest, study can designed known Type II error rate detect reject value. reason -priori power based smallest effect size interest generally preferred, whenever researchers able specify one (Aberson, 2019; Albers & Lakens, 2018; G. W. Brown, 1983; Cascio & Zedeck, 1983; Dienes, 2014; Lenth, 2001).","code":""},{"path":"power.html","id":"minimaldetectable","chapter":"8 Sample Size Justification","heading":"8.12 The Minimal Statistically Detectable Effect","text":"minimal statistically detectable effect, critical effect size, provides information smallest effect size , observed, statistically significant given specified alpha level sample size (J. Cook et al., 2014). critical t value (e.g., t = 1.96 \\(\\alpha\\) = 0.05, large sample sizes) can compute critical mean difference (Phillips et al., 2001), critical standardized effect size. two-sided independent t test critical mean difference :\\[M_{crit} = t_{crit}{\\sqrt{\\frac{sd_1^2}{n_1} + \\frac{sd_2^2}{n_2}}}\\]corresponding critical standardized mean difference :\\[d_{crit} = t_{crit}{\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]Figure 8.4, distribution Cohen’s d plotted 15 participants per group true effect size either d = 0 d = 0.5. figure similar Figure 8.2, addition critical d indicated. see small number observations group observed effects larger d = 0.75 statistically significant. Whether effect sizes interesting, can realistically expected, carefully considered justified.\nFigure 8.4: Critical effect size independent t test n = 15 per group \\(\\alpha\\) = 0.05.\nG*Power provides critical test statistic (critical t value) performing power analysis. example, Figure 8.5 shows correlation based two-sided test, \\(\\alpha\\) = 0.05, N = 30, effects larger r = 0.361 smaller r = -0.361 can statistically significant. reveals sample size relatively small, observed effect needs quite substantial statistically significant.\nFigure 8.5: critical correlation test based total sample size 30 α = 0.05 calculated G*Power.\nimportant realize due random variation study probability yield effects larger critical effect size, even true effect size small (even true effect size 0, case significant effect Type error). Computing minimal statistically detectable effect useful study -priori power analysis performed, studies published literature report sample size justification (Lakens, Scheel, et al., 2018), researchers rely heuristics sample size justification.can informative ask whether critical effect size study design within range effect sizes can realistically expected. , whenever significant effect observed published study, either effect size surprisingly larger expected, likely, upwardly biased effect size estimate. latter case, given publication bias, published studies lead biased effect size estimates. still possible increase sample size, example ignoring rules thumb instead performing -priori power analysis, . possible increase sample size, example due resource constraints, reflecting minimal statistically detectable effect make clear analysis data focus p values, effect size confidence interval (see Table 8.3).also useful compute minimal statistically detectable effect 'optimistic' power analysis performed. example, believe best case scenario true effect size d = 0.57 use optimistic expectation -priori power analysis, effects smaller d = 0.4 statistically significant collect 50 observations two independent group design. worst case scenario alternative hypothesis true effect size d = 0.35 design allow declare significant effect effect size estimates close worst case scenario observed. Taking account minimal statistically detectable effect size make reflect whether hypothesis test yield informative answer, whether current approach sample size justification (e.g., use rules thumb, letting resource constraints determine sample size) leads informative study, .","code":""},{"path":"power.html","id":"what-is-the-expected-effect-size","chapter":"8 Sample Size Justification","heading":"8.13 What is the Expected Effect Size?","text":"Although true population effect size always unknown, situations researchers reasonable expectation effect size study, want use expected effect size -priori power analysis. Even expectations observed effect size largely guess, always useful explicitly consider effect sizes expected. researcher can justify sample size based effect size expect, even study informative respect smallest effect size interest. cases study informative one inferential goal (testing whether expected effect size present absent), highly informative second goal (testing whether smallest effect size interest present absent).typically three sources expectations population effect size: meta-analysis, previous study, theoretical model. tempting researchers overly optimistic expected effect size -priori power analysis, higher effect size estimates yield lower sample sizes, optimistic increases probability observing false negative result. reviewing sample size justification based -priori power analysis, important critically evaluate justification expected effect size used power analyses.","code":""},{"path":"power.html","id":"using-an-estimate-from-a-meta-analysis","chapter":"8 Sample Size Justification","heading":"8.14 Using an Estimate from a Meta-Analysis","text":"perfect world effect size estimates meta-analysis provide researchers accurate information effect size expect. Due widespread publication bias science, effect size estimates meta-analyses regrettably always accurate. can biased, sometimes substantially . Furthermore, meta-analyses typically considerable heterogeneity, means meta-analytic effect size estimate differs subsets studies make meta-analysis. , although might seem useful use meta-analytic effect size estimate effect studying power analysis, need take great care .researcher wants enter meta-analytic effect size estimate -priori power analysis, need consider three things (see Table 8.5). First, studies included meta-analysis similar enough study performing reasonable expect similar effect size. essence, requires evaluating generalizability effect size estimate new study. important carefully consider differences meta-analyzed studies planned study, respect manipulation, measure, population, relevant variables.Second, researchers check whether effect sizes reported meta-analysis homogeneous. substantial heterogeinity meta-analytic effect sizes, means included studies can expected true effect size estimate. meta-analytic estimate used based subset studies closely represent planned study. Note heterogeneity remains possibility (even direct replication studies can show heterogeneity unmeasured variables moderate effect size sample (Kenny & Judd, 2019; Olsson-Collentine et al., 2020b)), main goal selecting similar studies use existing data increase probability expectation accurate, without guaranteeing .Third, meta-analytic effect size estimate biased. Check bias detection tests reported meta-analysis state---art, perform multiple bias detection tests (Carter et al., 2019), consider bias corrected effect size estimates (even though estimates might still biased, necessarily reflect true population effect size).\nTable 8.5: Overview recommendations justifying use meta-analytic effect size estimate power analysis.\n","code":""},{"path":"power.html","id":"using-an-estimate-from-a-previous-study","chapter":"8 Sample Size Justification","heading":"8.15 Using an Estimate from a Previous Study","text":"meta-analysis available, researchers often rely effect size previous study -priori power analysis. first issue requires careful attention whether two studies sufficiently similar. Just using effect size estimate meta-analysis, researchers consider differences studies terms population, design, manipulations, measures, factors lead one expect different effect size. example, intra-individual reaction time variability increases age, therefore study performed older participants expect smaller standardized effect size study performed younger participants. earlier study used strong manipulation, plan use subtle manipulation, smaller effect size expected. Finally, effect sizes generalize studies different designs. example, effect size comparison two groups often similar effect size interaction follow-study second factor added original design (Lakens & Caldwell, 2021).Even study sufficiently similar, statisticians warned using effect size estimates small pilot studies power analyses. Leon, Davis, Kraemer (2011) write:Contrary tradition, pilot study provide meaningful effect size estimate planning subsequent studies due imprecision inherent data small samples.two main reasons researchers careful using effect sizes studies published literature power analyses effect size estimates studies can differ true population effect size due random variation, publication bias inflates effect sizes. Figure 8.6 shows distribution \\(\\eta_p^2\\) study three conditions 25 participants condition null hypothesis true (dotted grey curve) 'medium' true effect \\(\\eta_p^2\\) = 0.0588 [solid black curve; Richardson (2011)]. Figure 8.4 critical effect size indicated, shows observed effects smaller \\(\\eta_p^2\\) = 0.08 significant given sample size. null hypothesis true, effects larger \\(\\eta_p^2\\) = 0.08 Type error (dark grey area), alternative hypothesis true effects smaller \\(\\eta_p^2\\) = 0.08 Type II error (light grey area). clear significant effects larger true effect size (\\(\\eta_p^2\\) = 0.0588), power analyses based significant finding (e.g., significant results published literature) based overestimate true effect size, introducing bias.even access effect sizes (e.g., pilot studies performed ) due random variation observed effect size sometimes quite small. Figure 8.6 shows quite likely observe effect \\(\\eta_p^2\\) = 0.01 small pilot study, even true effect size 0.0588. Entering effect size estimate \\(\\eta_p^2\\) = 0.01 -priori power analysis suggest total sample size 957 observations achieve 80% power follow-study. researchers follow pilot studies observe effect size pilot study , entered power analysis, yields sample size feasible collect follow-study, effect size estimates upwardly biased, power follow-study systematically lower desired (Albers & Lakens, 2018).\nFigure 8.6: Distribution partial eta squared null hypothesis (dotted grey curve) medium true effect 0.0588 (solid black curve) 3 groups 25 observations.\nessence, problem using small studies estimate effect size entered -priori power analysis due publication bias follow-bias effect sizes researchers end using power analysis come full F distribution, known truncated F distribution (Taylor & Muller, 1996). example, imagine extreme publication bias situation illustrated Figure 8.6. studies accessible researchers come part distribution \\(\\eta_p^2\\) > 0.08, test result statistically significant. possible compute effect size estimate , based certain assumptions, corrects bias. example, imagine observe result literature One-Way ANOVA 3 conditions, reported F(2, 42) = 0.017, \\(\\eta_p^2\\) = 0.176. take effect size face value enter effect size estimate -priori power analysis, suggested sample size achieve 80% power suggest need collect 17 observations condition.However, assume bias present, can use BUCSS R package (S. F. Anderson et al., 2017) perform power analysis attempts correct bias. example , power analysis takes bias account (specific model publication bias, based truncated F distribution significant results published) suggests collecting 73 participants condition instead. possible bias corrected estimate non-centrality parameter used compute power zero, case possible correct bias using method. alternative formally modeling correction publication bias whenever researchers assume effect size estimate biased, researchers can simply use conservative effect size estimate, example computing power based lower limit 60% two-sided confidence interval around effect size estimate, Perugini et al. (2014) refer safeguard power. approaches lead conservative power analysis, necessarily accurate power analysis. simply possible perform accurate power analysis basis effect size estimate study might biased /small sample size (Teare et al., 2014). possible specify smallest effect size interest, great uncertainty effect size expect, might efficient perform study sequential design (discussed ).summarize, effect size previous study -priori power analysis can used three conditions met (see Table 8.6). First, previous study sufficiently similar planned study. Second, low risk bias (e.g., effect size estimate comes Registered Report, analysis results impacted likelihood publication). Third, sample size large enough yield relatively accurate effect size estimate, based width 95% CI around observed effect size estimate. always uncertainty around effect size estimate, entering upper lower limit 95% CI around effect size estimate might informative consequences uncertainty effect size estimate -priori power analysis.\nTable 8.6: Overview recommendations justifying use effect size estimate single study.\n","code":""},{"path":"power.html","id":"using-an-estimate-from-a-theoretical-model","chapter":"8 Sample Size Justification","heading":"8.16 Using an Estimate from a Theoretical Model","text":"theoretical model sufficiently specific can build computational model, knowledge key parameters model relevant data plan collect, possible estimate effect size based effect size estimate derived computational model. example, one strong ideas weights feature stimuli share differ , possible compute predicted similarity judgments pairs stimuli based Tversky's contrast model (Tversky, 1977), estimate predicted effect size differences experimental conditions. Although computational models make point predictions relatively rare, whenever available, provide strong justification effect size researcher expects.","code":""},{"path":"power.html","id":"compute-the-width-of-the-confidence-interval-around-the-effect-size","chapter":"8 Sample Size Justification","heading":"8.17 Compute the Width of the Confidence Interval around the Effect Size","text":"researcher can estimate standard deviation observations collected, possible compute -priori estimate width 95% confidence interval around effect size (Kelley, 2007). Confidence intervals represent range around estimate wide enough long run true population parameter fall inside confidence intervals 100 - \\(\\alpha\\) percent time. single study true population effect either falls confidence interval, , long run one can act confidence interval includes true population effect size (keeping error rate mind). Cumming (2013) calls difference observed effect size upper bound 95% confidence interval (lower bound 95% confidence interval) margin error.compute 95% CI effect size d = 0 based t statistic sample size (Smithson, 2003), see 15 observations condition independent t test 95% CI ranges d = -0.716 d = 0.716. Confidence intervals around effect sizes can computed using MOTE Shiny app: https://www.aggieerin.com/shiny-server/. margin error half width 95% CI, 0.716. Bayesian estimator uses uninformative prior compute credible interval (similar) upper lower bound (Albers et al., 2018; Kruschke, 2011), might conclude collecting data left range plausible values population effect large informative. Regardless statistical philosophy plan rely analyzing data, evaluation can conclude based width interval tells us 15 observation per group learn lot.One useful way interpreting width confidence interval based effects able reject true effect size 0. words, effect, effects able reject given collected data, effect sizes rejected, effect? Effect sizes range d = 0.7 findings \"People become aggressive provoked\", \"People prefer group groups\", \"Romantic partners resemble one another physical attractiveness\" (Richard et al., 2003). width confidence interval tells can reject presence effects large, existed, probably already noticed . true effects study realistically much smaller d = 0.7, good possibility learn anything already know performing study n = 15. Even without data, research lines consider certain large effects plausible (although effect sizes plausible differ fields, discussed ). hand, large samples researchers can example reject presence effects larger d = 0.2, null hypothesis true, analysis width confidence interval suggest peers many research lines likely consider study informative.see margin error almost, exactly, minimal statistically detectable effect (d = 0.748). small variation due fact 95% confidence interval calculated based t distribution. true effect size zero, confidence interval calculated based non-central t distribution, 95% CI asymmetric. Figure 8.7 visualizes three t distributions, one symmetric 0, two asymmetric distributions noncentrality parameter (normalized difference means) 2 3. asymmetry clearly visible small samples (distributions plot 5 degrees freedom) remains noticeable larger samples calculating confidence intervals statistical power. example, true effect size d = 0.5 observed 15 observations per group yield \\(d_s\\) = 0.50, 95% CI [-0.23, 1.22]. compute 95% CI around critical effect size, get \\(d_s\\) = 0.75, 95% CI [0.00, 1.48]. see 95% CI ranges exactly 0 1.484, line relation confidence interval p value, 95% CI excludes zero test statistically significant. noted , different approaches recommended evaluate informative study often based information.\nFigure 8.7: Central (black dashed line) 2 non-central (dark grey light grey dashed lines) t distributions; ncp = noncentrality parameter.\n","code":""},{"path":"power.html","id":"plot-a-sensitivity-power-analysis","chapter":"8 Sample Size Justification","heading":"8.18 Plot a Sensitivity Power Analysis","text":"sensitivity power analysis fixes sample size, desired power, alpha level, answers question effect size study detect desired power. sensitivity power analysis therefore performed sample size already known. Sometimes data already collected answer different research question, data retrieved existing database, want perform sensitivity power analysis new statistical analysis. times, might carefully considered sample size initially collected data, want reflect statistical power study (ranges ) effect sizes interest analyzing results. Finally, possible sample size collected future, know due resource constraints maximum sample size can collect limited, want reflect whether study sufficient power effects consider plausible interesting (smallest effect size interest, effect size expected).Assume researcher plans perform study 30 observations collected total, 15 participant condition. Figure 8.8 shows perform sensitivity power analysis G*Power study decided use alpha level 5%, desire 90% power. sensitivity power analysis reveals designed study 90% power detect effects least d = 1.23. Perhaps researcher believes desired power 90% quite high, opinion still interesting perform study statistical power lower. can useful plot sensitivity curve across range smaller effect sizes.\nFigure 8.8: Sensitivity power analysis G*Power software.\ntwo dimensions interest sensitivity power analysis effect sizes, power observe significant effect assuming specific effect size. Fixing sample size, two dimensions can plotted create sensitivity curve. example, sensitivity curve can plotted G*Power clicking 'X-Y plot range values' button, illustrated Figure 8.9. Researchers can examine power -priori plausible range effect sizes, can examine effect sizes provide reasonable levels power. simulation-based approaches power analysis, sensitivity curves can created performing power analysis range possible effect sizes. Even 50% power deemed acceptable (case deciding act null hypothesis true non-significant result relatively noisy decision procedure), Figure 8.9 shows study design power extremely low large range effect sizes reasonable expect fields. Thus, sensitivity power analysis provides additional approach evaluate informative planned study , can inform researchers specific design unlikely yield significant effect range effects one might realistically expect.\nFigure 8.9: Plot effect size desired power n = 15 per group alpha = 0.05.\nnumber observations per group larger, evaluation might positive. might specific effect size mind, collected 150 observations per group, sensitivity analysis shown power sufficient range effects believe interesting examine, still approximately 50% power quite small effects. sensitivity analysis meaningful, sensitivity curve compared smallest effect size interest, range effect sizes expected. sensitivity power analysis clear cut-offs examine (Bacchetti, 2010). Instead, idea make holistic trade-different effect sizes one might observe care , associated statistical power.","code":""},{"path":"power.html","id":"the-distribution-of-effect-sizes-in-a-research-area","chapter":"8 Sample Size Justification","heading":"8.19 The Distribution of Effect Sizes in a Research Area","text":"personal experience commonly entered effect size estimate -priori power analysis independent t test Cohen's benchmark 'medium' effect size, known default effect. open G*Power, 'medium' effect default option -priori power analysis. Cohen's benchmarks small, medium, large effects used -priori power analysis (J. Cook et al., 2014; Correll et al., 2020), Cohen regretted proposed benchmarks (Funder & Ozer, 2019). large variety research topics means 'default' 'heuristic' used compute statistical power just unlikely correspond actual situation, also likely lead sample size substantially misaligned question trying answer collected data.researchers wondered better default , researchers basis decide upon effect size -priori power analysis. Brysbaert (2019) recommends d = 0.4 default psychology, average observed replication projects several meta-analyses. impossible know average effect size realistic, clear huge heterogeneity across fields research questions. average effect size often deviate substantially effect size expected planned study. researchers suggested change Cohen's benchmarks based distribution effect sizes specific field (Bosco et al., 2015; Funder & Ozer, 2019; Hill et al., 2008; Kraft, 2020; Lovakov & Agadullina, 2021). always, effect size estimates based published literature, one needs evaluate possibility effect size estimates inflated due publication bias. Due large variation effect sizes within specific research area, little use choosing large, medium, small effect size benchmark based empirical distribution effect sizes field perform power analysis.knowledge distribution effect sizes literature can useful interpreting confidence interval around effect size. specific research area almost effects larger value reject equivalence test (e.g., observed effect size 0, design reject effects larger example d = 0.7), -priori unlikely collecting data tell something already know.difficult defend use specific effect size derived empirical distribution effect sizes justification effect size used -priori power analysis. One might argue use effect size benchmark based distribution effects literature outperform wild guess, strong enough argument form basis sample size justification. point researchers need admit ready perform -priori power analysis due lack clear expectations (Scheel, Tiokhin, et al., 2021). Alternative sample size justifications, justification sample size based resource constraints, perhaps combination sequential study design, might line actual inferential goals study.","code":""},{"path":"power.html","id":"additional-considerations-when-designing-an-informative-study","chapter":"8 Sample Size Justification","heading":"8.20 Additional Considerations When Designing an Informative Study","text":"far, focus justifying sample size quantitative studies. number related topics can useful design informative study. First, addition -priori prospective power analysis sensitivity power analysis, important discuss compromise power analysis (useful) post-hoc retrospective power analysis (useful, e.g., Zumbo & Hubley (1998), Lenth (2007)). sample sizes justified based -priori power analysis can efficient collect data sequential designs data collection continued terminated based interim analyses data. Furthermore, worthwhile consider ways increase power test without increasing sample size. additional point attention good understanding dependent variable, especially standard deviation. Finally, sample size justification just important qualitative studies, although much less work sample size justification domain, proposals exist researchers can use design informative study. topics discussed turn.","code":""},{"path":"power.html","id":"compromise-power-analysis","chapter":"8 Sample Size Justification","heading":"8.21 Compromise Power Analysis","text":"compromise power analysis sample size effect fixed, error rates test calculated, based desired ratio Type Type II error rate. compromise power analysis useful large number observations collected, small number observations can collected.first situation researcher might fortunate enough able collect many observations statistical power test high effect sizes deemed interesting. example, imagine researcher access 2000 employees required answer questions yearly evaluation company testing intervention reduce subjectively reported stress levels. quite confident effect smaller d = 0.2 large enough subjectively noticeable individuals (Jaeschke et al., 1989). alpha level 0.05 researcher statistical power 0.994, Type II error rate 0.006. means smallest effect size interest d = 0.2 researcher 8.3 times likely make Type error Type II error.Although original idea designing studies control Type Type II error rates researchers need justify error rates (Neyman & Pearson, 1933), common heuristic set Type error rate 0.05 Type II error rate 0.20, meaning Type error 4 times unlikely Type II error. default use 80% power (Type II error rate/\\(\\beta\\) 0.20) based personal preference Cohen (1988), writes:proposed convention , investigator basis setting desired power value, value .80 used. means \\(\\beta\\) set .20. arbitrary reasonable value offered several reasons (Cohen, 1965, pp. 98-99). chief among takes consideration implicit convention \\(\\alpha\\) .05. \\(\\beta\\) .20 chosen idea general relative seriousness two kinds errors order .20/.05, .e., Type errors order four times serious Type II errors. .80 desired power convention offered hope ignored whenever investigator can find basis substantive concerns specific research investigation choose value ad hoc.see conventions built conventions: norm aim 80% power built norm set alpha level 5%. take away Cohen aim 80% power, justify error rates based relative seriousness error. compromise power analysis comes . share Cohen's belief Type error 4 times serious Type II error, building earlier study 2000 employees, makes sense adjust Type error rate Type II error rate low effect sizes interest (Cascio & Zedeck, 1983). Indeed, Erdfelder et al. (1996) created G*Power software part give researchers tool perform compromise power analysis.\nFigure 8.10: Compromise power analysis G*Power.\nFigure 8.10 illustrates compromise power analysis performed G*Power Type error deemed equally costly Type II error (, \\(\\beta/\\alpha\\) ratio = 1), study 1000 observations per condition lead Type error Type II error 0.0179. Faul, Erdfelder, Lang, Buchner (2007) write:course, compromise power analyses can easily result unconventional significance levels greater \\(\\alpha\\) = .05 (case small samples effect sizes) less \\(\\alpha\\) = .001 (case large samples effect sizes). However, believe benefit balanced Type Type II error risks often offsets costs violating significance level conventions.brings us second situation compromise power analysis can useful, know statistical power study low. Although highly undesirable make decisions error rates high, one finds oneself situation decision must made based little information, Winer (1962) writes:frequent use .05 .01 levels significance matter convention little scientific logical basis. power tests likely low levels significance, Type Type II errors approximately equal importance, .30 .20 levels significance may appropriate .05 .01 levels.example, plan perform two-sided t test, can feasibly collect 50 observations independent group, expect population effect size 0.5, 70% power set alpha level 0.05. Alternatively, using compromise power analysis, can choose weigh types error equally (\\(\\beta/\\alpha\\) ratio = 1) setting alpha level Type II error rate 0.149. , 85.10% power detect expected population effect size d = 0.5 instead. choice \\(\\alpha\\) \\(\\beta\\) compromise power analysis can extended take prior probabilities null alternative hypothesis account (Maier & Lakens, 2022; Miller & Ulrich, 2019; Murphy et al., 2014).compromise power analysis requires researcher specify sample size. sample size requires justification, compromise power analysis typically performed together resource constraint justification sample size. especially important perform compromise power analysis resource constraint justification strongly based need make decision, case researcher think carefully Type Type II error rates stakeholders willing accept. However, compromise power analysis also makes sense sample size large, researcher freedom set sample size. might happen , example, data collection part larger international study sample size based research questions. designs Type II error rate small (power high) statisticians also recommended lower alpha level prevent Lindley's paradox, situation significant effect (p < \\(\\alpha\\)) evidence null hypothesis (Good, 1992; Jeffreys, 1939). Lowering alpha level function statistical power test can prevent paradox, providing another argument compromise power analysis sample sizes large (Maier & Lakens, 2022). Finally, compromise power analysis needs justification effect size, either based smallest effect size interest effect size expected. Table 8.7 lists three aspects discussed alongside reported compromise power analysis.\nTable 8.7: Overview recommendations justifying error rates based compromise power analysis.\n","code":""},{"path":"power.html","id":"posthocpower","chapter":"8 Sample Size Justification","heading":"8.22 What to do if Your Editor Asks for Post-hoc Power?","text":"Post-hoc, retrospective, observed power used describe statistical power test computed assuming effect size estimated collected data true effect size (Lenth, 2007; Zumbo & Hubley, 1998). Post-hoc power therefore performed looking data, based effect sizes deemed interesting, -priori power analysis, unlike sensitivity power analysis range interesting effect sizes evaluated. post-hoc retrospective power analysis based effect size observed data collected, add information beyond reported p value, presents information different way. Despite fact, editors reviewers often ask authors perform post-hoc power analysis interpret non-significant results. sensible request, whenever made, comply . Instead, perform sensitivity power analysis, discuss power smallest effect size interest realistic range expected effect sizes.Post-hoc power directly related p value statistical test (Hoenig & Heisey, 2001). z test p value exactly 0.05, post-hoc power always 50%. reason relationship p value observed equals alpha level test (e.g., 0.05), observed z score test exactly equal critical value test (e.g., z = 1.96 two-sided test 5% alpha level). Whenever alternative hypothesis centered critical value half values expect observe alternative hypothesis true fall critical value, half fall critical value. Therefore, test observed p value identical alpha level exactly 50% power post-hoc power analysis, analysis assumes observed effect size true.statistical tests, alternative distribution symmetric (t test, alternative hypothesis follows non-central t distribution, see Figure 8.7), p = 0.05 directly translate observed power 50%, plotting post-hoc power observed p value see two statistics always directly related. Figure 8.11 shows, p value non-significant (.e., larger 0.05) observed power less approximately 50% t test. Lenth (2007) explains observed power also completely determined observed p value F tests, although statement non-significant p value implies power less 50% longer holds.\nFigure 8.11: Relationship p values power independent t test α = 0.05 n = 10.\neditors reviewers ask researchers report post-hoc power analyses like able distinguish true negatives (concluding effect, effect) false negatives (Type II error, concluding effect, actually effect). Since reporting post-hoc power just different way reporting p value, reporting post-hoc power provide answer question editors asking (Hoenig & Heisey, 2001; Lenth, 2007; Schulz & Grimes, 2005; Yuan & Maxwell, 2005). able draw conclusions absence meaningful effect, one perform equivalence test, design study high power reject smallest effect size interest. Alternatively, smallest effect size interest specified designing study, researchers can report sensitivity power analysis.","code":""},{"path":"power.html","id":"sequentialsamplesize","chapter":"8 Sample Size Justification","heading":"8.23 Sequential Analyses","text":"Whenever sample size justified based -priori power analysis can efficient collect data sequential design. Sequential designs control error rates across multiple looks data (e.g., 50, 100, 150 observations collected) can reduce average expected sample size collected compared fixed design data analyzed maximum sample size collected (Proschan et al., 2006; Wassmer & Brannath, 2016). Sequential designs long history (Dodge & Romig, 1929), exist many variations, Sequential Probability Ratio Test (Wald, 1945), combining independent statistical tests (Westberg, 1985), group sequential designs (Jennison & Turnbull, 2000), sequential Bayes factors (Schönbrodt et al., 2017), safe testing (Grünwald et al., 2019). approaches, Sequential Probability Ratio Test efficient data can analyzed every observation (Schnuerch & Erdfelder, 2020). Group sequential designs, data collected batches, provide flexibility data collection, error control, corrections effect size estimates (Wassmer & Brannath, 2016). Safe tests provide optimal flexibility dependencies observations (ter Schure & Grünwald, 2019).Sequential designs especially useful considerable uncertainty effect size, plausible true effect size larger smallest effect size interest study designed detect (Lakens, 2014). situations data collection possibility terminate early effect size larger smallest effect size interest, data collection can continue maximum sample size needed. Sequential designs can prevent waste testing hypotheses, stopping early null hypothesis can rejected, stopping early presence smallest effect size interest can rejected (.e., stopping futility). Group sequential designs currently widely used approach sequential analyses, can planned analyzed using rpact gsDesign. Shiny apps available rpact: https://rpact.shinyapps.io/public/ gsDesign: https://gsdesign.shinyapps.io/prod/.","code":""},{"path":"power.html","id":"increasing-power-without-increasing-the-sample-size","chapter":"8 Sample Size Justification","heading":"8.24 Increasing Power Without Increasing the Sample Size","text":"straightforward approach increase informational value studies increase sample size. resources often limited, also worthwhile explore different approaches increasing power test without increasing sample size. first option use directional tests relevant. Researchers often make directional predictions, ‘predict X larger Y’. statistical test logically follows prediction directional (one-sided) t test. directional test moves Type error rate one side tail distribution, lowers critical value, therefore requires less observations achieve statistical power.Although discussion directional tests appropriate, perfectly defensible Neyman-Pearson perspective hypothesis testing (Cho & Abe, 2013), makes (preregistered) directional test straightforward approach increase power test, riskiness prediction. However, might situations want ask directional question. Sometimes, especially research applied consequences, might important examine null effect can rejected, even effect opposite direction predicted. example, evaluating recently introduced educational intervention, predict intervention increase performance students, might want explore possibility students perform worse, able recommend abandoning new intervention. cases also possible distribute error rate 'lop-sided' manner, example assigning stricter error rate effects negative positive direction (Rice & Gaines, 1994).Another approach increase power without increasing sample size, increase alpha level test, explained section compromise power analysis. Obviously, comes increased probability making Type error. risk making either type error carefully weighed, typically requires taking account prior probability null hypothesis true (Cascio & Zedeck, 1983; Miller & Ulrich, 2019; Mudge et al., 2012; Murphy et al., 2014). make decision, want make claim, data can feasibly collect limited, increasing alpha level justified, either based compromise power analysis, based cost-benefit analysis (Baguley, 2004; Field et al., 2004).Another widely recommended approach increase power study use within participant design possible. almost cases researcher interested detecting difference groups, within participant design require collecting less participants participant design. reason decrease sample size explained equation Maxwell et al. (2017). number participants needed two group within-participants design (NW) relative number participants needed two group -participants design (NB), assuming normal distributions, :\\[NW = \\frac{NB (1-\\rho)}{2}\\]required number participants divided two within-participants design two conditions every participant provides two data points. extent reduces sample size compared -participants design also depends correlation dependent variables (e.g., correlation measure collected control task experimental task), indicated (1-\\(\\rho\\)) part equation. correlation 0, within-participants design simply needs half many participants participant design (e.g., 64 instead 128 participants). higher correlation, larger relative benefit within-participants designs, whenever correlation negative (-1) relative benefit disappears.Figure 8.12 see two normally distributed scores mean difference 6, standard deviation mean 15, correlation measurements 0. standard deviation difference score \\(\\sqrt{2}\\) times large standard deviation measurement, indeed, 15×\\(\\sqrt{2}\\) = 21.21, rounded 21. situation correlation measurements zero equals situation independent t-test, correlation measurements taken account.\nFigure 8.12: Distributions two dependent groups means 100 106 standard deviation 15, distribution differences, correlation 0.\nFigure 8.13 can see happens two variables correlated, example r = 0.7. Nothing changed plot means. correlation measurements now strongly positive, important difference standard deviation difference scores, 11 instead 21 uncorrelated example. standardized effect size difference divided standard deviation, effect size (Cohen’s \\(d_z\\) within designs) larger test uncorrelated test.\nFigure 8.13: Distributions two independent groups means 100 106 standard deviation 15, distribution differences, correlation 0.7.\ncorrelation dependent variables important aspect within designs. recommend explicitly reporting correlation dependent variables within designs (e.g., participants responded significantly slower (M = 390, SD = 44) used feet used hands (M = 371, SD = 44, r = .953), t(17) = 5.98, p < 0.001, Hedges' g = 0.43, \\(M_{diff}\\) = 19, 95% CI [12; 26]). Since dependent variables within designs psychology positively correlated, within designs increase power can achieve given sample size available. Use within-designs possible, weigh benefits higher power downsides order effects carryover effects might problematic within-subject design (Maxwell et al., 2017).can use Shiny app play around different means, standard deviations, correlations, see effect distribution difference scores.general, smaller variation, larger standardized effect size (dividing raw effect smaller standard deviation) thus higher power given number observations. additional recommendations provided literature (Allison et al., 1997; Bausell & Li, 2002; Hallahan & Rosenthal, 1996), :Use better ways screen participants studies participants need screened participation.Assign participants unequally conditions (data control condition much cheaper collect data experimental condition, example).Use reliable measures low error variance (Williams et al., 1995).Smart use preregistered covariates (Meyvis & Van Osselaer, 2018).important consider ways reduce variation data come large cost external validity. example, intention--treat analysis randomized controlled trials participants comply protocol maintained analysis effect size study accurately represents effect implementing intervention population, effect intervention people perfectly follow protocol (Gupta, 2011). Similar trade-offs reducing variance external validity exist research areas.","code":""},{"path":"power.html","id":"knowyourmeasure","chapter":"8 Sample Size Justification","heading":"8.25 Know Your Measure","text":"Although convenient talk standardized effect sizes, generally preferable researchers can interpret effects raw (unstandardized) scores, knowledge standard deviation measures (Baguley, 2009; Lenth, 2001). make possible research community realistic expectations standard deviation measures collect, beneficial researchers within research area use validated measures. provides reliable knowledge base makes easier plan desired accuracy, use smallest effect size interest unstandardized scale -priori power analysis.addition knowledge standard deviation important knowledge correlations dependent variables (example Cohen's dz dependent t test relies correlation means). complex model, aspects data-generating process need known make predictions. example, hierarchical models researchers need knowledge variance components able perform power analysis (DeBruine & Barr, 2021; Westfall et al., 2014). Finally, important know reliability measure (Parsons et al., 2019), especially relying effect size published study used measure different reliability, measure used different populations, case possible measurement reliability differs populations. increasing availability open data, hopefully become easier estimate parameters using data earlier studies.calculate standard deviation sample, value estimate true value population. small samples, estimate can quite far , due law large numbers, sample size increases, measuring standard deviation accurately. Since sample standard deviation estimate uncertainty, can calculate confidence interval around estimate (Smithson, 2003), design pilot studies yield sufficiently reliable estimate standard deviation. confidence interval variance \\(\\sigma^2\\) provided following formula, confidence standard deviation square root limits:\\[(N - 1)s^2/\\chi^2_{N-1:\\alpha/2},(N - 1)s^2/\\chi^2_{N-1:1-\\alpha/2}\\]Whenever uncertainty parameters, researchers can use sequential designs perform internal pilot study (Wittes & Brittain, 1990). idea behind internal pilot study researchers specify tentative sample size study, perform interim analysis, use data internal pilot study update parameters variance measure, finally update final sample size collected. long interim looks data blinded (e.g., information conditions taken account) sample size can adjusted based updated estimate variance without practical consequences Type error rate (Friede & Kieser, 2006; Proschan, 2005). Therefore, researchers interested designing informative study Type Type II error rates controlled, lack information standard deviation, internal pilot study might attractive approach consider (Chang, 2016).","code":""},{"path":"power.html","id":"conventions-as-meta-heuristics","chapter":"8 Sample Size Justification","heading":"8.26 Conventions as meta-heuristics","text":"Even researcher might use heuristic directly determine sample size study, indirect way heuristics play role sample size justifications. Sample size justifications based inferential goals power analysis, accuracy, decision require researchers choose values desired Type Type II error rate, desired accuracy, smallest effect size interest. Although sometimes possible justify values described (e.g., based cost-benefit analysis), solid justification values might require dedicated research lines. Performing research lines always possible, studies might worth costs (e.g., might require less resources perform study alpha level peers consider conservatively low, collect data required determine alpha level based cost-benefit analysis). situations, researchers might use values based convention.comes desired width confidence interval, desired power, input values required perform sample size computation, important transparently report use heuristic convention (example using accompanying online Shiny app). convention use 5% Type 1 error rate 80% power practically functions lower threshold minimum informational value peers expected accept without justification (whereas justification, higher error rates can also deemed acceptable peers). important realize none values set stone. Journals free specify desire higher informational value author guidelines (e.g., Nature Human Behavior requires Registered Reports designed achieve 95% statistical power, department required staff submit ERB proposals , whenever possible, study designed achieve 90% power). Researchers choose design studies higher informational value conventional minimum receive credit .past fields changed conventions, 5 sigma threshold now used physics declare discovery instead 5% Type error rate. fields attempts unsuccessful (e.g., Johnson (2013)). Improved conventions context dependent, seems sensible establish consensus meetings (Mullan & Jacoby, 1985). Consensus meetings common medical research, used decide upon smallest effect size interest (example, see Fried et al. (1993)). many research areas current conventions can improved. example, seems peculiar default alpha level 5% single studies meta-analyses, one imagine future default alpha level meta-analyses much lower 5%. Hopefully, making lack adequate justification certain input values specific situations transparent motivate fields start discussion improve current conventions. online Shiny app links good examples justifications possible, continue updated better justifications developed future.","code":""},{"path":"power.html","id":"sample-size-justification-in-qualitative-research","chapter":"8 Sample Size Justification","heading":"8.27 Sample Size Justification in Qualitative Research","text":"value information perspective sample size justification also applies qualitative research. sample size justification qualitative research based consideration cost collecting data additional participants yield new information valuable enough given inferential goals. One widely used application idea known saturation indicated observation new data replicates earlier observations, without adding new information (Morse, 1995). example, imagine ask people pet. Interviews might reveal reasons grouped categories, interviewing 20 people, new categories emerge, point saturation reached. Alternative philosophies qualitative research exist, value planning saturation. Regrettably, principled approaches justify sample sizes developed alternative philosophies (Marshall et al., 2013).sampling, goal often pick representative sample, sample contains sufficiently diverse number subjects saturation reached efficiently. Fugard Potts (2015) show move towards informed justification sample size qualitative research based 1) number codes exist population (e.g., number reasons people pets), 2) probability code can observed single information source (e.g., probability someone interview mention possible reason pet), 3) number times want observe code. provide R formula based binomial probabilities compute required sample size reach desired probability observing codes.advanced approach used Rijnsoever (2017), also explores importance different sampling strategies. general, purposefully sampling information sources expect yield novel information much efficient random sampling, also requires good overview expected codes, sub-populations code can observed. Sometimes, possible identify information sources , interviewed, least yield one new code (e.g., based informal communication interview). good sample size justification qualitative research based 1) identification populations, including sub-populations, 2) estimate number codes (sub-)population, 3) probability code encountered information source, 4) sampling strategy used.","code":""},{"path":"power.html","id":"discussion","chapter":"8 Sample Size Justification","heading":"8.28 Discussion","text":"Providing coherent sample size justification essential step designing informative study. multiple approaches justifying sample size study, depending goal data collection, resources available, statistical approach used analyze data. overarching principle approaches researchers consider value information collect relation inferential goals.process justifying sample size designing study sometimes lead conclusion worthwhile collect data, study sufficient informational value justify costs. cases unlikely ever enough data perform meta-analysis (example lack general interest topic), information used make decision claim, statistical tests allow test hypothesis reasonable error rates estimate effect size sufficient accuracy. good justification collect maximum number observations one can feasibly collect, performing study anyway waste time /money (G. W. Brown, 1983; Button et al., 2013; S. D. Halpern et al., 2002).awareness sample sizes past studies often small meet realistic inferential goals growing among psychologists (Button et al., 2013; Fraley & Vazire, 2014; Lindsay, 2015; Sedlmeier & Gigerenzer, 1989). increasing number journals start require sample size justifications, researchers realize need collect larger samples used . means researchers need request money participant payment grant proposals, researchers need increasingly collaborate (Moshontz et al., 2018). believe research question important enough answered, able answer question current resources, one approach consider organize research collaboration peers, pursue answer question collectively.","code":""},{"path":"power.html","id":"test-yourself-6","chapter":"8 Sample Size Justification","heading":"8.29 Test Yourself","text":"Q1: student 2 months collect data. need pay participants participation, budget limited 250 euro. decide collect participants can amount time, money available. type sample size justification ?Collecting entire populationA resource justificationA heuristicNo justificationQ2: goal -priori power analysis?Achieve desired statistical power true effect size, controlling Type 1 error rate.Achieve desired statistical power effect size interest, controlling Type 1 error rate.Achieve desired statistical power true effect size, controlling Type 2 error rate.Achieve desired statistical power effect size interest, controlling Type 2 error rate.Q3: researcher already knows sample size able collect. Given sample size, choose compute equal Type 1 Type 2 error rates effect size interest. known :-priori power analysisA sensitivity power analysisA post-hoc power analysisA compromise power analysisQ4: Looking formula section 'Increasing Power Without Increasing Sample Size'. two factors contribute fact within subject designs can much power, number participants, subject designs?fact participant contributes multiple observations, fact effect sizes within individuals typically larger effect sizes individuals.fact participant contributes multiple observations, effect correlation measurements.fact order effects increase effect size, effect correlation measurements.fact order effects increase effect size, fact effect sizes within individuals typically larger effect sizes individuals.Q5: factors determine minimal statistically detectable effect?power studyThe true effect size studyThe sample size alpha levelThe observed effect size sampleQ6: else equal, want perform study highest possible informational value, approach specifying effect size interest best choice?Specify smallest effect size interestCompute minimal statistically detectable effectUse effect size estimate meta-analysisPerform sensitivity power analysisQ7: -priori power analysis based empirical estimate literature, 2 issues important consider, using estimate meta-analysis, single study?Evaluate risk bias estimate, evaluate uncertainty effect size estimate.Evaluate heterogeneity underlying effect size estimate, evaluate similarity study/studies estimate based study plan perform.Evaluate risk bias estimate, evaluate similarity study/studies estimate based study plan perform.Evaluate heterogeneity underlying effect size estimate, evaluate uncertainty effect size estimate.Q8: Imagine researcher justify sample size performing study, justification sample size choose. submitting scientific article journal reviewers ask justification sample size. course, honesty requires authors write justification, can still evaluate informational value study effect sizes interest?Perform -priori power analysisPerform compromise power analysisPerform sensitivity power analysisPerform post-hoc retrospective power analysisQ9: can useful consider effect size distribution findings specific research area evaluating informational value study planning?study can reject effects large unlikely observed specific research area, collecting data teach us anything already know.can use information design study high power smallest effect size observed specific literature, lead study high informational value.can use information design study high power detect median effect size literature, lead study high informational value.can use information design study high power reject median effect size literature, lead study high informational value.Q10: nonsensical ask researchers perform post-hoc retrospective power analysis, observed effect size collected sample size used calculate statistical power test, non-significant finding observed?Post-hoc power analyses always based assumptions, therefore, assumptions wrong, post-hoc power analysis informative.Due relationship post-hoc power p-value, whenever effect non-significant, post-hoc power low, post-hoc power analysis provide useful additional information.post-hoc power analysis mathematically identical sensitivity power analysis specific effect size estimate, better plot power range effect sizes, specific value.question whether non-significant effect true negative, false negative, risk errors controlled advance -priori power analysis, data collected post-hoc power analysis.Q11: Researchers perform post-hoc power analysis. two solutions, one can implemented designing study, one interpreting non-significant result data . solution can implemented data ?Evaluate accuracy effect size estimate, perform sensitivity power analysis.Plan study high power equivalence test, perform sensitivity power analysis.Evaluate accuracy effect size estimate, perform compromise power analysis. D) Plan study high power equivalence test, perform compromise power analysis.Q12: way/ways increase statistical power test, without increasing sample size?Perform one-sided test instead two-sided test.Lower alpha-level test.Use measures higher (compared lower) error variance.answer options correct.","code":""},{"path":"power.html","id":"open-questions-6","chapter":"8 Sample Size Justification","heading":"8.29.1 Open Questions","text":"resource constraints, primary justification, always secondary justification (possible collect data entire population)?resource constraints, primary justification, always secondary justification (possible collect data entire population)?goal -priori power analysis, goal achieve desired Type 2 error rate true effect size?goal -priori power analysis, goal achieve desired Type 2 error rate true effect size?benefit planning precision, given effect size typically unknown (might even 0). aspect decisions need made planning precision difficult justify?benefit planning precision, given effect size typically unknown (might even 0). aspect decisions need made planning precision difficult justify?problem using heuristics basis sample size justification?problem using heuristics basis sample size justification?seems counter-intuitive ‘justification’ category chapter sample size justification, important explicitly state justification?seems counter-intuitive ‘justification’ category chapter sample size justification, important explicitly state justification?effect sizes might related inferential goal study, 6 categories Table 8.2 best approach (can specified)?effect sizes might related inferential goal study, 6 categories Table 8.2 best approach (can specified)?can’t simply take effect size estimate meta-analysis basis -priori power analysis related study?can’t simply take effect size estimate meta-analysis basis -priori power analysis related study?can’t simply take effect size estimate single study basis -priori power analysis related study?can’t simply take effect size estimate single study basis -priori power analysis related study?goal compromise power analysis?goal compromise power analysis?‘post-hoc’ ‘retrospective’ power useful way draw inferences non-significant results?‘post-hoc’ ‘retrospective’ power useful way draw inferences non-significant results?can beneficial use within-design compared -design (possible)?can beneficial use within-design compared -design (possible)?","code":""},{"path":"equivalencetest.html","id":"equivalencetest","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9 Equivalence Testing and Interval Hypotheses","text":"scientific studies designed test prediction effect difference exists. new intervention work? relationship two variables? studies commonly analyzed null hypothesis significance test. statistically significant p-value observed, null hypothesis can rejected, researchers can claim intervention works, relationship two variables, maximum error rate. p-value statistically significant, researchers often draw logically incorrect conclusion: conclude effect based p > 0.05.Open result section article writing, result section article recently read. Search \"p > 0.05\", look carefully scientists concluded (results section, also check claim make discussion section). see conclusion 'effect' 'association variables', found example researchers forgot absence evidence evidence absence (Altman & Bland, 1995). non-significant result tells us reject null hypothesis. tempting ask p > 0.05 ', true effect zero'? p-value null hypothesis significance test answer question. might useful think answer question whether effect absent observing p > 0.05 無 (mu), used non-dualistic answer, neither yes , 'unasking question'. simply possible answer question whether meaningful effect absent based p > 0.05.many situations researchers interested examining whether meaningful effect absent. example, can important show two groups differ factors might confound experimental design (e.g., examining whether manipulation intended increase fatigue affect mood participants, showing positive negative affect differ groups). Researchers might want know two interventions work equally well, especially newer intervention costs less requires less effort (e.g., online therapy just efficient person therapy?). times might interested demonstrate absence effect theoretical model predicts effect, believe previously published study false positive, expect show absence effect replication study (Dienes, 2014). yet, ask researchers ever designed study goal show effect, example predicting difference two conditions, many people say never designed study main prediction effect size 0. Researchers almost always predict difference. One reason might many researchers even know statistically support prediction effect size 0, trained use equivalence testing.never possible show effect exactly 0. Even collected data every person world, effect single study randomly vary around true effect size 0 - might end mean difference close , exactly, zero, finite sample. Hodges & Lehmann (1954) first discuss statistical problem testing whether two populations mean. suggest (p. 264) : “test means differ amount specified represent smallest difference practical interest”. Nunnally (1960) similarly proposed ‘fixed-increment’ hypothesis researchers compare observed effect range values deemed small meaningful. Defining range values considered practically equivalent absence effect known equivalence range (Bauer & Kieser, 1996) region practical equivalence (Kruschke, 2013). equivalence range specified advance, requires careful consideration smallest effect size interest.Although researchers repeatedly attempted introduce tests equivalence range social sciences (Cribbie et al., 2004; Hoenig & Heisey, 2001; Levine et al., 2008; Quertemont, 2011; J. L. Rogers et al., 1993), statistical approach recently become popular. replication crisis, researchers searched tools interpret null results performing replication studies. Researchers wanted able publish informative null results replicating findings literature suspected false positives. One notable example studies pre-cognition Daryl Bem, ostensibly showed participants able predict future (Bem, 2011). Equivalence tests proposed statistical approach answer question whether observed effect small enough conclude previous study replicated (S. F. Anderson & Maxwell, 2016; Lakens, 2017; Simonsohn, 2015). Researchers specify smallest effect size interest (example effect 0.5, two-sided test value outside range -0.5 0.5) test whether effects extreme range can rejected. , can reject presence effects deemed large enough meaningful.One can distinguish nil null hypothesis, null hypothesis effect 0, non-nil null hypothesis, null hypothesis effect 0, example effects extreme smallest effect size interest (Nickerson, 2000). Nickerson writes:distinction important one, especially relative controversy regarding merits shortcomings NHST inasmuch criticisms may valid applied nil hypothesis testing necessarily valid directed null hypothesis testing general sense.Equivalence tests specific implementation interval hypothesis tests, instead testing null hypothesis effect (, effect size 0; nil null hypothesis), effect tested null hypothesis represents range non-zero effect sizes (non-nil null hypothesis). Indeed, one widely suggested improvements mitigates important limitations null hypothesis significance testing replace nil null hypothesis test range prediction (specifying non-nil null hypothesis) interval hypothesis test (Lakens, 2021). illustrate difference, Panel Figure 9.1 visualizes results predicted two-sided null hypothesis test nil hypothesis, test examines whether effect 0 can rejected. Panel B shows interval hypothesis effect 0.5 2.5 predicted, non-nill null hypothesis consists values smaller 0.5 larger 2.5, interval hypothesis test examines whether values ranges can rejected. Panel C illustrates equivalence test, basically identical interval hypothesis test, predicted effects located range around 0, contain effects deemed small meaningful.\nFigure 9.1: Two-sided null hypothesis test (), interval hypothesis test (B), equivalence test (C) minimum effect test (D).\nequivalence test reversed, researcher designs study reject effects less extreme smallest effect size interest (see Panel D Figure 9.1), called minimum effect test (Murphy & Myors, 1999). researcher might just interested rejecting effect 0 (null hypothesis significance test) rejecting range effects small meaningful. else equal, study designed high power minimum effect requires observations goal reject effect zero. confidence interval needs reject value closer observed effect size (e.g., 0.1 instead 0) needs narrow, requires observations.One benefit minimum effect test compared null hypothesis test distinction statistical significance practical significance. test value chosen represent minimum effect interest, whenever rejected, effect statistically practically significant (Murphy et al., 2014). Another benefit minimum effect tests , especially correlational studies social sciences, variables often connected causal structures result real theoretically uninteresting nonzero correlations variables, labeled 'crud factor' (Meehl, 1990; Orben & Lakens, 2020). effect zero unlikely true large correlational datasets, rejecting nil null hypothesis severe test. Even hypothesis incorrect, likely effect 0 rejected due 'crud'. reason, researchers suggested test minimum effect r = 0.1, correlations threshold quite common due theoretically irrelevant correlations variables (Ferguson & Heene, 2021).Figure 9.1 illustrates two-sided tests, often intuitive logical perform one-sided tests. case, minimum effect test , example, aim reject effects smaller 0.1, equivalence test aim reject effects larger example 0.1. Instead specifying upper lower bound range, sufficient specify single value one-sided tests. final variation one-sided non-nil null hypothesis test known test non-inferiority, examines effect larger lower bound equivalence range. test example performed novel intervention noticeably worse existing intervention, can tiny bit worse. example, difference novel existing intervention smaller -0.1, effects smaller -0.1 can rejected, one can conclude effect non-inferior (Mazzolari et al., 2022; Schumi & Wittes, 2011). see extending nil null hypothesis tests non-nil null hypotheses allow researchers ask questions might interesting.","code":""},{"path":"equivalencetest.html","id":"equivalence-tests","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.1 Equivalence tests","text":"Equivalence tests first developed pharmaceutical sciences (Hauck & Anderson, 1984; Westlake, 1972) later formalized two one-sided tests (TOST) approach equivalence testing (Schuirmann, 1987; Seaman & Serlin, 1998; Wellek, 2010). TOST procedure entails performing two one-sided tests examine whether observed data surprisingly larger lower equivalence boundary (\\(\\Delta_{L}\\)), surprisingly smaller upper equivalence boundary (\\(\\Delta_{U}\\)):\\[\nt_{L} = \\frac{{\\overline{M}}_{1} - {\\overline{M}}_{2} - \\Delta_{L}}{\\sigma\\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}}\n\\]\\[\nt_{U} = \\frac{{\\overline{M}}_{1} - {\\overline{M}}_{2}{- \\Delta}_{U}}{\\sigma\\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}}\n\\]M indicates means sample, n sample size, σ \npooled standard deviation:\\[\n\\sigma = \\sqrt{\\frac{\\left( n_{1} - 1 \\right)\\text{sd}_{1}^{2} + \\left( n_{2} - 1 \\right)\\text{sd}_{2}^{2}}{n_{1} + \\ n_{2} - 2}}\n\\]one-sided tests significant, can reject presence effects large enough meaningful. formulas highly similar normal formula t-statistic. difference NHST t-test TOST procedure lower equivalence boundary \\(\\Delta_{L}\\) upper equivalence boundary \\(\\Delta_{U}\\) subtracted mean difference groups (normal t-test, compare mean difference 0, thus delta drops formula 0).perform equivalence test, need learn new statistical tests, just well-known t-test different value 0. somewhat surprising use ttests perform equivalence tests taught alongside use null hypothesis significance tests, indication prevent common misunderstandings p-values (Parkhurst, 2001). look example equivalence test using TOST procedure.study researchers manipulating fatigue asking participants carry heavy boxes around, researchers want ensure manipulation inadvertently alter participants’ moods. researchers assess positive negative emotions conditions, want claim differences positive mood. Let’s assume positive mood experimental fatigue condition (\\(m_1\\) = 4.55, \\(sd_1\\) = 1.05, \\(n_1\\) = 15) differ mood control condition (\\(m_2\\) = 4.87, \\(sd_2\\) = 1.11, \\(n_2\\) = 15). researchers conclude: “Mood differ conditions, t = -0.81, p = .42”. course, mood differ conditions, 4.55 - 4.87 = -0.32. claim meaningful difference mood, make claim correct manner, first need specify difference mood large enough meaningful. now, assume researcher consider effect less extreme half scale point small meaningful. now test observed mean difference -0.32 small enough can reject presence effects large enough matter.TOSTER package (originally created recently redesigned Aaron Caldwell) can used plot two t-distributions critical regions indicating can reject presence effects smaller -0.5 larger 0.5. can take time get used idea rejecting values extreme equivalence bounds. Try consistently ask hypothesis test: values can test reject? nil null hypothesis test, can reject effect 0, equivalence test Figure , can reject values lower -0.5 higher 0.5. Figure 9.2 see two t-distributions centered upper lower bound specified equivalence range (-0.5 0.5).\nFigure 9.2: mean difference confidence interval plotted t-distributions used perform two-one-sided tests -0.5 0.5.\ntwo curves see line represents confidence interval ranging -0.99 0.35, dot line indicates observed mean difference -0.32. first look left curve. see green highlighted area tails highlights observed mean differences extreme enough statistically reject effect -0.5. observed mean difference -0.32 lies close -0.5, look left distribution, mean far enough away -0.5 fall green area indicates observed differences statistically significant. can also perform equivalence test using TOSTER package, look results.line 't-test' output shows traditional nil null hypothesis significance test (already knew statistically significant: t = 0.46, p = 0.42. Just like default t-test R, tsum_TOST function default calculate Welch’s t-test (instead Student’s t-test), better default (Delacre et al., 2017), can request Student’s t-test adding var.equal = TRUE argument function.also see test indicated TOST Lower. first one-sided test examining can reject effects lower -0.5. test result, see case: t = 0.46, p = 0.33. ordinary t-test, just effect -0.5. reject differences extreme -0.5, possible difference consider meaningful (e.g., difference -0.60) present. look one-sided test upper bound equivalence range (0.5) see can statistically reject presence mood effects larger 0.5, line TOST Upper see t = -2.08, p = 0.02. final conclusion therefore , even though can reject effects extreme 0.5 based observed mean difference -0.32, reject effects extreme -0.5. Therefore, completely reject presence meaningful mood effects. data allow us claim effect different 0, effect , anything, small matter (based equivalence range -0.5 0.5), data inconclusive. distinguish Type 2 error (effect, study just detect ) true negative (really effect large enough matter).Note fail reject one-sided test lower equivalence bound, possibility remains true effect size large enough considered meaningful. statement true, even effect size observed (-0.32) closer zero equivalence bound -0.5. One might think observed effect size needs extreme (.e., < -0.5 > 0.5) equivalence bound maintain possibility effect large enough considered meaningful. required. 90% CI indicates values -0.5 rejected. can expect 90% confidence intervals long run capture true population parameter, perfectly possible true effect size extreme -0.5. , effect might even extreme values captured confidence interval, 10% time, computed confidence interval expected contain true effect size. Therefore, fail reject smallest effect size interest, retain possibility effect interest exists. can reject nil null hypothesis, fail reject values extreme equivalence bounds, can claim effect, might large enough meaningful.One way reduce probability inconclusive effect collect sufficient data. imagine researchers collected 15 participants condition, 200 participants. otherwise observe exactly data. explained chapter confidence intervals, sample size increases, confidence interval becomes narrow. TOST equivalence test able reject upper lower bound equivalence range, confidence interval needs fall completely within equivalence range. Figure 9.3 see result Figure 9.2, now collected 200 observations. larger sample size, confidence narrow collected 15 participants. see 90% confidence interval around observed mean difference now excludes upper lower equivalence bound. means can now reject effects outside equivalence range (even though barely, p = 0.048 one-sided test lower equivalence bound just statistically significant).\nFigure 9.3: mean difference confidence interval equivalence test equivalence range -0.5 0.5.\nFigure 9.4 see results, now visualized confidence density plot (Schweder & Hjort, 2016), graphical summary distribution confidence. confidence density plot allows see effects can rejected difference confidence interval widths. see bounds green area (corresponding 90% confidence interval) fall inside equivalence bounds. Thus, equivalence test statistically significant, can statistically reject presence effects outside equivalence range. can also see 95% confidence interval excludes 0, therefore, traditional null hypothesis significance test also statistically significant.\nFigure 9.4: mean difference confidence interval equivalence test equivalence range -0.5 0.5.\nwords, null hypothesis test equivalence test yielded significant results. means can claim observed effect statistically different zero, effect statistically smaller effects deemed large enough matter specified equivalence range -0.5 0.5. illustrates combining equivalence tests nil null hypothesis tests can prevent us mistaking statistically significant effects practically significant effects. case, 200 participants, can reject effect 0, effect, , large enough meaningful.","code":"\nTOSTER::tsum_TOST(m1 = 4.55, \n                  m2 = 4.87, \n                  sd1 = 1.05, \n                  sd2 = 1.11,\n                  n1 = 15, \n                  n2 = 15, \n                  low_eqbound = -0.5, \n                  high_eqbound = 0.5)## \n## Welch Modified Two-Sample t-Test\n## Hypothesis Tested: Equivalence\n## Equivalence Bounds (raw):-0.500 & 0.500\n## Alpha Level:0.05\n## The equivalence test was non-significant, t(27.91) = 0.456, p = 3.26e-01\n## The null hypothesis test was non-significant, t(27.91) = -0.811, p = 4.24e-01\n## NHST: don't reject null significance hypothesis that the effect is equal to zero \n## TOST: don't reject null equivalence hypothesis\n## \n## TOST Results \n##                     t        SE       df    p.value\n## t-test     -0.8111280 0.3945124 27.91398 0.42415467\n## TOST Lower  0.4562595 0.3945124 27.91398 0.32586680\n## TOST Upper -2.0785154 0.3945124 27.91398 0.02348582\n## \n## Effect Sizes \n##                 estimate        SE   lower.ci  upper.ci conf.level\n## Raw           -0.3200000 0.3945124 -0.9911879 0.3511879        0.9\n## Hedges' g(av) -0.2881401 0.3812249 -0.9301965 0.3193638        0.9\n## \n## Note: SMD confidence intervals are an approximation. See vignette(\"SMD_calcs\").## \n## Welch Modified Two-Sample t-Test\n## Hypothesis Tested: Equivalence\n## Equivalence Bounds (raw):-0.500 & 0.500\n## Alpha Level:0.05\n## The equivalence test was significant, t(396.78) = 1.666, p = 4.82e-02\n## The null hypothesis test was significant, t(396.78) = -2.962, p = 3.24e-03\n## NHST: reject null significance hypothesis that the effect is equal to zero \n## TOST: reject null equivalence hypothesis\n## \n## TOST Results \n##                    t        SE       df      p.value\n## t-test     -2.961821 0.1080417 396.7773 3.242190e-03\n## TOST Lower  1.666024 0.1080417 396.7773 4.824893e-02\n## TOST Upper -7.589665 0.1080417 396.7773 1.156039e-13\n## \n## Effect Sizes \n##                 estimate        SE   lower.ci   upper.ci conf.level\n## Raw           -0.3200000 0.1080417 -0.4981286 -0.1418714        0.9\n## Hedges' g(av) -0.2956218 0.1008059 -0.4625958 -0.1310411        0.9\n## \n## Note: SMD confidence intervals are an approximation. See vignette(\"SMD_calcs\")."},{"path":"equivalencetest.html","id":"reporting-equivalence-tests","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.2 Reporting Equivalence Tests","text":"common practice report test yielding higher p-value two one-sided tests reporting equivalence test. one-sided tests need statistically significant reject null hypothesis equivalence test (.e., presence effects large enough matter), larger two hypothesis tests rejects equivalence bound, test. Unlike null hypothesis significance tests common report standardized effect sizes equivalence tests, can situations researchers might want discuss far effect removed equivalence bounds raw scale. Prevent erroneous interpretation claim 'effect', effect 'absent', true effect size 'zero', vague verbal descriptions, two groups yielded 'similar' 'comparable' data. significant equivalence test rejects effects extreme equivalence bounds. Smaller true effects rejected, thus remains possible true effect. TOST procedure frequentist test based p-value, misconceptions p-values prevented well.summarizing main result equivalence test, example abstract, always report equivalence range data tested . Reading 'based equivalence test concluded absence meaningful effect' means something different equivalence bounds d =-0.9 0.9 bounds d =-0.2 d =0.2. instead, write 'based equivalence test equivalence range d =-0.2 0.2, conclude absence effect deemed meaningful'. course, whether peers agree correctly concluded absence meaningful effect depends whether agree justification smallest effect interest! neutral conclusion statement : 'based equivalence test, rejected presence effects extreme -0.2 0.2, can act (error rate alpha) effect, , less extreme equivalence range'. , use value-laden terms 'meaningful'. null hypothesis test equivalence test non-significant, finding best described 'inconclusive': enough data reject null, smallest effect size interest. null hypothesis test equivalence test statistically significant, can claim effect, time claim effect small interest (given justification equivalence range).Equivalence bounds can specified raw effect sizes, standardized mean differences. better specify equivalence bounds terms raw effect sizes. Setting terms Cohen's d leads bias statistical test, observed standard deviation used translate specified Cohen's d raw effect size equivalence test (set equivalence bounds standardized mean differences, TOSTER warn: \"Warning: setting bound type SMD produces biased results!\"). bias practice problematic single equivalence test, able specify equivalence bounds standardized mean differences lowers threshold perform equivalence test know standard deviation measure. equivalence testing becomes popular, fields establish smallest effect sizes interest, raw effect size differences, standardized effect size differences.","code":""},{"path":"equivalencetest.html","id":"MET","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.3 Minimum Effect Tests","text":"researcher specified smallest effect size interest, interested testing whether effect population larger smallest effect interest, minimum effect test can performed. hypothesis test, can reject smallest effect interest whenever confidence interval around observed effect overlap . case minimum effect test, however, confidence interval fall completely beyond smallest effect size interest. example, assume researcher performs minimum effect test 200 observations per condition smallest effect size interest mean difference 0.5.\nFigure 9.5: mean difference confidence interval plotted t-distributions used perform two-one-sided tests -0.5 0.5 performing minimum effect test.\ntwo curves see line represents confidence interval ranging 0.68 1.04, dot line indicates observed mean difference 0.86. entire confidence interval lies well minimum effect 0.5, can therefore just reject nil null hypothesis, also effects smaller minimum effect interest. Therefore, can claim effect large enough just statistically significant, also practically significant (long justified smallest effect size interest well). performed two-sided minimum effect test, minimum effect test also significant confidence interval completely opposite side -0.5.Earlier discussed combining traditional NHST equivalence test lead informative results. also possible combine minimum effect test equivalence test. One might even say combination informative test prediction whenever smallest effect size interest can specified. principle, true. long able collect enough data, always get informative straightforward answer combine minimum effect test equivalence test: Either can reject effects small interest, can reject effects large enough interest. see section power analysis interval hypotheses, whenever true effect size close smallest effect size interest, large amount observations need collected. true effect size happens identical smallest effect size interest, neither minimum effect test equivalence test can correctly rejected (significant test Type 1 error). researcher can collect sufficient data (test high statistical power), relatively confident true effect size larger smaller smallest effect interest, combination minimum effect test equivalence test can attractive hypothesis test likely yield informative answer research question.","code":"## \n## Welch Modified Two-Sample t-Test\n## Hypothesis Tested: Minimal Effect\n## Equivalence Bounds (raw):-0.500 & 0.500\n## Alpha Level:0.05\n## The minimal effect test was significant, t(396.78) = 12.588, p = 4.71e-04\n## The null hypothesis test was significant, t(396.78) = 7.960, p = 1.83e-14\n## NHST: reject null significance hypothesis that the effect is equal to zero \n## TOST: reject null MET hypothesis\n## \n## TOST Results \n##                    t        SE       df      p.value\n## t-test      7.959893 0.1080417 396.7773 1.827800e-14\n## TOST Lower 12.587737 0.1080417 396.7773 1.000000e+00\n## TOST Upper  3.332048 0.1080417 396.7773 4.714941e-04\n## \n## Effect Sizes \n##                estimate        SE  lower.ci  upper.ci conf.level\n## Raw           0.8600000 0.1080417 0.6818714 1.0381286        0.9\n## Hedges' g(av) 0.7944836 0.1041808 0.6263676 0.9689959        0.9\n## \n## Note: SMD confidence intervals are an approximation. See vignette(\"SMD_calcs\")."},{"path":"equivalencetest.html","id":"power-analysis-for-interval-hypothesis-tests","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.4 Power Analysis for Interval Hypothesis Tests","text":"designing study sensible strategy always plan presence absence effect. Several scientific journals require sample size justification Registered Reports statistical power reject null hypothesis high, study also capable demonstrating absence effect, example also performing power analysis equivalence test. saw chapter error control likelihoods null results expected, think possibility observing null effect data collected, often late.statistical power interval hypotheses depend alpha level, sample size, smallest effect interest decide test , true effect size. equivalence test, common perform power analysis assuming true effect size 0, might always realistic. closer expected effect size smallest effect size interest, larger sample size needed reach desired power. tempted assume true effect size 0, good reason expect small non-zero true effect size. sample size power analysis indicates need collect might smaller, reality also higher probability inconclusive result. Earlier versions TOSTER enabled researchers perform power analyses equivalence tests assuming true effect size 0, new power function Aaron Caldwell allows users specify delta, expected effect size.Assume researchers desired achieve 90% power equivalence test equivalence range -0.5 0.5, alpha level 0.05, assuming population effect size 0. power analysis equivalence test can performed examine required sample size.see required sample size 88 participants condition independent t-test. compare power analysis situation researcher expects true effect d = 0.1, instead true effect 0. able reliably reject effects larger 0.5, need larger sample size, just need larger sample size null hypothesis test powered detect d = 0.4 null hypothesis test powered detect d = 0.5.see sample size now increased 109 participants condition. mentioned , necessary perform two-sided equivalence test. also possible perform one-sided equivalence test. example situation directional test appropriate replication study. previous study observed effect d = 0.48, perform replication study, might decide consider effect smaller d = 0.2 failure replicate - including effect opposite direction, effect d = -0.3. Although software equivalence tests requires specify upper lower bound equivalence range, can mimic one-sided test setting equivalence bound direction want ignore low value one-sided test value always statistically significant. can also used perform power analysis minimum effect test, one bound minimum effect interest, bound set extreme value side expected effect size.power analysis equivalence test example , lower bound set -5 (set low enough lowering even noticeable effect). see new power function TOSTER package takes directional prediction account, just directional predictions nil null hypothesis test, directional prediction equivalence test efficient, 70 observations needed achieve 90% power.Statistical software offers options power analyses statistical tests, tests. Just power analysis nil null hypothesis test, can necessary use simulation-based approach power analysis.","code":"\nTOSTER::power_t_TOST(power = 0.9, delta = 0,\n                     alpha = 0.05, type = \"two.sample\",\n                     low_eqbound = -0.5, high_eqbound = 0.5)## \n##      Two-sample TOST power calculation \n## \n##           power = 0.9\n##            beta = 0.1\n##           alpha = 0.05\n##               n = 87.26261\n##           delta = 0\n##              sd = 1\n##          bounds = -0.5, 0.5\n## \n## NOTE: n is number in *each* group\nTOSTER::power_t_TOST(power = 0.9, delta = 0.1,\n                     alpha = 0.05, type = \"two.sample\",\n                     low_eqbound = -0.5, high_eqbound = 0.5)## \n##      Two-sample TOST power calculation \n## \n##           power = 0.9\n##            beta = 0.1\n##           alpha = 0.05\n##               n = 108.9187\n##           delta = 0.1\n##              sd = 1\n##          bounds = -0.5, 0.5\n## \n## NOTE: n is number in *each* group\n# New TOSTER power functions allows power for expected non-zero effect.\nTOSTER::power_t_TOST(power = 0.9, delta = 0,\n                     alpha = 0.05, type = \"two.sample\",\n                     low_eqbound = -5, high_eqbound = 0.5)## \n##      Two-sample TOST power calculation \n## \n##           power = 0.9\n##            beta = 0.1\n##           alpha = 0.05\n##               n = 69.19784\n##           delta = 0\n##              sd = 1\n##          bounds = -5.0, 0.5\n## \n## NOTE: n is number in *each* group"},{"path":"equivalencetest.html","id":"ROPE","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.5 The Bayesian ROPE procedure","text":"Bayesian estimation, one way argue absence meaningful effect region practical equivalence (ROPE) procedure (Kruschke (2013)), “somewhat analogous frequentist equivalence testing” (Kruschke & Liddell (2017)). ROPE procedure, equivalence range specified, just equivalence testing, Bayesian highest density interval based posterior distribution (explained chapter Bayesian statistics) used instead confidence interval.prior used Kruschke perfectly uniform, ROPE procedure equivalence test used confidence interval (e.g., 90%), two tests yield identical results. philosophical differences numbers interpreted. BEST package R can used perform ROPE procedure default uses ‘broad’ prior, therefore results ROPE procedure equivalence test exactly , close. One might even argue two tests 'practically equivalent'. R code , random normally distributed data two conditions generated (means 0 standard deviation 1) ROPE procedure TOST equivalence test performed.90% HDI ranges -0.06 0.39, estimated mean based prior data 0.164. HDI falls completely upper lower bound equivalence range, therefore values extreme -0.5 0.5 deemed implausible. 95% CI ranges -0.07 0.36 observed mean difference 0.15. see numbers identical, Bayesian estimation observed values combined prior, mean estimate purely based data. results similar, cases lead similar inferences. BEST R package also enables researchers perform simulation based power analyses, take long time , using broad prior, yield result basically identical sample size power analysis equivalence test. biggest benefit ROPE TOST allows incorporate prior information. reliable prior information, ROPE can use information, especially useful don’t lot data. use informed priors, check robustness posterior reasonable changes prior sensitivity analyses.","code":""},{"path":"equivalencetest.html","id":"whichinterval","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.6 Which interval width should be used?","text":"TOST procedure based two one-sided tests, 90% confidence interval used one-sided tests performed alpha level 5%. test upper bound test lower bound needs statistically significant declare equivalence (explained chapter error control intersection-union approach multiple testing) necessary correct fact two tests performed. alpha level adjusted multiple comparisons, alpha level justified instead relying default 5% level (), corresponding confidence interval used, CI = 100 - (2 * \\(\\alpha\\)). Thus, width confidence interval directly related choice alpha level, making decisions reject smallest effect size interest, , based whether confidence interval excluded effect tested .using Highest Density Interval Bayesian perspective, ROPE procedure, choice width confidence interval follow logically desired error rate, principle. Kruschke (2014) writes: “define 'reasonably credible'? One way saying points within 95% HDI reasonably credible.” McElreath (2016) recommended use 67%, 89%, 97%, \"reason. prime numbers, makes easy remember.\". suggestions lack solid justification. Gosset (Student), observed (1904):Results valuable amount probably differ truth small insignificant purposes experiment. odds selected depends-\n1. degree accuracy nature experiment allows, \n2. importance issues stake.two principled solutions. First, highest density interval width used make claims, claims made certain error rates, researchers quantify risk erroneous claims computing frequentist error rates. make ROPE procedure Bayesian/Frequentist compromise procedure, computation posterior distribution allows Bayesian interpretations parameters values believed probable, decisions based whether HDI falls within equivalence range formally controlled error rate. Note using informative prior, HDI match CI, error rate using HDI can derived simulations. second solution make claims, present full posterior distribution, let readers draw conclusions.","code":""},{"path":"equivalencetest.html","id":"sesoi","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.7 Setting the Smallest Effect Size of Interest","text":"able falsify predictions using equivalence test specify observed values small predicted theory. can never say effect exactly zero, can examine whether observed effects small theoretically practically interesting. requires specify smallest effect size interest (SESOI). concept goes many names, minimal important difference, clinically significant difference (King, 2011). Take moment think smallest effect size still consider theoretically practically meaningful next study designing. might difficult determine smallest effect size consider interesting, question smallest effect size interest might something never really thought begin . However, determining smallest effect size interest important practical benefits. First, researchers field able specify effects small matter, becomes straightforward power study effects meaningful. second benefit specifying smallest effect size interest makes study falsifiable. predictions falsified someone else might feel great personally, quite useful science whole (Popper, 2002). , way prediction can wrong, anyone impressed prediction right?start thinking effect sizes matter, ask whether effect predicted direction actually support alternative hypothesis. example, effect size Cohen's d 10 support hypothesis? psychology, rare theory prediucts huge effect, observed d = 10, probably check either computation error, confound study. end scale, effect d = 0.001 line theoretically proposed mechanism? effect incredibly small, well individual notice, fall just noticeable difference given perceptual cognitive limitations. Therefore, d = 0.001 cases lead researchers conclude \"Well, really small something theory predicted, small effect practically equivalent absence effect.\" However, make directional prediction, say types effects part alternative hypothesis. Even though many researchers agree tiny effects small matter, still officially support alternative hypothesis directional prediction nil null hypothesis. Furthermore, researchers rarely resources statistically reject presence effects small, claim effects still support theoretical prediction makes theory practically unfalsifiable: researcher simply respond replication study showing non-significant small effect (e.g., d = 0.05) saying: \"falsify prediction. suppose effect just bit smaller d = 0.05\", without ever admit prediction falsified. problematic, process replication falsification, scientific discipline risks slide towards unfalsifiable (Ferguson & Heene, 2012). whenever possible, design experiment theory theoretical prediction, carefully think , clearly state, smallest effect size interest .","code":""},{"path":"equivalencetest.html","id":"specifying-a-sesoi-based-on-theory","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.8 Specifying a SESOI based on theory","text":"One example theoretically predicted smallest effect size interest can found study Burriss et al. (2015), examined whether women displayed increased redness face fertile phase ovulatory cycle. hypothesis slightly redder skin signals greater attractiveness physical health, sending signal men yields evolutionary advantage. hypothesis presupposes men can detect increase redness naked eye. Burriss et al. collected data 22 women showed redness facial skin indeed increased fertile period. However, increase large enough men detect naked eye, hypothesis falsified. just-noticeable difference redness skin can measured, possible establish theoretically motivated SESOI. theoretically motivated smallest effect size interest can derived just-noticeable differences, provide lower bound effect sizes can influence individuals, based computational models, can provide lower bound parameters model still able explain observed findings empirical literature.","code":""},{"path":"equivalencetest.html","id":"anchor-based-methods-to-set-a-sesoi","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.9 Anchor based methods to set a SESOI","text":"Building idea just-noticeable difference, psychologists often interested effects large enough noticed single individuals. One procedure estimate constitutes meaningful change individual level anchor-based method (Jaeschke et al., 1989; King, 2011; Norman et al., 2004). Measurements collected two time points (e.g., quality life measure treatment). second time point, independent measure (anchor) used determine individuals show change compared time point 1, improved, worsened. Often, patient directly asked answer anchor question, indicate subjectively feel , better, worse time point 2 compared time point 1. Button et al. (2015) used anchor-based method estimate minimal clinically important difference Beck Depression Inventory corresponded 17.5% reduction scores baseline.Anvari Lakens (2021) applied anchor-based method examine smallest effect interest measured widely used Positive Negative Affect Scale (PANAS). Participants completed 20 item PANAS two time points several days apart (using Likert scale going 1 = “slightly ”, 5 = “extremely”). second time point also asked indicate affect changed little, lot, . people indicated affect changed “little”, average change Likert units 0.26 scale points positive affect 0.28 scale points negative affect. Thus, intervention improve people’s affective state lead individuals subjectively consider least little improvement might set SESOI 0.3 units PANAS.","code":""},{"path":"equivalencetest.html","id":"specifying-a-sesoi-based-on-a-cost-benefit-analysis","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.10 Specifying a SESOI based on a cost-benefit analysis","text":"Another principled approach justify smallest effect size interest perform cost-benefit analysis. Research shows cognitive training may improve mental abilities older adults might benefit older drivers (Ball et al., 2002). Based findings, Viamonte, Ball, Kilgore (2006) performed cost-benefit analysis concluded based cost intervention ($247.50), probability accident drivers older 75 (p = 0.0710), cost accident ($22,000), performing intervention drivers aged 75 older efficient intervening intervening screening test. Furthermore, sensitivity analyses revealed intervening drivers remain beneficial long reduction collision risk 25%. Therefore, 25% reduction probability elderly 75 getting car accident set smallest effect size interest.another example, economists examined value statistical life, based willingness pay reduce risk death, $1.5 - $2.5 million (year 2000, western countries, see Mrozek & Taylor (2002)). Building work, Abelson (2003) calculated willingness pay prevent acute health issues eye irritation $40-$50 per day. researcher may examining psychological intervention reduces amount times people touch face close eyes, thereby reducing eye irritations caused bacteria. intervention costs $20 per year administer, therefore reduce average number days eye irritation population least 0.5 days intervention worth cost. cost-benefit analysis can also based resources required empirically study small effect weighed value knowledge scientific community.","code":""},{"path":"equivalencetest.html","id":"specifying-the-sesoi-using-the-small-telescopes-approach","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.11 Specifying the SESOI using the small telescopes approach","text":"Ideally, researchers publish empirical claims always specify observations falsify claim. Regrettably, yet common practice. particularly problematic researcher performs close replication earlier work. never possible prove effect exactly zero, original authors seldom specify range effect sizes falsify hypotheses, proven difficult interpret outcome replication study (S. F. Anderson & Maxwell, 2016). new data contradict original finding?Consider study want test idea wisdom crowds. ask 20 people estimate number coins jar, expecting average close true value. research question whether people can average correctly guess number coins, 500. observed mean guess 20 people 550, standard deviation 100. observed difference true value statistically significant, t(19)=2.37, p = 0.0375, Cohen’s d 0.5. Can really group average far ? Wisdom Crowds? something special coins used make especially difficult guess number? just fluke? set perform close replication study.want study informative, regardless whether effect . means need design replication study allow draw informative conclusion, regardless whether alternative hypothesis true (crowd estimate true number coins accurately) whether null hypothesis true (crowd guess 500 coins, original study fluke). since original researcher specify smallest effect size interest, replication study allow conclude original study contradicted new data? Observing mean exactly 500 perhaps considered quite convincing, due random variation (almost) never find mean score exactly 500. non-significant result can’t interpreted absence effect, study might small sample size detect meaningful effects, result might Type 2 error. can move forward define effect size meaningful? can design study ability falsify previous finding?Uri Simonsohn (2015) defines small effect “one give 33% power original study”. words, effect size give original study odds 2:1 observing statistically significant result effect. idea original study 33% power, probability observing significant effect, true effect, low reliably distinguish signal noise (situations true effect situations true effect). Simonsohn (2015, p. 561) calls small telescopes approach, writes: “Imagine astronomer claiming found new planet telescope. Another astronomer tries replicate discovery using larger telescope finds nothing. Although prove planet exist, nevertheless contradict original findings, planets observable smaller telescope also observable larger one.”Although approach setting smallest effect size interest (SESOI) arbitrary (30% power, 35%?) suffices practical purposes (free choose power level think low). nice thing definition SESOI know sample size original study, can always calculate effect size study 33% power detect. can thus always use approach set smallest effect size interest. fail find support effect size original study 33% power detect, mean true effect, even effect small theoretical practical interest. using small telescopes approach good first step, since get conversation started effects meaningful allows researchers want replicate study specify consider original claim falsified.small telescopes approach, SESOI based sample size original study. smallest effect size interest set effects direction. effects smaller effect (including large effects opposite direction) interpreted failure replicate original results. see small telescopes approach one-sided equivalence test, upper bound specified, smallest effect size interest determined based sample size original study. test examines can reject effects large larger effect original study 33% power detect. simple one-sided test, 0, SESOI.example, consider study 20 guessers tried estimate number coins. results analyzed two-sided one-sample t-test, using alpha level 0.05. determine effect size study 33% power , can perform sensitivity analysis. sensitivity analysis compute required effect size given alpha, sample size, desired statistical power. Note Simonsohn uses two-sided test power analyses, follow – original study reported pre-registered directional prediction, power analysis based one-sided test. case, alpha level 0.05, total sample size 20, desired power 33%. compute effect size gives us 33% power see Cohen’s d 0.358. means can set smallest effect size interest replication study d = 0.358. can reject effects large larger d = 0.358, can conclude effect smaller anything original study 33% power . screenshot illustrates correct settings G*Power, code R :\nFigure 9.6: Screenshot illustrating sensitivity power analysis G*Power compute effect size original study 33% power detect.\nDetermining SESOI based effect size original study 33% power detect additional convenient property. Imagine true effect size actually 0, perform statistical test see data statistically smaller SESOI based small telescopes approach (called inferiority test). increase sample size 2.5 times, approximately 80% power one-sided equivalence test, assuming true effect size exactly 0 (e.g., d = 0). People replication study can follow small telescope recommendations, easily determine smallest effect size interest, sample size needed design informative replication study, assuming true effect size 0 (see section -priori power analyses want test equivalence, expect true effect size 0).figure , Simonsohn (2015) illustrates small telescopes approach using real-life example. original study Zhong Liljenquist (2006) tiny sample size 30 participants condition observed effect size d = 0.53, barely statistically different zero. Given sample size 30 per condition, study 33% power detect effects larger d = 0.401. “small effect” indicated green dashed line. R, smallest effect size interest calculated using:Note 33% power rounded value, calculation uses 1/3 (0.3333333…).\nFigure 9.7: Example used Simonsohn (2015) original study two replication studies.\ncan see first replication Gámez colleagues also relatively small sample size (N = 47, compared N = 60 original study), designed yield informative results interpreted small telescopes approach. confidence interval wide includes null effect (d = 0) smallest effect size interest (d = 0.401). Thus, study inconclusive. can’t reject null, can also reject effect sizes 0.401 larger still considered line original result. second replication much larger sample size, tells us can’t reject null, can reject smallest effect size interest, suggesting effect smaller considered interesting effect based small telescopes approach.Although small telescope recommendations easy use, one take care turn statistical procedure heuristic. example 20 referees, Cohen’s d 0.358 used smallest effect size interest, sample size 50 collected (2.5 times original 20), someone make effort perform replication study, relatively easy collect larger sample size. Alternatively, original study extremely large, high power effects might practically significant, want collect 2.5 times many observations replication study. Indeed, Simonsohn writes: “whether need 2.5 times original sample size depends question wish answer. interested testing whether effect size smaller d33%, , yes, need 2.5 times original sample size matter big original sample . samples large, however, may question interest.” Always think question want ask, design study provides informative answer question interest. automatically follow 2.5 times n heuristic, always reflect whether use suggested procedure appropriate situation.","code":"\nlibrary(\"pwr\")\n\npwr::pwr.t.test(\n  n = 20, \n  sig.level = 0.05, \n  power = 0.33, \n  type = \"one.sample\",\n  alternative = \"two.sided\"\n)## \n##      One-sample t test power calculation \n## \n##               n = 20\n##               d = 0.3577466\n##       sig.level = 0.05\n##           power = 0.33\n##     alternative = two.sided\npwr::pwr.t.test(\n  n = 30, \n  sig.level = 0.05, \n  power = 1/3, \n  type = \"two.sample\",\n  alternative = \"two.sided\"\n)## \n##      Two-sample t test power calculation \n## \n##               n = 30\n##               d = 0.401303\n##       sig.level = 0.05\n##           power = 0.3333333\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group"},{"path":"equivalencetest.html","id":"setting-the-smallest-effect-size-of-interest-to-the-minimal-statistically-detectable-effect","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.12 Setting the Smallest Effect Size of Interest to the Minimal Statistically Detectable Effect","text":"Given sample size alpha level, every test minimal statistically detectable effect. example, given test 86 participants group, alpha level 5%, t-tests yield t ≥ 1.974 statistically significant. words, t = 1.974 critical t-value. Given sample size alpha level, critical t-value can transformed critical d-value. visualized Figure 9.8, n = 50 group alpha level 5% critical d-value 0.4. means effects larger 0.4 yield p < α. critical d-value influenced sample size per group, alpha level, depend true effect size.\nFigure 9.8: Null alternative distribution Type 1 Type 2 error indicating smallest effect size statistically significant n = 50 per condition.\npossible observe statistically significant test result true effect size smaller critical effect size. Due random variation, possible observe larger value sample true value population. reason statistical power test never 0 null hypothesis significance test. illustrated Figure 9.9, even true effect size smaller critical value (.e., true effect size 0.2) see distribution can expect observed effect sizes larger 0.4 true population effect size d = 0.2 – compute statistical power test, turns can expect 16.77% observed effect sizes larger 0.4, long run. lot, something. also reason publication bias combined underpowered research problematic: leads large overestimation true effect size observed effect sizes statistically significant findings underpowered studies end scientific literature.\nFigure 9.9: Null alternative distribution Type 1 Type 2 error indicating smallest effect size statistically significant n = 50 per condition.\ncan use minimal statistically detectable effect set SESOI replication studies. attempt replicate study, one justifiable option choosing smallest effect size interest (SESOI) use smallest observed effect size statistically significant study replicating. words, decide effects yielded p-value less α original study considered meaningful replication study. assumption original authors interested observing significant effect, thus interested observed effect sizes yielded significant result. might likely original authors consider effect sizes study good statistical power detect, interested smaller effects gambled observing especially large effect sample purely result random variation. Even , building earlier research specify SESOI, justifiable starting point might set SESOI smallest effect size , observed original study, statistically significant. researchers might agree (e.g., original authors might say actually cared just much effect d =0.001). However, try change field current situation one specifies falsify hypothesis, smallest effect size interest , approach one way get started. practice, explained section post-hoc power, due relation p = 0.05 50% power observed effect size, justification SESOI mean SESOI set effect size original study 50% power detect independent ttest. approach ways similar small telescopes approach Simonsohn (2015), except lead somewhat larger SESOI.Setting smallest effect size interest replication study bit like tennis match. Original authors serve hit ball across net, saying ‘look, something going ’. approach set SESOI effect size significant original study return volley allows say ‘seem anything large enough significant original study’ performing well-designed replication study high statistical power reject SESOI. never end match – original authors can attempt return ball specific statement effects theory predicts, demonstrate smaller effect size present. ball back court, want continue claim effect, support claim new data.Beyond replication studies, amount data collected limits inferences one can make. also possible compute minimal statistically detectable effect based sample sizes typically used research field. example, imagine line research hypothesis almost always\ntested performing one-sample t-test, sample sizes collected always smaller 100 observations. one-sample t-test 100 observations, using alpha .05 (two sided), 80% power detect effect d = 0.28 (can calculated sensitivity power analysis). new study, concluding one can reliably reject presence effects extreme d = 0.28 suggests sample sizes 100 might enough detect effects research lines. Rejecting presence effects extreme d = 0.28 test theoretical prediction, contributes literature answering resource question. suggests future studies research line need change design studies substantially increasing sample size. Setting smallest effect size interest based approach answer theoretical question (, SESOI based theoretical prediction). informing peers given sample size commonly collected field field, effect large enough can reliably studied useful contribution literature. mean effect interesting per se, field might decide time examine research question collaboratively, coordinating research lines, collecting enough data reliably study whether smaller effect present.","code":""},{"path":"equivalencetest.html","id":"test-yourself-7","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.13 Test Yourself","text":"","code":""},{"path":"equivalencetest.html","id":"questions-about-equivalence-tests","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.13.1 Questions about equivalence tests","text":"Q1: 90% CI around mean difference falls just within equivalence range -0.4 0.4, can reject smallest effect size interest. Based knowledge confidence intervals, equivalence range changed -0.3 – 0.3, needed equivalence test significant (assuming effect size estimate standard deviation remains )?larger effect size.lower alpha level.larger sample size.Lower statistical power.Q2: incorrect conclude effect, equivalence test statistically significant?equivalence test statement data, presence absence effect.result equivalence test Type 1 error, therefore, one conclude effect, Type 1 error observed.equivalence test rejects values large larger smallest effect size interest, possibility small non-zero effect rejected.conclude effect equivalence test non-significant, equivalence test significant.Q3: Researchers interested showing students use online textbook perform just well students use paper textbook. , can recommend teachers allow students choose preferred medium, benefit, recommend medium leads better student performance. randomly assign students use online textbook paper textbook, compare grades exam course (worst possible grade, 1, best possible grade, 10). find groups students perform similarly, paper textbook condition m = 7.35, sd = 1.15, n = 50, online textbook m = 7.13, sd = 1.21, n = 50). Let’s assume consider effect large larger half grade point (0.5) worthwhile, difference smaller 0.5 small matter, alpha level set 0.05. authors conclude? Copy code R, replacing zeroes correct numbers. Type ?tsum_TOST help function.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.Q4: increase sample size question Q3 150 participants condition, assuming observed means standard deviations exactly , conclusion draw?can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.Q5: increase sample size question Q3 500 participants condition, assuming observed means standard deviations exactly , conclusion draw?can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.Sometimes result test inconclusive, null hypothesis test, equivalence test, statistically significant. solution case collect additional data. Sometimes null hypothesis test equivalence test statistically significant, case effect statistically different zero, practically insignificant (based justification SESOI).Q6: might wonder statistical power test Q3, assuming true difference two groups (true effect size 0). Using new improved power_t_TOST function TOSTER R package, can compute power using sensitivity power analysis (.e., entering sample size per group 50, assumed true effect size 0, equivalence bounds, alpha level. Note equivalence bounds specified raw scale Q3, also need specify estimate true standard deviation population. assume true standard deviation 1.2. Round answer two digits decimal. Type ?power_t_TOST help function. power Q3?0.000.050.330.40Q7: Assume 15 participants group Q3, instead 50. statistical power test smaller sample size (keeping settings Q6)? Round answer 2 digits.0.000.050.330.40Q8: might remember discussions statistical power null hypothesis significance test statistical power never smaller 5% (true effect size 0, power formally undefined, observe least 5% Type 1 errors, power increases introducing true effect). two-sided equivalence tests, power can lower alpha level. ?equivalence test Type 1 error rate bounded 5%.equivalence test null hypothesis alternative hypothesis reversed, therefore Type 2 error rate lower bound (just Type 1 error rate NHST lower bound).confidence interval needs fall lower upper bound equivalence interval, small sample sizes, probability can close one (confidence interval wide).equivalence test based confidence interval, p-value, therefore power limited alpha level.Q9: well designed study high power detect effect interest, also reject smallest effect size interest. Perform -priori power analysis situation described Q3. sample size group needs collected achieved desired statistical power 90% (0.9), assuming true effect size 0, still assume true standard deviation 1.2? Use code , round sample size (collect partial observation).100126200252Q10: Assume performing power analysis Q9 expect true effect size 0, actually expected mean difference 0.1 grade point. sample size group need collect equivalence test, now expect true effect size 0.1? Change variable delta power_t_TOST answer question.1171573143118Q11: Change equivalence range -0.1 0.1 Q9 (set expected effect size delta 0). able reject effects outside narrow equivalence range, ’ll need large sample size. alpha 0.05, desired power 0.9 (90%), many participants need group?1107115724683118You can see takes large sample size high power reliably reject small effects. surprising. , also requires large sample size detect small effects! typically leave future meta-analysis detect, reject, presence small effects.Q12: can equivalence tests tests. TOSTER package functions t-tests, correlations, differences proportions, meta-analyses. test want perform included software, remember can just use 90% confidence interval, test whether can reject smallest effect size interest. Let’s perform equivalence test meta-analysis. Hyde, Lindberg, Linn, Ellis, Williams (2008) report effect sizes gender differences\nmathematics tests across 7 million students US represent trivial differences, trivial difference specified effect size smaller d =0.1. table Cohen’s d se reproduced :grade 2, perform equivalence test boundaries d =-0.1 d =0.1, using alpha 0.01, conclusion can draw? Use TOSTER function TOSTmeta, enter alpha, effect size (ES), standard error (se), equivalence bounds.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.can reject effect size zero, can reject presence effects large larger smallest effect size interest.","code":"\nTOSTER::tsum_TOST(\n  m1 = 0.00,\n  sd1 = 0.00,\n  n1 = 0,\n  m2 = 0.00,\n  sd2 = 0.00,\n  n2 = 0,\n  low_eqbound = -0.0,\n  high_eqbound = 0.0,\n  eqbound_type = \"raw\",\n  alpha = 0.05\n)\nTOSTER::power_t_TOST(\n  n = 00,\n  delta = 0.0,\n  sd = 0.0,\n  low_eqbound = -0.0,\n  high_eqbound = 0.0,\n  alpha = 0.05,\n  type = \"two.sample\"\n)\nTOSTER::power_t_TOST(\n  power = 0.00,\n  delta = 0.0,\n  sd = 0.0,\n  low_eqbound = -0.0,\n  high_eqbound = 0.0,\n  alpha = 0.05,\n  type = \"two.sample\"\n)\nTOSTER::TOSTmeta(\n  ES = 0.00,\n  se = 0.000,\n  low_eqbound_d = -0.0,\n  high_eqbound_d = 0.0,\n  alpha = 0.05\n)"},{"path":"equivalencetest.html","id":"questions-about-the-small-telescopes-approach","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.13.2 Questions about the small telescopes approach","text":"Q13: smallest effect size interest based small telescopes approach, original study collected 20 participants condition independent t-test, alpha level 0.05. Note answer, happens depend whether enter power 0.33 1/3 (0.333). can use code , relies pwr package.d =0.25 (setting power 0.33) 0.26 (setting power 1/3)d =0.33 (setting power 0.33) 0.34 (setting power 1/3)d =0.49 (setting power 0.33) 0.50 (setting power 1/3)d =0.71 (setting power 0.33) 0.72 (setting power 1/3)Q14: Let’s assume trying replicate previous result based correlation two-sided test. study 150 participants. Calculate SESOI using small telescopes justification replication study use alpha level 0.05. Note answer, happens depend whether enter power 0.33 1/3 (0.333). can use code .r = 0.124 (setting power 0.33) 0.125 (setting power 1/3)r = 0.224 (setting power 0.33) 0.225 (setting power 1/3)r = 0.226 (setting power 0.33) 0.227 (setting power 1/3)r = 0.402 (setting power 0.33) 0.403 (setting power 1/3)Q15: age big data researchers often access large databases, can run correlations samples thousands observations. Let’s assume original study previous question 150 observations, 15000 observations. still use alpha level 0.05. Note answer, happens depend whether enter power 0.33 1/3 (0.333). SESOI based small telescopes approach?r = 0.0124 (setting power 0.33) 0.0125 (setting power 1/3)r = 0.0224 (setting power 0.33) 0.0225 (setting power 1/3)r = 0.0226 (setting power 0.33) 0.0227 (setting power 1/3)r = 0.0402 (setting power 0.33) 0.0403 (setting power 1/3)effect likely practically theoretically significant? Probably . situation small telescopes approach useful procedure determine smallest effect size interest.Q16: Using small telescopes approach, set SESOI replication study d = 0.35, set alpha level 0.05. collecting data well-powered replication study close original study practically possible, find significant effect, can reject effects large larger d = 0.35. correct interpretation result?effect.can statistically reject (using alpha 0.05) effects anyone find theoretically meaningful.can statistically reject (using alpha 0.05) effects anyone find practically relevant.can statistically reject (using alpha 0.05) effects original study 33% power detect.","code":"\npwr::pwr.t.test(\n  n = 0, \n  sig.level = 0.00, \n  power = 0, \n  type = \"two.sample\",\n  alternative = \"two.sided\"\n)\npwr::pwr.r.test(\n  n = 0, \n  sig.level = 0, \n  power = 0, \n  alternative = \"two.sided\")"},{"path":"equivalencetest.html","id":"questions-about-specifying-the-sesoi-as-the-minimal-statistically-detectable-effect","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.13.3 Questions about specifying the SESOI as the Minimal Statistically Detectable Effect","text":"Q17: Open online Shiny app can used compute minimal statistically detectable effect two independent groups: https://shiny.ieis.tue.nl/d_p_power/. Three sliders influence figure looks like: sample size per condition, true effect size, alpha level. statement true?critical d-value influenced sample size per group, true effect size, alpha level.critical d-value influenced sample size per group, alpha level, true effect size.critical d-value influenced alpha level, true effect size, sample size per group.critical d-value influenced sample size per group, alpha level, true effect size.Q18: Imagine researchers performed study 18 participants condition, performed t-test using alpha level 0.01. Using Shiny app, smallest effect size statistically significant study?d = 0.47d = 0.56d = 0.91d = 1Q19: expect true effect size next study d = 0.5, plan use alpha level 0.05. collect 30 participants group independent t-test. statement true?low power possible effect sizes.sufficient (.e., > 80%) power effect sizes interested .Observed effect sizes d = 0.5 never statistically significant.Observed effect sizes d = 0.5 statistically significant.example used far based performing independent t-test, idea can generalized. shiny app F-test available : https://shiny.ieis.tue.nl/f_p_power/. effect size associated power F-test partial eta squared (\\(\\eta_{p}^{2})\\), One-Way ANOVA (visualized Shiny app) equals eta-squared.distribution eta-squared looks slightly different distribution Cohen’s d, primarily F-test one-directional test (, eta-squared values positive, Cohen’s d can positive negative). light grey line plots expected distribution eta-squared null true, red area curve indicating Type 1 errors, black line plots expected distribution eta-squared true effect size η = 0.059. blue area indicates expected effect sizes smaller critical η 0.04, statistically significant, thus Type 2 errors.\nFigure 9.10: Illustration criticial F-value two groups, 50 observations per group, alpha level 0.05.\nQ20: Set number participants (per condition) 14, number groups 3. Using Shiny app https://shiny.ieis.tue.nl/f_p_power/ effect sizes (expressed partial eta-squared, indicated vertical axis) can statistically significant n = 14 per group, 3 groups?effects larger 0.11Only effects larger 0.13Only effects larger 0.14Only effects larger 0.16Every sample size alpha level implies minimal statistically detectable effect can statistically significant study. Looking observed effects can detect useful way make sure actually detect smallest\neffect size interested .Q21: Using minimal statistically detectable effect, set SESOI replication study d = 0.35, set alpha level 0.05. collecting data well-powered replication study close original study practically possible, find significant effect, can reject effects large larger d = 0.35. correct interpretation result?effect.can statistically reject (using alpha 0.05) effects anyone find theoretically meaningful.can statistically reject (using alpha 0.05) effects anyone find practically relevant.can statistically reject (using alpha 0.05) effects statistically significant original study.","code":""},{"path":"equivalencetest.html","id":"open-questions-7","chapter":"9 Equivalence Testing and Interval Hypotheses","heading":"9.13.4 Open Questions","text":"meant statement ‘Absence evidence evidence absence’?meant statement ‘Absence evidence evidence absence’?goal equivalence test?goal equivalence test?difference nil null hypothesis non-nil null hypothesis?difference nil null hypothesis non-nil null hypothesis?minimal effect test?minimal effect test?conclusion can draw null-hypothesis significance test equivalence test\nperformed data, neither test statistically significant?conclusion can draw null-hypothesis significance test equivalence test\nperformed data, neither test statistically significant?designing equivalence tests desired statistical power, need \nlarger sample size, narrower equivalence range ?designing equivalence tests desired statistical power, need \nlarger sample size, narrower equivalence range ?incorrect say ‘effect’ equivalence test statistically significant?incorrect say ‘effect’ equivalence test statistically significant?Specify one way Bayesian ROPE procedure equivalence test similar, specify one way different.Specify one way Bayesian ROPE procedure equivalence test similar, specify one way different.two approaches specify smallest effect size interest?two approaches specify smallest effect size interest?idea behind ‘small telescopes’ approach equivalence testing?idea behind ‘small telescopes’ approach equivalence testing?","code":""},{"path":"sequential.html","id":"sequential","chapter":"10 Sequential Analysis","heading":"10 Sequential Analysis","text":"Repeatedly analyzing incoming data data collection progress many advantages. Researchers can stop data collection interim analysis can reject null hypothesis smallest effect size interest, even willing collect data needed, results show unexpected problem study (e.g., participants misunderstand instructions questions). One easily argue psychological researchers ethical obligation repeatedly analyze accumulating data, given continuing data collection whenever desired level confidence reached, whenever sufficiently clear expected effects present, waste time participants money provided taxpayers. addition ethical argument, designing studies make use sequential analyses can efficient data analyzed single time, maximum sample size researcher willing collect reached.Sequential analyses confused optional stopping, discussed chapter error control. optional stopping, researchers use unadjusted alpha level (e.g., 5%) repeatedly analyze data comes , can substantially inflate Type 1 error rate. critical difference sequential analysis Type 1 error rate controlled. lowering alpha level interim analysis, overall Type error rate can controlled, much like Bonferroni correction used prevent inflation Type 1 error rate multiple comparisons. Indeed, Bonferroni correction valid (conservative) approach control error rate sequential analyses (Wassmer & Brannath, 2016).sequential analysis researcher designs study able perform interim analyses, say 25%, 50%, 75% data collected. interim analysis test performed corrected alpha level, planned analyses desired Type 1 error rate maintained. Sequential analyses commonly used medical trials, quickly discovering effective treatment can matter life death. interim analysis, researchers decide new drug effective, turn may well want terminate trial give working drug patients control condition improve health, even save lives. example, safety efficacy Pfizer–BioNTech COVID-19 vaccine used experimental design planned analyze data 5 times, controlled overall Type 1 error rate lowering alpha level interim analysis.\nFigure 10.1: Screenshot planned interim analyses examining safety Efficacy BNT162b2 mRNA Covid-19 Vaccine.\nuse sequential analyses slowly becoming popular many scientific disciplines, sequential analysis techniques long history. early 1929, Dodge Romig realized analyzing data sequentially efficient (Dodge & Romig, 1929). Wald (1945), popularized idea sequential tests hypotheses 1945, performed work second world war. allowed publish findings war ended, explains historical note:substantial savings expected number observations effected sequential probability ratio test, simplicity test procedure practical applications, National Defense Research Committee considered developments sufficiently useful war effort make desirable keep results reach enemy, least certain period time. author , therefore, requested submit findings restricted report dated September, 1943.Sequential analyses well-established procedures, developed great detail last decades (Jennison & Turnbull, 2000; Proschan et al., 2006; Wassmer & Brannath, 2016). , explain basics control error rates group sequential analyses, perform -priori power analysis compare sequential designs less efficient fixed designs. discuss topics, useful clarify terminology. look (also called stage) means analyzing data collected specific point; , look 50, 100, 150 observations, analyze data collected point. 50 100 observations perform interim analysis, 150 observations perform final analysis, always stop. looks occur practice. analysis reveals statistically significant result look 1, data collection can terminated. can stop reject \\(H_0\\) (e.g., null hypothesis significance test), reject \\(H_1\\) (e.g., equivalence test). can also stop curtailment futility: either impossible, unlikely final analysis yield p < alpha. overall alpha level sequential design differs alpha level look. example, want overall Type error rate 5% two-sided test 3 looks, alpha level look 0.0221 (decide use correction alpha level proposed Pocock (1977)). chapter focus group sequential designs, data collected multiple groups, sequential approaches exist, explained chapter sample size justification.","code":""},{"path":"sequential.html","id":"choosing-alpha-levels-for-sequential-analyses.","chapter":"10 Sequential Analysis","heading":"10.1 Choosing alpha levels for sequential analyses.","text":"one analyze data multiple looks without correcting alpha level, Type 1 error rate inflate (Armitage et al., 1969). Armitage colleagues show, equally spaced looks, alpha level inflates 0.142 5 looks, 0.374 100 looks, 0.530 1000 looks. Looking data twice conceptually similar deciding result significant one two dependent variables shows statistically significant effect. However, important difference case sequential analyses multiple tests independent, dependent. test look 2 combines old data collected look 1 new data look 2. means Type 1 error rate inflates less quickly compared independent tests, see enables efficient flexible solutions controlling error rates.controlling Type 1 error rate sequential analyses, decision needs made spend alpha level across looks data. example, researcher plans study one interim look one final look data, boundary critical Z-values need set first look (n N observations) second look (N observations). two critical values, \\(c_1\\) \\(c_2\\) (first second analysis) need chosen overall probability (Pr) null hypothesis rejected – first analysis observed Z-score larger critical value first look, \\(Z_n\\) ≥ \\(c_1\\), (reject hypothesis first analysis, \\(Z_n\\) < \\(c_1\\), continue data collection) second analysis observed Z-score larger critical value second look, \\(Z_N\\) ≥ \\(c_2\\) – equals desired overall alpha level null hypothesis true. formal terms, directional test:\\[\nPr\\{Z_n \\geq c_1\\} + Pr\\{Z_n < c_1, Z_N \\geq c_2\\} = \\alpha\n\\]one interim analysis, additional critical values determined following rationale. combine multiple looks data multiple comparisons, correct alpha level twice, multiple comparisons, multiple looks. alpha level corrected, matter statistical test perform look, matters p-value compared corrected alpha level. corrections discussed valid design data normally distributed, group observations independent previous group.","code":""},{"path":"sequential.html","id":"the-pocock-correction","chapter":"10 Sequential Analysis","heading":"10.2 The Pocock correction","text":"first decision researchers need make want correct Type error rate across looks. Four common approaches Pocock correction, O'Brien-Fleming correction, Haybittle & Peto correction, Wang Tsiatis approach. Users also free specify preferred way spend alpha level across looks.Pocock correction simplest way correct alpha level multiple looks. Conceptually, similar Bonferroni correction. Pocock correction created alpha level identical look data, resulting constant critical values (expressed z values) \\(u_k = c\\) reject null hypothesis, \\(H_0\\), look \\(k\\). following code uses package rpact design study sequential analysis:output tells us designed study 2 looks (one interim, one final) using Pocock spending function. last line returns one-sided alpha levels. rpact package focuses Confirmatory Adaptive Clinical Trial Design Analysis. clinical trials, researchers mostly test directional predictions, thus, default setting perform one-sided test. clinical trials common use 0.025 significance level one-sided tests, many fields, 0.05 common default. can get two-sided alpha levels multiplying one-sided alpha levels two:can check output Wikipedia page Pocock correction indeed see 2 looks data alpha level look 0.0294. Pocock correction slightly efficient using Bonferroni correction (case alpha levels 0.025), dependency data (second look, data analyzed first look part analysis).rpact makes easy plot boundaries (based critical values) look. Looks plotted function 'Information Rate', percentage total data collected look. Figure 10.2 two equally spaced looks, 50% data collected (Information Rate 0.5) 100% data collected (Information Rate 1). see critical values (solid black lines) larger 1.96 use fixed design 5% alpha level, namely Z = 2.178 (black dashed line). Whenever observe test statistic extreme critical values first second look, can reject null hypothesis.\nFigure 10.2: Plot critical boundaries look 2 look design Pocock correction.\nanalysis can also performed rpact shiny app also allows users create plots simple menu options, download complete report analyses (e.g., preregistration document).\nFigure 10.3: Screenshot rpact Shiny app.\n","code":"\nlibrary(rpact)\ndesign <- getDesignGroupSequential(\n  kMax = 2,\n  typeOfDesign = \"P\",\n  sided = 2,\n  alpha = 0.05,\n  beta = 0.1\n)\ndesign## Design parameters and output of group sequential design:\n## \n## User defined parameters:\n##   Type of design                               : Pocock \n##   Maximum number of stages                     : 2 \n##   Stages                                       : 1, 2 \n##   Significance level                           : 0.0500 \n##   Type II error rate                           : 0.1000 \n##   Test                                         : two-sided \n## \n## Derived from user defined parameters:\n##   Information rates                            : 0.500, 1.000 \n## \n## Default parameters:\n##   Two-sided power                              : FALSE \n##   Tolerance                                    : 0.00000001 \n## \n## Output:\n##   Cumulative alpha spending                    : 0.02939, 0.05000 \n##   Critical values                              : 2.178, 2.178 \n##   Stage levels (one-sided)                     : 0.01469, 0.01469\ndesign$stageLevels * 2## [1] 0.02938579 0.02938579"},{"path":"sequential.html","id":"comparing-spending-functions","chapter":"10 Sequential Analysis","heading":"10.3 Comparing Spending Functions","text":"can vizualize corrections different types designs 3 looks (2 interim looks one final look) plot (see Figure 10.4). plot shows Pocock, O’Brien-Fleming, Haybittle-Peto, Wang-Tsiatis correction \\(\\Delta\\) = 0.25. see researchers can choose different approaches spend alpha level across looks. Researchers can choose spend alpha conservatively (keeping alpha last look), liberally (spending alpha earlier looks, increases probability stopping early many true effect sizes).\nFigure 10.4: Four different spending functions 3 looks: O'Brien-Fleming (), Pocock (P), Haybittle-Peto (HP), Wang-Tsiatis (WT).\ncan see O'Brien Fleming correction much conservative first look close uncorrected critical value 1.96 last look (black dashed line - two-sided tests critical values mirrored negative direction): 3.471, 2.454, 2.004. Pocock correction critical value look (2.289, 2.289, 2.289). Haybittle Peto correction critical value look last (3, 3, 1.975). Wang Tsiatis correction, critical values decrease look (2.741, 2.305, 2.083).conservative early looks sensible mainly want monitor results unexpected developments. Pocock correction useful substantial uncertainty whether effect present large effect size , gives higher probability stopping experiment early effects large. statistical power test depends alpha level, lowering alpha level final look means statistical power lower compared fixed design, achieve desired power, sample size study needs increased maintain statistical power last look. increase sample size can compensated stopping data collection early, case sequential design efficient fixed design. alpha last look O’Brien-Fleming Haybittle-Peto designs similar statistical power fixed design one look, required sample size also similar. Pocock correction requires larger increase maximum sample size achieve desired power compared fixed design.Corrected alpha levels can computed many digits, quickly reaches level precision meaningless real life. observed Type error rate tests lifetime noticeably different set alpha level 0.0194, 0.019, 0.02 (see concept ‘significant digits'. Even calculate use alpha thresholds many digits sequential tests, messiness research makes alpha levels false precision. Keep mind interpreting data.","code":""},{"path":"sequential.html","id":"alpha-spending-functions","chapter":"10 Sequential Analysis","heading":"10.4 Alpha spending functions","text":"approaches specify shape decision boundaries across looks discussed far important limitation (Proschan et al., 2006). require pre-specified number looks (e.g., 4), sample size interim looks need pre-specified well (e.g., 25%, 50%, 75%, 100% observations). logistically always feasible stop data collection exactly 25% planned total sample size. important contribution sequential testing literature made Lan DeMets (1983) introduced alpha spending approach correct alpha level. approach cumulative Type error rate spent across looks pre-specified function (alpha spending function) control overall significance level \\(\\alpha\\) end study.main benefit alpha spending functions error rates interim analyses can controlled, neither number timing looks needs specified advance. makes alpha spending approaches much flexible earlier approaches controlling Type 1 error group sequential designs. using alpha spending function important decision perform interim analysis based collected data, can still increase Type error rate. long assumption met, possible update alpha levels look study.\nFigure 10.5: Comparison Pocock (P) O'Brien-Fleming correction (), Pocock-like (asP) O'Brien-Fleming like (asOF) alpha spending functions, 5 looks.\n","code":""},{"path":"sequential.html","id":"updating-boundaries-during-a-study","chapter":"10 Sequential Analysis","heading":"10.5 Updating boundaries during a study","text":"Although alpha spending functions control Type error rate even deviations pre-planned number looks, timing, require recalculating boundaries used statistical test based amount information observed. Let us assume researcher designs study three equally spaced looks data (two interim looks, one final look), using Pocock-type alpha spending function, results analyzed two-sided t-test overall desired Type error rate 0.05, desired power 0.9 Cohen’s d 0.5. -priori power analysis (explain next section) shows achieve desired power sequential design plan look 65.4, 130.9, 196.3 observations condition. Since collect partial participants, round numbers , 2 independent groups, collect 66 observations look 1 (33 condition), 132 second look (66 condition) 198 third look (99 condition). code computes alpha levels look (stage) two-sided test:Now imagine due logistical issues, manage analyze data collected data 76 observations (38 condition) instead planned 66 observations. logistical issues common practice one main reasons alpha spending functions group sequential designs developed. first look data occur planned time collecting 33.3% total sample, 76/198 = 38.4% planned sample. can recalculate alpha level use look data, based current look, planned future looks. Instead using alpha levels 0.0226, 0.0217, 0.0217 three respective looks (calculated , note Pocock-like alpha spending function alpha levels almost, exactly, look, unlike Pocock correction identical look). can adjust information rates explicitly specifying using informationRates code . first look now occurs 76/198 planned sample. second look still planned occur 2/3 sample, final look planned maximum sample size.updated alpha levels 0.0253 current look, 0.0204 second look, 0.0216 final look. alpha level use first look therefore 0.0226 (originally planned) slightly higher 0.0253. second look now use slightly lower alpha 0.0204 instead 0.0217. differences small, fact formal method control alpha level provides flexibility look different times originally planned extremely useful.also possible correct alpha level final look data changes, example able collect intended sample size, due unforeseen circumstances collect data planned. nowadays increasingly common people preregister studies, publish using Registered Reports. Sometimes end slightly data planned, raises question analyze planned sample size, data. Analyzing collected data prevents wasting responses participants, uses information available, increases flexibility data analysis (researchers can now choose analyze either data planned sample, data collected). Alpha spending functions solve conundrum allowing researchers analyze data, updating alpha level used control overall alpha level.data collected planned, can longer use alpha spending function chosen (.e., Pocock spending function), instead provide user-defined alpha spending function updating timing alpha spending function reflect data collection actually occurred final look. Assuming second look earlier example occurred originally planned 2/3 data planned collect, last look occurred 206 participants instead 198, can compute updated alpha level last look. Given current total sample size, need recompute alpha levels earlier looks, now occurred 76/206 = 0.369, 132/206 = 0.641, last look 206/206 = 1.first second look occurred adjusted alpha levels computed first adjustment (alpha levels 0.0253 0.0204). already spent part total alpha first two looks. can look \"Cumulative alpha spent' results design specified , see much Type error rate spent far:see spent 0.0253 look 1, 0.0382 look 2. also know want spend remainder Type error rate last look, total 0.05.actual alpha spending function longer captured Pocock spending function collecting data planned, instead specify user defined spending function. can perform calculations using code specifying userAlphaSpending information, choosing asUser design:alpha levels looks past correspond alpha levels used, final alpha level (0.0208) gives alpha level use final analysis based sample size larger planned collect. difference alpha level used collected planned sample size really small (0.0216 vs. 0.0208), part miss planned sample size lot. small differences alpha levels really noticeable practice, useful formally correct solution deal collecting data planned, controlling Type 1 error rate. use sequential designs, can use corrections whenever overshoot sample size planned collect preregistration.","code":"\ndesign <- getDesignGroupSequential(kMax = 3, \n                                   typeOfDesign = \"asP\",\n                                   sided = 2, \n                                   alpha = 0.05, \n                                   beta = 0.1)\ndesign$stageLevels * 2## [1] 0.02264162 0.02173822 0.02167941\ndesign <- getDesignGroupSequential(\n  typeOfDesign = \"asP\", \n  informationRates = c(76/198, 2/3, 1), \n  alpha = 0.05, \n  sided = 2)\ndesign$stageLevels * 2## [1] 0.02532710 0.02043978 0.02164755\ndesign$alphaSpent## [1] 0.02532710 0.03816913 0.05000000\ndesign <- getDesignGroupSequential(\n  typeOfDesign = \"asUser\", \n  informationRates = c(72/206, 132/206, 1), \n  alpha = 0.05, \n  sided = 2, \n  userAlphaSpending = c(0.0253, 0.0382, 0.05)\n)\ndesign$stageLevels * 2## [1] 0.02530000 0.01987072 0.02075796"},{"path":"sequential.html","id":"sample-size-for-sequential-designs","chapter":"10 Sequential Analysis","heading":"10.6 Sample Size for Sequential Designs","text":"final look, sequential designs require somewhat participants fixed design, depending much alpha level look lowered due correction multiple comparisons. said, due early stopping, sequential designs average require less participants. first examine many participants need fixed design, analyze data . alpha level 0.05, Type 2 (beta) error 0.1 - words, desired power 90%. perform one test, assuming normal distribution critical Z-score 1.96, alpha level 5%.see need 85 participants group, (86, since sample size actually 85.03 required number observations rounded , need 172 participants total. power analysis software, G*Power, yield required sample size. can now examine design 2 looks Pocock-like alpha spending function 2 sided test alpha 0.05. look 2 times, expect true effect d = 0.5 (enter specifying alternative 0.5, stDev 1).sample size per condition first look 47.24, second look 94.47, means now collecting 190 instead 172 participants. consequence lowering alpha level look (0.05 0.028). compensate lower alpha level, need increase sample size study achieve power.However, maximum sample size expected sample size design, possibility can stop data collection earlier look sequential design. long run, d = 0.5, use Pocock-like alpha spending function, ignoring upward rounding can collect complete number observations, sometimes collect 96 participants stop first look, remaining time continue 190 participants. see rows 'Reject per stage' data collection expected stop first look 0.6 studies observed significant result. remainder time (1 - 0.6) = 0.4.means , assuming true effect d = 0.5, expected sample size average probability stopping look, multiplied number observations collect look, 0.6 * 96 + 0.3 * 190 = 133.39. rpact package returns 132.06 \"Expected number subjects \\(H_1\\)\" - small difference due fact rpact round number observations , although ). , assuming true effect d = 0.5, single study might need collect slightly data fixed design (collect 172), average need collect less observations sequential design.power curve, true effect size unknown, useful plot power across range possible effect sizes, can explore expected sample size, long run, use sequential design, different true effect sizes.\nFigure 10.6: Power curve sequential design 2 looks.\nblue line Figure 10.6 indicates expected number observations need collect. surprisingly, true effect size 0, almost always continue data collection end. stop observe Type 1 error, rare, thus expected number observations close maximum sample size willing collect. side graph see scenario true effect size d = 1. large effect size, high power first look, almost always able stop first look. red line indicates power final look, green line indicates probability stopping early.Pocock correction leads substantially lower alpha level last look, requires increase sample size compensate. saw , O'Brien-Fleming spending function require severe reduction alpha level last look. power analysis shows, 2 looks, design need increase sample size practice.design meets desired power collect 172 participants - exactly many look data . basically get free look data, expected number participants (assuming d = 0.5) dropping 149.1. Increasing number looks 4 comes small required increase number participants maintain statistical power, decreases expected sample size. Especially conservative -priori power analysis, performing -priori power analysis smallest effect size interest, decent probability true effect size larger, using sequential analysis attractive option.","code":"\ndesign <- getDesignGroupSequential(\n  kMax = 1,\n  typeOfDesign = \"P\",\n  sided = 2,\n  alpha = 0.05,\n  beta = 0.1\n)\n\npower_res <- getSampleSizeMeans(\n  design = design,\n  groups = 2,\n  alternative = 0.5, \n  stDev = 1, \n  allocationRatioPlanned = 1,\n  normalApproximation = FALSE)\n\npower_res## Design plan parameters and output for means:\n## \n## Design parameters:\n##   Critical values                              : 1.96 \n##   Two-sided power                              : FALSE \n##   Significance level                           : 0.0500 \n##   Type II error rate                           : 0.1000 \n##   Test                                         : two-sided \n## \n## User defined parameters:\n##   Alternatives                                 : 0.5 \n## \n## Default parameters:\n##   Mean ratio                                   : FALSE \n##   Theta H0                                     : 0 \n##   Normal approximation                         : FALSE \n##   Standard deviation                           : 1 \n##   Treatment groups                             : 2 \n##   Planned allocation ratio                     : 1 \n## \n## Sample size and output:\n##   Number of subjects fixed                     : 170.1 \n##   Number of subjects fixed (1)                 : 85 \n##   Number of subjects fixed (2)                 : 85 \n##   Lower critical values (treatment effect scale) : -0.303 \n##   Upper critical values (treatment effect scale) : 0.303 \n##   Local two-sided significance levels          : 0.0500 \n## \n## Legend:\n##   (i): values of treatment arm i\nseq_design <- getDesignGroupSequential(\n  kMax = 2,\n  typeOfDesign = \"asP\",\n  sided = 2,\n  alpha = 0.05,\n  beta = 0.1\n  )\n\n# Compute the sample size we need\npower_res_seq <- getSampleSizeMeans(\n  design = seq_design,\n  groups = 2,\n  alternative = 0.5, \n  stDev = 1, \n  allocationRatioPlanned = 1,\n  normalApproximation = FALSE)\n\npower_res_seq## Design plan parameters and output for means:\n## \n## Design parameters:\n##   Information rates                            : 0.500, 1.000 \n##   Critical values                              : 2.157, 2.201 \n##   Futility bounds (non-binding)                : -Inf \n##   Cumulative alpha spending                    : 0.03101, 0.05000 \n##   Local one-sided significance levels          : 0.01550, 0.01387 \n##   Two-sided power                              : FALSE \n##   Significance level                           : 0.0500 \n##   Type II error rate                           : 0.1000 \n##   Test                                         : two-sided \n## \n## User defined parameters:\n##   Alternatives                                 : 0.5 \n## \n## Default parameters:\n##   Mean ratio                                   : FALSE \n##   Theta H0                                     : 0 \n##   Normal approximation                         : FALSE \n##   Standard deviation                           : 1 \n##   Treatment groups                             : 2 \n##   Planned allocation ratio                     : 1 \n## \n## Sample size and output:\n##   Reject per stage [1]                         : 0.6022 \n##   Reject per stage [2]                         : 0.2978 \n##   Early stop                                   : 0.6022 \n##   Maximum number of subjects                   : 188.9 \n##   Maximum number of subjects (1)               : 94.5 \n##   Maximum number of subjects (2)               : 94.5 \n##   Number of subjects [1]                       : 94.5 \n##   Number of subjects [2]                       : 188.9 \n##   Expected number of subjects under H0         : 186 \n##   Expected number of subjects under H0/H1      : 172.7 \n##   Expected number of subjects under H1         : 132.1 \n##   Lower critical values (treatment effect scale) [1] : -0.451 \n##   Lower critical values (treatment effect scale) [2] : -0.323 \n##   Upper critical values (treatment effect scale) [1] : 0.451 \n##   Upper critical values (treatment effect scale) [2] : 0.323 \n##   Local two-sided significance levels [1]      : 0.03101 \n##   Local two-sided significance levels [2]      : 0.02774 \n## \n## Legend:\n##   (i): values of treatment arm i\n##   [k]: values at stage k\n# Use getPowerMeans and set max N to 188 based on analysis above\nsample_res <- getPowerMeans(\n  design = seq_design,\n  groups = 2,\n  alternative = seq(0, 1, 0.01), \n  stDev = 1, \n  allocationRatioPlanned = 1,\n  maxNumberOfSubjects = 190, \n  normalApproximation = FALSE)\n\nplot(sample_res, type = 6)\nseq_design <- getDesignGroupSequential(\n  kMax = 2,\n  typeOfDesign = \"asOF\",\n  sided = 2,\n  alpha = 0.05,\n  beta = 0.1\n  )\n\n# Compute the sample size we need\npower_res_seq <- getSampleSizeMeans(\n  design = seq_design,\n  groups = 2,\n  alternative = 0.5, \n  stDev = 1, \n  allocationRatioPlanned = 1,\n  normalApproximation = FALSE)\n\nsummary(power_res_seq)## Sample size calculation for a continuous endpoint\n## \n## Sequential analysis with a maximum of 2 looks (group sequential design), overall \n## significance level 5% (two-sided).\n## The sample size was calculated for a two-sample t-test, H0: mu(1) - mu(2) = 0, \n## H1: effect = 0.5, standard deviation = 1, power 90%.\n## \n## Stage                                         1      2 \n## Information rate                            50%   100% \n## Efficacy boundary (z-value scale)         2.963  1.969 \n## Overall power                            0.2525 0.9000 \n## Expected number of subjects               149.1 \n## Number of subjects                         85.3  170.6 \n## Cumulative alpha spent                   0.0031 0.0500 \n## Two-sided local significance level       0.0031 0.0490 \n## Lower efficacy boundary (t)              -0.661 -0.304 \n## Upper efficacy boundary (t)               0.661  0.304 \n## Exit probability for efficacy (under H0) 0.0031 \n## Exit probability for efficacy (under H1) 0.2525 \n## \n## Legend:\n##   (t): treatment effect scale"},{"path":"sequential.html","id":"stopping-for-futility","chapter":"10 Sequential Analysis","heading":"10.7 Stopping for futility","text":"far, sequential designs discussed stop interim analysis can reject \\(H_0\\). well-designed study also takes account possibility effect, discussed chapter equivalence testing. sequential analysis literature, stopping reject presence smallest effect size interest called stopping futility. extreme case, impossible interim analysis final analysis yield statistically significant result. illustrate hypothetical scenario, imagine collecting 182 192 observations, observed mean difference two independent conditions 0.1, study designed idea smallest effect deemed worthwhile mean difference 0.5. primary dependent variable measured 7 point Likert scale, might even every remaining 5 participants control condition answers 1, every remaining participants experimental condition answers 7, effect size 192 observations yield p < \\(\\alpha\\). goal study detect whether effect least mean difference 0.5, point researcher knows goal reached. Stopping study interim analysis final result yield significant effect called non-stochastic curtailment.less extreme common situations, might still possible study observe significant effect, probability might small. probability finding significant result, given data observed interim analysis, called conditional power. Performing conditional power analysis effect size originally expected might optimistic, also undesirable use observed effect size, typically quite uncertainty. One proposal update expected effect size based observed data. Bayesian updating procedure used, called predictive power (D. J. Spiegelhalter et al., 1986). possible use adaptive designs allow researchers increase final number observations based interim analysis without inflating Type 1 error rate (see Wassmer & Brannath (2016)).Alternatively, observed effect size smaller expected, one might want stop futility. illustration simple stopping rule futility, imagine researcher stop futility whenever observed effect size either zero, opposite direction predicted. Figure 10.7 red line indicates critical values declare significant effect. essence, means observed z-score interim test either 0 negative, data collection terminated. can specified adding futilityBounds = c(0, 0) specification sequential design. One can choose advance stop whenever criteria stop futility met, (.e., binding futility rule), typically recommended allow possibility continue data collection (.e., non-binding futility rule, specified setting bindingFutility = FALSE).Figure 10.7 see sequential design data collection stopped reject \\(H_0\\) observed z-score larger values indicated red line, computed based Pocock-like alpha spending function (Figure 10.4. addition, data collection stop interim analysis z-score lower equal 0 observed, indicated blue line. last look, red blue lines meet, either reject \\(H_0\\) critical value, fail reject \\(H_0\\).\nFigure 10.7: Pocock-type boundaries 3 looks stop rejecting \\(H_0\\) (red line) stop futility (blue line) observed effect opposite direction.\nManually specifying futility bounds ideal, risk stopping data collection fail reject \\(H_0\\), high probability Type 2 error. better set futility bounds directly controlling Type 2 error across looks data. Just willing distribute Type error rate across interim analyses, can distribute Type II error rate across looks, decide stop futility fail reject effect size interest desired Type 2 error rate.study designed null hypothesis significance test 90% power detect effect d = 0.5, 10% time \\(H_0\\) rejected . 10% cases make Type 2 error, conclusion effect 0.5 present, reality, effect d = 0.5 (larger). equivalence smallest effect size interest d = 0.5, conclusion effect 0.5 larger present, reality effect d = 0.5 (larger), called Type 1 error: incorrectly conclude effect practically equivalent zero. Therefore, Type 2 error NHST \\(H_0\\) d = 0 \\(H_1\\) = d = 0.5 Type 1 error equivalence test \\(H_0\\) d = 0.5 \\(H_1\\) d = 0 (Jennison & Turnbull, 2000). Controlling Type 2 error sequential design can therefore seen controlling Type 1 error equivalence test effect size study powered . design study 5% Type 1 error rate equally low Type 2 error rate (e.g., 5%, 95% power), study informative test presence absence effect interest.true effect size (close ) 0, sequential designs stop futility efficient designs stop futility. Adding futility bounds based beta-spending functions reduces power, needs compensated increasing sample size, can compensated fact studies can stop earlier futility, can make designs efficient. specifying smallest effect size interest possible, researchers might want incorporate stopping futility study design. control Type 2 error rate across looks, beta-spending function needs chosen, Pocock type beta spending function, O'Brien-Fleming type beta spending function, user defined beta spending function (beta-spending function need alpha-spending function). rpact beta-spending functions can chosen directional (one-sided) tests.\nFigure 10.8: Pocock-type boundaries 3 looks stop rejecting \\(H_0\\) (red line) stop futility (blue line) based Pocock-type beta-spending function.\nbeta-spending function, expected number subjects \\(H_1\\) increase, alternative hypothesis true, designing study able stop futility comes cost. However, possible \\(H_0\\) true, , stopping futility reduces expected sample size. Figure 10.9 can see probability stopping (green line) now also high true effect size 0, now stop futility, , expected sample size (blue line) lower compared 10.6. important design studies high informational value reject presence meaningful effect final analysis, whether stopping futility early option want build study choice requires considering probability null hypothesis true (perhaps small) increase sample size.\nFigure 10.9: Power curve sequential design 2 looks stopping futility.\n","code":"\ndesign <- getDesignGroupSequential(\n  sided = 1,\n  alpha = 0.05,\n  beta = 0.1,\n  typeOfDesign = \"asP\",\n  futilityBounds = c(0, 0),\n  bindingFutility = FALSE\n)"},{"path":"sequential.html","id":"reporting-the-results-of-a-sequential-analysis","chapter":"10 Sequential Analysis","heading":"10.8 Reporting the results of a sequential analysis","text":"Group sequential designs developed efficiently test hypotheses using Neyman-Pearson approach statistical inference, goal decide act, controlling error rates long run. Group sequential designs goal quantify strength evidence, provide accurate estimates effect size (Proschan et al., 2006). Nevertheless, reached conclusion whether hypothesis can rejected , researchers often want also interpret effect size estimate reporting results.challenge interpreting observed effect size sequential designs whenever study stopped early \\(H_0\\) rejected, risk data analysis stopped , due random variation, large effect size observed time interim analysis. means observed effect size interim analyses -estimates true effect size. Schönbrodt et al. (2017) show, meta-analysis studies used sequential designs yield accurate effect size, studies stop early smaller sample sizes, weighted less, compensated smaller effect size estimates sequential studies reach final look, weighted larger sample size. However, researchers might want interpret effect sizes single studies meta-analysis can performed, case, reporting adjusted effect size estimate can useful. Although sequential analysis software allows one compute adjusted effect size estimates certain statistical tests, recommend reporting adjusted effect size possible, always also report unadjusted effect size estimate future meta-analyses.similar issue play reporting p values confidence intervals. sequential design used, distribution p value account sequential nature design longer uniform \\(H_0\\) true. p value probability observing result least extreme result observed, given \\(H_0\\) true. longer straightforward determine 'least extreme' means sequential design (T. D. Cook, 2002). widely recommended procedure determine \"least extreme\" means order outcomes series sequential analyses terms look study stopped, earlier stopping extreme later stopping, studies higher z values extreme, different studies stopped time (Proschan et al., 2006). referred stagewise ordering, treats rejections earlier looks stronger evidence \\(H_0\\) rejections later study (Wassmer & Brannath, 2016). Given direct relationship p value confidence interval, confidence intervals sequential designs also developed.Reporting adjusted p values confidence intervals, however, might criticized. sequential design, correct interpretation Neyman-Pearson framework conclude \\(H_0\\) rejected, alternative hypothesis rejected, results inconclusive. reason adjusted p values reported sequential designs allow readers interpret measure evidence. Dupont (1983) provides good arguments doubt adjusted p values provide valid measure strength evidence. Furthermore, strict interpretation Neyman-Pearson approach statistical inferences also provides argument interpreting p values measures evidence (Lakens, 2022a). Therefore, recommended, researchers interested communicating evidence data \\(H_0\\) relative alternative hypothesis, report likelihoods Bayes factors, can always reported interpreted data collection completed. Reporting unadjusted p-value relation alpha level communicates basis reject hypotheses, although might important researchers performing meta-analysis based p-values (e.g., p-curve z-curve analysis, explained chapter bias detection) sequential p-values. Adjusted confidence intervals useful tools evaluate observed effect estimate relative variability interim final look data. Note adjusted parameter estimates available statistical software commonly used designs pharmaceutical trials, comparisons mean differences groups, survuval analysis., see sequential design started , 2 looks Pocock-type alpha spending function. completing study planned sample size 95 participants per condition (collect 48 participants look 1, remaining 47 look 2), can now enter observed data using function getDataset. means standard deviations entered stage, second look, data second 95 participants condition used compute means (1.51 1.01) standard deviations (1.03 0.96).Imagine performed study planned 2 equally spaced looks data, perform two-sided test alpha 0.05, use Pocock type alpha spending function, observe mean differences two conditions last look. Based Pocock-like alpha spending function two equally spaced looks alpha level two-sided t-test 0.03101, 0.02774. can thus reject \\(H_0\\) look 2. also like report effect size, adjusted p values confidence intervals.results show action look 1 continue data collection, reject \\(H_0\\) second look. unadjusted mean difference provided row \"Overall effect size\" final look 0.293. adjusted mean difference provided row \"Median unbiased estimate\" lower, adjusted confidence interval row \"Final confidence interval\", giving result 0.281, 95% CI [-0.02, 0.573].unadjusted p values one-sided test reported row \"Overall p-value\". actual p values two-sided test twice large, 0.6668, 0.0477. adjusted p-value final look provided row \"Final p-value\" 0.06662.","code":"\nseq_design <- getDesignGroupSequential(\n  kMax = 2,\n  typeOfDesign = \"asP\",\n  sided = 2,\n  alpha = 0.05,\n  beta = 0.1\n)\n\ndataMeans <- getDataset(\n  n1 = c(48, 47), \n  n2 = c(48, 47), \n  means1 = c(1.12, 1.51), # for directional test, means 1 > means 2\n  means2 = c(1.03, 1.01),\n  stDevs1 = c(0.98, 1.03), \n  stDevs2 = c(1.06, 0.96)\n  )\n\nres <- getAnalysisResults(\n  seq_design, \n  equalVariances = TRUE,\n  dataInput = dataMeans\n  )\n\nres## [PROGRESS] Stage results calculated [0.0482 secs] \n## [PROGRESS] Conditional power calculated [0.0438 secs] \n## [PROGRESS] Conditional rejection probabilities (CRP) calculated [0.0017 secs] \n## [PROGRESS] Repeated confidence interval of stage 1 calculated [0.7355 secs] \n## [PROGRESS] Repeated confidence interval of stage 2 calculated [0.6394 secs] \n## [PROGRESS] Repeated confidence interval calculated [1.38 secs] \n## [PROGRESS] Repeated p-values of stage 1 calculated [0.2179 secs] \n## [PROGRESS] Repeated p-values of stage 2 calculated [0.3489 secs] \n## [PROGRESS] Repeated p-values calculated [0.568 secs] \n## [PROGRESS] Final p-value calculated [0.0012 secs] \n## [PROGRESS] Final confidence interval calculated [0.0603 secs]## Analysis results (means of 2 groups, group sequential design):\n## \n## Design parameters:\n##   Information rates                            : 0.500, 1.000 \n##   Critical values                              : 2.157, 2.201 \n##   Futility bounds (non-binding)                : -Inf \n##   Cumulative alpha spending                    : 0.03101, 0.05000 \n##   Local one-sided significance levels          : 0.01550, 0.01387 \n##   Significance level                           : 0.0500 \n##   Test                                         : two-sided \n## \n## User defined parameters: not available\n## \n## Default parameters:\n##   Normal approximation                         : FALSE \n##   Direction upper                              : TRUE \n##   Theta H0                                     : 0 \n##   Equal variances                              : TRUE \n## \n## Stage results:\n##   Cumulative effect sizes                      : 0.0900, 0.2928 \n##   Cumulative (pooled) standard deviations      : 1.021, 1.013 \n##   Stage-wise test statistics                   : 0.432, 2.435 \n##   Stage-wise p-values                          : 0.333390, 0.008421 \n##   Overall test statistics                      : 0.432, 1.993 \n##   Overall p-values                             : 0.33339, 0.02384 \n## \n## Analysis results:\n##   Assumed standard deviation                   : 1.013 \n##   Actions                                      : continue, accept \n##   Conditional rejection probability            : 0.007317, NA \n##   Conditional power                            : NA, NA \n##   Repeated confidence intervals (lower)        : -0.36630, -0.03306 \n##   Repeated confidence intervals (upper)        : 0.5463, 0.6187 \n##   Repeated p-values                            : >0.5, 0.08195 \n##   Final stage                                  : 2 \n##   Final p-value                                : NA, 0.06662 \n##   Final CIs (lower)                            : NA, -0.02007 \n##   Final CIs (upper)                            : NA, 0.5734 \n##   Median unbiased estimate                     : NA, 0.2814"},{"path":"sequential.html","id":"test-yourself-8","chapter":"10 Sequential Analysis","heading":"10.9 Test Yourself","text":"Q1: Sequential analyses can increase efficiency studies perform. statement true sequential design researchers stop \\(H_0\\) can rejected (specify rule stop futility)?Sequential analyses reduce sample size every study perform.Sequential analyses average reduce sample size studies perform.Sequential analyses average reduce sample size studies perform, long true effect (rule stop futility specified).Sequential analyses average require sample size fixed designs, offer flexibility.Q2: difference sequential analysis optional stopping?difference sequential analysis transparently reporting, optional stopping typically disclosed paper.sequential analysis Type 1 error rate controlled, optional stopping Type 1 error rate inflated.optional stopping data collection terminated significant result observed, sequential analysis data collection can also stop absence meaningful effect established.sequential analysis possible design study analyze data every participant, can optional stopping.Q3: defining feature Pocock correction?uses conservative alpha level early looks, alpha level last look close unadjusted alpha level fixed design.uses alpha level look (almost alpha level look, using Pocock-like alpha spending function).uses critical value 3 interim analysis, spends remaining Type 1 error rate last look.parameter can chosen Type 1 error rate spent conservatively liberally early interim analyses.Q4: benefit O’Brien-Fleming correction alpha level last look close alpha level. benefit?means sample size based -priori power analysis (depends alpha level) close sample size fixed design, allowing additional looks data.means Type 1 error rate inflated little bit, compared fixed design.means Type 1 error rate bit conservative, compared fixed design.means sample size based -priori power analysis (depends alpha level) always identical sample size fixed design, allowing additional looks data.Q5: researcher uses sequential design study 5 looks data, desired overall alpha level 0.05 two-sided test, chooses Pocock correction. continuing data collect third look, researcher observes p-value 0.011. statement true? Note: remember rpact returns one-sided alpha levels. can use following code replacing 0 specifying typeOfDesign:researcher can reject null hypothesis can terminate data collection.researcher fails reject null hypothesis needs continue data collection.Q6: researcher uses sequential design study 5 looks data, desired overall alpha level 0.05, chooses O’Brien-Fleming correction. continuing data collect third look, researcher observes p-value 0.011. statement true (can use code Q5)?researcher can reject null hypothesis can terminate data collection.researcher fails reject null hypothesis needs continue data collection.Q7: design Q5 (using Pocock correction), sample size required achieve 80% power (default – can change default specifying different value beta = 0.2 getDesignGroupSequential function) effect size d = 0.5 (equals mean difference 0.5 standard deviation 1). can use code .64 (32 independent group).128 (64 independent group)154 (77 independent group)158 (79 independent group)Q8: design Q5, sample size required achieve 80% power effect size d = 0.5 fixed design one look? First update design (changing kMax 1) re-run function getSampleSizeMeans.64 (32 independent group).128 (64 independent group)154 (77 independent group)158 (79 independent group)see sample size increases quite bit choice Pocock correction, number looks (5, lead low alpha level final look). ratio maximum sample size sequential design sample size fixed design known inflation factor, independent effect size. Although -priori power analyses programmed types tests, inflation factor can used compute increased number observations required relative fixed design test. Researchers can perform -priori power analysis fixed design tool normally use, multiply total number observations inflation factor determine required sample size sequential design. inflation factor can retrieved using getDesignCharacteristics function.Q9: First, re-run code create sequential design 5 looks data used Q5. , run code , find inflation factor. inflation factor, required increase sample size sequential design 5 looks using Pocock correction, compared fixed design? Note rpact round number observations per group whole numbers calculating inflation factor.inflation factor 1The inflation factor 1.0284The inflation factor 1.2286The inflation factor 1.2536Q10: see inflation factor quite large, certain probability collect observations using fixed design. Re-run code Q7 (Pocock design 5 looks). see average, true effect 0.5, efficient fixed design. expected number subjects \\(H_1\\), provided rpact?101.9104.3125.3152.8We see sequential design average efficient fixed design, decision trade-specific sequential design used, whether possible benefit worth risk collecting additional data, must made case--case basis.Q11: First, re-run code create sequential design 5 looks data used Q6 (using O’Brien-Fleming correction). , run code , find inflation factor design. inflation factor?inflation factor 1The inflation factor 1.0284The inflation factor 1.2286The inflation factor 1.2536Q12: also possible stop futility (reject presence specific effect interest). Researchers decide binding non-binding beta-spending functions, need decide binding non-binding alpha spending functions. researcher observed statistically significant result interim analysis, decides stop data collection, continue data collection (example get precise effect size estimate) consequences?Type 1 error rate inflate, Type 2 error rate inflate.Type 1 error rate inflate, Type 2 error rate inflate.Type 1 error rate inflate, Type 2 error rate inflate.Type 1 error rate inflate, Type 2 error rate inflate.Q13: plot see t-score boundaries sequential design stop reject \\(H_0\\) (red line) reject \\(H_1\\) (blue line). second interim look, perform test, observe t-value 2. decision make?\nFigure 10.10: Example O'Brien-Fleming-type boundaries 3 looks stop rejecting \\(H_0\\) (red line) stop futility (blue line) 5% Type 1 Type 2 error.\ncan reject \\(H_0\\) stop data collection.can reject \\(H_1\\) stop data collection.reject \\(H_0\\) \\(H_1\\) stop data collection.fail reject \\(H_0\\) \\(H_1\\) continue data collection.","code":"\ndesign <- rpact::getDesignGroupSequential(\n  kMax = 0,\n  typeOfDesign = \"\",\n  sided = 0,\n  alpha = 0.0\n)\ndesign\npower_res <- rpact::getSampleSizeMeans(\n  design = design,\n  groups = 2,\n  alternative = 0.5, \n  stDev = 1, \n  allocationRatioPlanned = 1,\n  normalApproximation = FALSE)\n\npower_res\nrpact::getDesignCharacteristics(design)"},{"path":"sequential.html","id":"open-questions-8","chapter":"10 Sequential Analysis","heading":"10.9.1 Open Questions","text":"difference sequential analysis optional stopping?difference sequential analysis optional stopping?possible benefit using sequential design fixed design?possible benefit using sequential design fixed design?mean stop data collection futility?mean stop data collection futility?difference philosophy alpha spent across looks Pocock O’Brien-Fleming approaches?difference philosophy alpha spent across looks Pocock O’Brien-Fleming approaches?benefit fact alpha level final look using O’Brien-Fleming correction close uncorrected alpha level?benefit fact alpha level final look using O’Brien-Fleming correction close uncorrected alpha level?difference Pocock O’Brien-Fleming correction, corresponding Pocock O’Brien-Fleming alpha spending functions developed Lan DeMets?difference Pocock O’Brien-Fleming correction, corresponding Pocock O’Brien-Fleming alpha spending functions developed Lan DeMets?can even though maximum sample size sequential design slightly larger sample size fixed design, sequential designs can still efficient?can even though maximum sample size sequential design slightly larger sample size fixed design, sequential designs can still efficient?incorporating stopping rule futility increase efficiency sequential design?incorporating stopping rule futility increase efficiency sequential design?average, effect stopping early sequential design effect size estimate? argument correct effect size estimate reporting ?average, effect stopping early sequential design effect size estimate? argument correct effect size estimate reporting ?","code":""},{"path":"meta.html","id":"meta","chapter":"11 Meta-analysis","heading":"11 Meta-analysis","text":"Every single study just data-point future meta-analysis. draw small samples population, mean standard deviation sample can differ considerably mean standard deviation population. great variability small samples. Parameter estimates small samples imprecise, therefore 95% confidence intervals around effect sizes wide. Indeed, led Cohen (1994) write “suspect main reason confidence intervals reported embarrassingly large!” want precise estimate parameter interest, mean difference correlation population, need either run extremely large single studies, alternatively, combine data several studies performing meta-analysis. common approach combine studies perform meta-analysis effect size estimates.can perform meta-analysis set studies single article plan publish (often called internal meta-analysis), can search literature multiple studies reported many different articles possible, perform meta-analysis studies others published. excellent introduction meta-analyses provided book Borenstein (2009). commercial software can use perform meta-analyses, highly recommend using software. Almost commercial software packages lack transparency, allow share analysis code data researchers. chapter, using R perform meta-analysis effect sizes, using metafor package Viechtbauer (2010). important benefit using metafor meta-analysis can made completely reproducible. plan perform narrative review, relatively little additional effort also code effect sizes sample size, perform effect size meta-analysis, code statistical tests p-values, perform p-curve z-curve analysis (discussed next chapter bias detection).","code":""},{"path":"meta.html","id":"random-variation","chapter":"11 Meta-analysis","heading":"11.1 Random Variation","text":"People find difficult think random variation. mind strongly geared towards recognizing patterns randomness. section, goal learn random variation looks like, number observations collected determines amount variation.Intelligence tests designed mean Intelligence Quotient entire population adults 100, standard deviation 15. true every sample draw population. Let’s get feel IQ scores sample look like. IQ scores people sample ?start manually calculating mean standard deviation random sample 10 individuals. IQ scores : 91.15, 86.52, 75.64, 115.72, 95.83, 105.44, 87.10, 100.81, 82.63, 106.22. sum 10 scores divide 10, get mean sample: 94.71. can also calculate standard deviation sample. First, subtract overall mean (94.71) individual IQ score. , square differences sum squared differences (giving 1374.79). divide sum squared difference sample size minus 1 (10-1=9), finally take square root value, gives standard deviation: 12.36. Copy code , remove set.seed(3190) line (makes code reproducible creates data plot time) run randomly simulate 10 IQ scores plot .\nFigure 11.1: Simulation 10 random datapoints mean = 100 sd = 15 population.\nplot provides one example randomly simulated dataset 10 points drawn normal distribution mean 100 standard deviation 15. grey bars indicate frequency IQ score observed. red dotted line illustrates normal distribution based mean sd population. observed mean (97; thin vertical dashed line), well observed standard deviation (14), differ true population values. simulate 4 additional datasets, see mean standard deviation vary.Imagine yet know mean IQ population (M = 100), standard deviation (SD = 15), access one dataset. estimate might rather far . type variation expected small samples 10 participants, given true standard deviation. variability mean determined standard deviation measurement. real life, standard deviation can reduced example using multiple reliable measurements (IQ test just one question, many different questions). can also make sure sample mean closer population mean increasing sample size.new simulated sample 100 participants plotted . slowly seeing known normal distribution (frequency scores start resemble red dotted line illustrating normal distribution population). well-known\nbell shaped curve represents distribution many variables scientific research (although types distributions quite common well). mean standard deviation much closer true mean standard deviation, true simulated samples set n <- 100 code run additional simulations.\nFigure 11.2: 100 random datapoints mean = 100 sd = 15 population.\nsimulate really large sample 1000 observations, see benefits collecting large sample size terms accuracy measurement. every simulated study 1000 people yield true mean standard deviation, happen quite often. note although distribution close normal distribution, even 1000 people perfect.\nFigure 11.3: 1000 random datapoints mean = 100 sd = 15 population.\nfar, simulated single group observations, also informative examine variation observe compare means two independent groups. Assume new IQ training program increase people's IQ score 6 points. People condition 1 control condition – get IQ training. People condition 2 get IQ training. Let’s simulate 10 people group, assuming mean IQ control condition 100 experimental group 106 (SD still 15 group).\nFigure 11.4: Simulation 10 observations two independent groups.\ntwo groups differ close true means, consequence, difference groups varies well. Note difference main variable statistical analyses comparing two groups example t-test. specific simulation, got quite extreme results, score 96 (population mean 100) score 111 (population mean 106). sample, due random variation, calculate effect size estimate quite bit larger true effect size. simulate 4 additional datasets see variation.\nFigure 11.5: Four simulated samples independent groups.\nsee quite variation, point one simulation sample means opposite direction population means. , increasing sample size mean , long run, sample means get closer population means, accurately estimating difference conditions. 250 observations group, randomly simulated set observations two groups might look like Figure 11.6. Note difference might look impressive. However, difference pass significance test (independent t-test) low alpha level.\nFigure 11.6: Simulated sample 250 independent observations.\nvariation estimate mean decreases sample size increases. larger sample size, precise estimate mean becomes. standard deviation sample (\\(\\sigma_x\\)) single IQ scores 15, irrespective sample size, larger sample size, accurately can measure true standard deviation. standard deviation sampling distribution sample mean (\\(\\sigma_{\\overline{x}}\\)) decreases, sample size increases, referred standard error (SE). estimated standard deviation sample mean, standard error, calculated based observed standard deviation sample (\\(\\sigma_x\\)) :\\[SE = \\sigma_{\\overline{x}} = \\frac{\\sigma_x}{\\sqrt{n}}\\]\nBased formula, assuming observed standard deviation sample 15, standard error mean 4.74 sample size 10, 0.95 sample size 250. estimates lower standard error precise, effect size estimates meta-analysis weighed based standard error, precise estimates getting weight.far seen random variation means, correlations show similar variation function sample size. continue example measuring IQ scores, now search fraternal (identical) twins, measure IQ. Estimates literature suggest true correlation IQ scores fraternal twins around r = 0.55. find 30 fraternal twins, measure IQ scores, plot relation IQ individuals. simulation, assume twins mean IQ 100 standard deviation 15.correlation calculated based IQ scores one fraternal twin (x) IQ scores fraternal twin (y) pair twins, total number pairs (N). numerator formula, number pairs multiplied sum product x y, value sum x multiplied sum y subtracted. denominator, square root taken number pairs multiplied sum x squared, sum x, squared, subtracted, multiplied calculation now y.\\[r=\\frac{n \\Sigma x y-(\\Sigma x )(\\Sigma y)}{\\sqrt{[n \\Sigma x^{2}-(\\Sigma x)^{2}][n \\Sigma y^{2}-(\\Sigma y)^{2}]}}\\]\nrandomly simulate observations 30 twins, get following result.\nFigure 11.7: Correlation based 30 pairs.\nx-axis, see IQ score one twin, y-axis see IQ score second twin, pair. black dotted diagonal line illustrates true correlation (0.55), yellow line shows observed correlation (case, r = 0.43). slope yellow line determined observed correlation, position line influenced mean IQ scores groups (simulation, mean y-axis 105, somewhat 100, mean x-axis 102, also slightly 100. blue area 95% confidence interval around observed correlation. saw chapter confidence intervals, 95% time (long run) blue area contain true correlation (dotted black line). examples based means, increasing sample size 300 narrows confidence interval considerably, mean time correlation sample much closer correlation population. sample size increases, estimate correlation becomes precise, following formula standard error correlation:\\[SE_{r_{xy}} = \\frac{1 - r^2_{xy}}{\\sqrt{(n - 2)}}\\]\nFigure 11.8: Correlation based 300 pairs.\nestimates means, standard deviations, correlations based small samples relatively large uncertainty, preferable collect larger samples. However, always possible, often goal study provide accurate estimate, test hypothesis. study often requires less observations achieve sufficient power hypothesis test, required able accurately estimate parameter (Maxwell et al., 2008). Therefore, scientists often rely meta-analyses, data multiple studies combined, provide accurate estimates.","code":"\nlibrary(ggplot2)\nset.seed(3190) # set seed for reproducibility\nn <- 10 # set sample size\nx <- rnorm(n = n, mean = 100, sd = 15) # simulate data\n\n# plot data adding normal distribution and annotations\nggplot(as.data.frame(x), aes(x)) +\n  geom_histogram(colour = \"black\", fill = \"grey\", aes(y = ..density..), binwidth = 2) +\n  stat_function(fun = dnorm, args = c(mean = 100, sd = 15), size = 1, color = \"red\", lty = 2) +\n  xlab(\"IQ\") +\n  ylab(\"number of people\") +\n  theme_bw(base_size = 20) +\n  geom_vline(xintercept = mean(x), colour = \"gray20\", linetype = \"dashed\") +\n  coord_cartesian(xlim = c(50, 150)) +\n  scale_x_continuous(breaks = seq(50, 150, 10)) +\n  annotate(\"text\", x = mean(x), y = 0.02, label = paste(\"Mean = \", round(mean(x)), \"\\n\", \"SD = \", round(sd(x)), sep = \"\"), size = 8) + \n  theme(plot.background = element_rect(fill = \"#fffafa\")) + \n  theme(panel.background = element_rect(fill = \"#fffafa\"))"},{"path":"meta.html","id":"a-single-study-meta-analysis","chapter":"11 Meta-analysis","heading":"11.2 A single study meta-analysis","text":"Let’s first begin something hardly ever real life: meta-analysis single study. little silly, simple t-test correlation tell thing – educational compare t-test meta-analysis single study, look multiple studies combined meta-analysis.difference independent t-test meta-analysis t-test performed raw data, meta-analysis typically performed effect size(s) individual studies. metafor R package contains useful function called escalc can used calculate effect sizes, variances, confidence intervals around effect size estimates. let’s start calculating effect size enter meta-analysis. explained chapter effect sizes two main effect sizes used meta-analyses continuous variables standardized mean difference (d) correlation (r), although course also possible perform meta-analyses dichotomous variables (see example ). code calculate standardized mean difference (SMD) two independent groups means (specified m1i m2i), standard deviations (sd1i sd2i), number observations group (n1i n2i). default, metafor computes effect size ‘Hedges’ g’ unbiased version Cohen’s d (see section Cohen's d chapter Effect Sizes).output gives Hedge’s g (yi column, always returns effect size, case standardized mean difference) variance effect size estimate (vi). explained Borenstein (2009) formula 4.18 4.24 standardized mean difference Hedges’ g calculated dividing difference means pooled standard deviation, multiplied correction factor, J:\\[\nJ = (1 - \\ \\ 3/(4df - 1))\n\\]\\[\ng = J \\times \\ \\left( \\frac{{\\overline{X}}_{1} - {\\overline{X}}_{2}}{S_{\\text{within}}} \\right)\n\\]good approximation variance Hedges’ g provided :\\[\nVg = J^{2} \\times \\left( \\frac{n_{1} + n_{2}}{n_{1}n_{2}} + \\frac{g^{2}}{2(n_{1} + n_{2})} \\right)\n\\]variance standardized mean difference depends sample size (n1 n2) value standardized mean difference . perform required calculations meta-analysis, need effect sizes variance. means coded effect sizes sample sizes (per group) studies literature, information need perform meta-analysis. need manually calculate effect size variance using two formula – escalc function . can now easily perform single study meta-analysis using rma function metafor package:'Model Results' find effect size Hedges’ g (0.56) standard error (0.2), Z-test statistic testing mean difference null-hypothesis (2.72), 95% confidence interval [ci.lb = 0.16; ci.ub = 0.95] around effect size (interval width can specified using ‘level =’ option). also see p-value test meta-analytic effect size 0. case can reject null-hypothesis (p = 0.006).meta-analysis, Z-test used examine whether null-hypothesis can rejected. assumes normally distributed random effect size model. Normally, analyze data single study two groups using t-test, surprisingly uses t-distribution. know statistical computations sometimes care lot small amount bias (difference effect size d g, example) sometimes (difference Z t), meta-analysts seem happy Z-scores (fact, large enough sample sizes (commonly true meta-analysis) difference Z-test t-test tiny). directly compare single-study meta-analysis based Z-test t-test, see tiny differences results.explained chapter effect sizes can directly calculate effect size Hedges’ g (95% confidence interval) using MOTE (Buchanan et al., 2017). MOTE package uses t-distribution calculating confidence intervals around effect size (can see makes tiny difference compared using Z-distribution meta-analysis 50 observations group).t-value 2.835, p-value 0.006. results similar computed performing meta-analysis, g = 0.55, 95% CI[0.16; 0.94], effect size upper bound confidence interval differ 0.01 rounding.now common visualize results meta-analysis using forest plot. Acording H. M. Cooper et al. (2009) first forest plot published 1978 (Freiman et al., 1978), goal visualize large set studies concluded absence effect based non-significant results small studies (see Figure 11.9). plotting width confidence interval study, becomes possible see even though studies reject effect size 0, thus non-significant, many studies also reject presence meaningful favorable treatment effect. make large studies noticeable forest plot, later versions added square indicate estimated effect size, size square proportional weight assigned study computing combined effect.\nFigure 11.9: First version forest plot Freiman colleagues, 1978 (image https://www.jameslindlibrary.org/freiman-ja-chalmers-tc-smith-h-kuebler-rr-1978/).\nFigure 11.10 see modern version forest plot, effect size Study 1 marked black square 0.56, confidence interval visualized lines extending 0.16 left 0.95 right. numbers printed right-hand side forest plot provide exact values effect size estimate lower upper bound confidence interval. lower half forest plot, see stretched-diamond, row labeled 'RE Model', 'Random Effects model'. diamond summarizes meta-analytic effect size estimate, centred effect size estimate left right endpoints 95% confidence interval estimate. single study, meta-analytic effect size estimate effect size estimate single study.\nFigure 11.10: Forest plot single study.\n","code":"\nlibrary(metafor)\ng <- escalc(measure = \"SMD\",\n            n1i = 50, # sample size in Group 1\n            m1i = 5.6, # observed mean in Group 1\n            sd1i = 1.2, # observed standard deviation in Group 1\n            n2i = 50, # sample size in Group 2\n            m2i = 4.9, # observed mean in Group 2\n            sd2i = 1.3) # observed standard deviation in Group 2\ng\nmeta_res <- rma(yi, vi, data = g)\nmeta_res## \n## Random-Effects Model (k = 1; tau^2 estimator: REML)\n## \n## tau^2 (estimated amount of total heterogeneity): 0\n## tau (square root of estimated tau^2 value):      0\n## I^2 (total heterogeneity / total variability):   0.00%\n## H^2 (total variability / sampling variability):  1.00\n## \n## Test for Heterogeneity:\n## Q(df = 0) = 0.0000, p-val = 1.0000\n## \n## Model Results:\n## \n## estimate      se    zval    pval   ci.lb   ci.ub    ​ \n##   0.5553  0.2038  2.7243  0.0064  0.1558  0.9547  ** \n## \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"meta.html","id":"simulating-meta-analyses-of-mean-standardized-differences","chapter":"11 Meta-analysis","heading":"11.3 Simulating meta-analyses of mean standardized differences","text":"Meta-analyses get bit exciting using analyze results multiple studies. multiple studies combined meta-analysis, effect size estimates simply averaged, weighed precision effect size estimate, determined standard error, turn determined sample size study. Thus, larger sample size individual study, weight gets meta-analysis, meaning influence meta-analytic effect size estimate.One intuitive way learn meta-analyses simulate studies meta-analyze . code simulates 12 studies. true effect simulated studies, difference means population 0.4 (given standard deviation 1, Cohen's d = 0.4 well). studies vary sample size 30 observations 100 observations per condition. meta-analysis performed, forest plot created.\nFigure 11.11: Forest plot 12 simulated studies.\nsee 12 rows, one study, effect size confidence interval. look closely, can see squares indicate effect size estimate study differ size. larger sample size, bigger square. Study 5 relatively small sample size, can seen small square relatively wide confidence interval. Study 9 larger sample size, thus slightly larger square narrower confidence interval. bottom graph find meta-analytic effect size confidence interval, visualized diamond numerically. model referred FE Model, Fixed Effect (FE) model. alternative approach RE Model, Random Effects (RE) model (difference discussed ).might notice first two studies meta-analysis statistically significant. Take moment think continued research line, finding effect twice row. feel like , run code several times (remove set.seed argued used make simulation reproducible first, get result time) see often happens population effect size range sample sizes simulation. clear discussion mixed results chapter likelihoods, important think meta-analytically. long run, situations find one two non-significant results early research line, even true effect.also look statistical results meta-analysis, bit interesting now 12 studies:see test heterogeneity, topic return . see model results, specific simulation yielded meta-analytic effect size estimate 0.4. confidence interval around effect size estimate [0.3 ; 0.51] much narrower saw single study. 12 studies simulated together quite large sample size, larger sample size, smaller standard error, thus narrower confidence interval . meta-analytic effect size estimate statistically different 0 (p < 0.0001) can reject null hypothesis even use stringent alpha level, 0.001. Note , discussed chapter sample size justification line section justifying error rates chapter error control, seems sensible use much lower alpha level 5% meta-analyses. possible set alpha level metafor, e.g. using level = 0.999 (alpha level 0.001), adjusts confidence intervals, including individual studies, mostly used alpha level 0.05, easier just manually check test significant chosen alpha level (e.g., 0.001).","code":"\nset.seed(94)\nnSims <- 12 # number of simulated studies\nm1 <- 0.4 # population mean Group 1\nsd1 <- 1 # standard deviation Group 1\nm2 <- 0 # population mean Group 2\nsd2 <- 1 # standard deviation Group 1\nmetadata <- data.frame(yi = numeric(0), vi = numeric(0)) # create dataframe\n\nfor (i in 1:nSims) { # for each simulated study\n  n <- sample(30:100, 1) # pick a sample size per group\n  x <- rnorm(n = n, mean = m1, sd = sd1) \n  y <- rnorm(n = n, mean = m2, sd = sd2)\n  metadata[i,1:2] <- metafor::escalc(n1i = n, n2i = n, m1i = mean(x), \n       m2i = mean(y), sd1i = sd(x), sd2i = sd(y), measure = \"SMD\")\n}\nresult <- metafor::rma(yi, vi, data = metadata, method = \"FE\")\npar(bg = \"#fffafa\")\nmetafor::forest(result)## \n## Fixed-Effects Model (k = 12)\n## \n## I^2 (total heterogeneity / total variability):   0.00%\n## H^2 (total variability / sampling variability):  0.25\n## \n## Test for Heterogeneity:\n## Q(df = 11) = 2.7368, p-val = 0.9938\n## \n## Model Results:\n## \n## estimate      se    zval    pval   ci.lb   ci.ub     ​ \n##   0.4038  0.0538  7.5015  <.0001  0.2983  0.5093  *** \n## \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"meta.html","id":"fixed-effect-vs-random-effects","chapter":"11 Meta-analysis","heading":"11.4 Fixed Effect vs Random Effects","text":"two possible models performing meta-analysis. One model, known fixed effect model, assumes one effect size generates data studies meta-analysis. model assumes variation individual studies – exactly true effect size. author metafor package used chapter prefers use term equal-effects model instead fixed effect model. perfect example simulations done far. specified single true effect population, generated random samples population effect.Alternatively, one can use model true effect differs way individual study. don’t single true effect population, range randomly distributed true effect sizes (hence ‘random effects’ model). Studies differs way (sets studies differ sets), true effect sizes differ well. Note difference fixed effect model, random effects model, plural ‘effects’ used latter. Borenstein et al (2009) state two reasons use fixed effect model: studies functionally equivalent, goal generalize populations. makes random effects model generally better choice, although people raised concern random-effects models give weight smaller studies, can biased. default, metafor use random effects model. used method=\"FE\" command explicitly ask fixed effect model. meta-analyses simulate rest chapter, leave command simulate random effects meta-analyses, better choice many real life meta-analyses.","code":""},{"path":"meta.html","id":"simulating-meta-analyses-for-dichotomous-outcomes","chapter":"11 Meta-analysis","heading":"11.5 Simulating meta-analyses for dichotomous outcomes","text":"Although meta-analyses mean differences common, meta-analysis can performed different effect sizes measures. show slightly less common example, let’s simulate meta-analysis based odds ratios. Sometimes main outcome experiment dichotomous variable, success failure task. study designs can calculate risk ratios, odds ratios, risk differences effect size measure. Risk differences sometimes judged easiest interpret, odds ratios often used meta-analysis attractive statistical properties. odds ratio ratio two odds. illustrate odds ratio calculated, useful consider four possible outcomes 2 x 2 table outcomes:odds ratio calculated :\n\\[= \\ \\frac{\\text{AD}}{\\text{BC}}\\]\nmeta-analysis performed log transformed odds ratios (log transformed odds ratios symmetric around 1, see Borenstein et al., 2009), thus log odds ratio used, variance approximated :\n\\[\\text{Var}\\left( \\log\\text{} \\right) = \\ \\frac{1}{} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}\\]Let’s assume train students using spaced learning strategy (work textbook every week instead cramming week exam). Without training, 70 100 students succeed passing course first exam, training, 80 100 students\npass.odds passing experimental group 80/20, 4, odds control condition 70/30, 2.333. ratio two odds : 4/2.333 = 1.714, :\\[\n= \\ \\frac{80 \\times 30}{20\\  \\times 70} = 1.714\n\\]can simulate studies dichotomous outcomes, set percentage successes \nfailures experimental control condition. script , default percentage success experimental condition 70%, control condition 50%.forest plot presents studies four columns data study label, contain number successes failures experimental groups (E+ E-), number successes failures control group (C+ C-). Imagine study percentage people get job within 6 months job training program, compared control condition. Study 1, 50 participants condition, 29 people job training condition got job within 6 months, 21 get job. control condition, 23 people got job, 27 . effect size estimate random effects model 0.65. Feel free play around script, adjusting number studies, sample sizes study, examine effect meta-analytic effect size estimate.can also get meta-analytic test results printing test output. see heterogeneity meta-analysis. true (simulated identical studies), highly unlikely ever happen real life variation effect sizes studies included meta-analysis much realistic scenario.","code":"\nlibrary(metafor)\nset.seed(5333)\nnSims <- 12 # Number of simulated experiments\n\npr1 <- 0.7 # Set percentage of successes in Group 1\npr2 <- 0.5 # Set percentage of successes in Group 2\n\nai <- numeric(nSims) # set up empty vector for successes Group 1\nbi <- numeric(nSims) # set up empty vector for failures Group 1\nci <- numeric(nSims) # set up empty vector for successes Group 2\ndi <- numeric(nSims) # set up empty vector for failures Group 2\n\nfor (i in 1:nSims) { # for each simulated experiment\n  n <- sample(30:80, 1)\n  x <- rbinom(n, 1, pr1) # participants (1 = success, 0 = failure)\n  y <- rbinom(n, 1, pr2) # participants (1 = success, 0 = failure)\n  ai[i] <- sum(x == 1) # Successes Group 1\n  bi[i] <- sum(x == 0) # Failures Group 1\n  ci[i] <- sum(y == 1) # Successes Group 2\n  di[i] <- sum(y == 0) # Failures Group 2\n}\n\n# Combine data into dataframe\nmetadata <- cbind(ai, bi, ci, di)\n# Create escalc object from metadata dataframe \nmetadata <- escalc(measure = \"OR\", \n                   ai = ai, bi = bi, ci = ci, di = di, \n                   data = metadata)\n# Perform Meta-analysis\nresult <- rma(yi, vi, data = metadata)\n# Create forest plot. Using ilab and ilab.xpos arguments to add counts\npar(mar=c(5, 4, 0, 2))\npar(bg = \"#fffafa\")\nforest(result, \n       ilab = cbind(metadata$ai, metadata$bi, metadata$ci, metadata$di), \n       xlim = c(-10, 8), \n       ilab.xpos = c(-7, -6, -5, -4))\ntext(c(-7, -6, -5, -4), 14.7, c(\"E+\", \"E-\", \"C+\", \"C-\"), font = 2, cex = .8)\n# Print result meta-analysis\nresult## \n## Random-Effects Model (k = 12; tau^2 estimator: REML)\n## \n## tau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0645)\n## tau (square root of estimated tau^2 value):      0\n## I^2 (total heterogeneity / total variability):   0.00%\n## H^2 (total variability / sampling variability):  1.00\n## \n## Test for Heterogeneity:\n## Q(df = 11) = 4.8886, p-val = 0.9364\n## \n## Model Results:\n## \n## estimate      se    zval    pval   ci.lb   ci.ub     ​ \n##   0.6548  0.1132  5.7824  <.0001  0.4328  0.8767  *** \n## \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"meta.html","id":"heterogeneity","chapter":"11 Meta-analysis","heading":"11.6 Heterogeneity","text":"Although researchers often primarily use meta-analysis compute meta-analytic effect size estimate, test whether effect statistically different zero, arguably much important use meta-analyses explain variation (sets ) studies. variation among (sets ) studies referred heterogeneity. One goal meta-analyses just code effect sizes estimate meta-analytic effect size, code factors studies can explain heterogeneity, examine factors account heterogeneity. can help theory evaluation theory development. Tests developed examine whether studies included meta-analysis vary expected underlying true effect size studies , measures developed quantify variation.studies true population effect size, source variation random error. real differences (sets ) studies, two sources variation, namely random variation study study real differences effect sizes (sets ) studies.classical measure heterogeneity Cochran’s \\(Q\\) statistic, weighted sum squared differences effect size estimates study, meta-analytic effect size estimate. \\(Q\\) statistic can used test whether absence heterogeneity can statistically rejected (comparing expected amount variation, degrees freedom, df, number studies -1, see Borenstein et al., 2009), can low power number studies meta-analysis small (Huedo-Medina et al., 2006).theoretical grounds one might argue heterogeneity always happen meta-analysis, therefore interesting quantify extent heterogeneity. \\(^2\\) index aims quantify statistical heterogeneity. calculated follows: \\[^{2} = \\ \\frac{(Q - k - 1)}{Q} \\times 100\\%\\] \\(k\\) number studies (\\(k-1\\) degrees freedom). \\(^2\\) ranges 0 100 can interpreted percentage total variability set effect sizes due heterogeneity. \\(^2\\) = 0 variability effect size estimates can explained within-study error, \\(^2\\) = 50 half total variability can explained true heterogeneity. \\(^2\\) values 25%, 50%, 75% can interpreted low, medium, high heterogeneity. Finally, random effects meta-analysis, \\(\\tau^2\\) estimates variance true effects, \\(\\tau\\) estimated standard deviation, expressed scale effect size. benefit \\(\\tau^2\\) depend precision, \\(^2\\) , tends 100% studies included meta-analysis large (Rücker et al., 2008), downside \\(\\tau^2\\) difficult interpret (Harrer et al., 2021).script simulates similar meta-analysis example dichotomous outcomes , small variation. first half simulated experiments based population success rates 0.7 0.2, second half simulated experiments based population success rates 0.9 0.7. Thus, set studies odds ratio differs first half studies, compared second half (successes Group 1 2 set 0.2 0.7 first half, 0.7 0.9 second half). true heterogeneity. use confint function metafor package report \\(^2\\) \\(\\tau^2\\), confidence intervals.Based test heterogeneity, can reject null hypothesis heterogeneity meta-analysis. Tests heterogeneity Type 1 Type 2 error rates, small number studies (example, n = 12) tests heterogeneity can low power. remove set.seed command run code multiple times, see test heterogeneity often significant, even though true heterogeneity simulation. large meta-analyses, power can high test always yields p-value small enough reject null hypothesis, important look \\(^2\\) estimate.Recently considerable attention possibility effect sizes within research lines substantial heterogeneity (Bryan et al., 2021). Large heterogeneity can impact power studies, therefore consequences studies planned (Kenny & Judd, 2019). Although heterogeneity seems low direct replication studies (Olsson-Collentine et al., 2020b) high meta-analyses, argued reflect lack understanding effects research lines (Linden & Hönekopp, 2021).","code":"\nlibrary(metafor)\nset.seed(2942)\nnSims <- 12 # Number of simulated experiments\n\npr1 <- 0.7 # Set percentage of successes in Group 1\npr2 <- 0.2 # Set percentage of successes in Group 2\n\nai <- numeric(nSims) # set up empty vector for successes Group 1\nbi <- numeric(nSims) # set up empty vector for failures Group 1\nci <- numeric(nSims) # set up empty vector for successes Group 2\ndi <- numeric(nSims) # set up empty vector for failures Group 2\n\nfor (i in 1:nSims/2) { # for half (/2) of the simulated studies\n  n <- sample(30:80, 1)\n  x <- rbinom(n, 1, pr1) # produce simulated participants (1 = success, 0 = failure)\n  y <- rbinom(n, 1, pr2) # produce simulated participants (1 = success, 0 = failure)\n  ai[i] <- sum(x == 1) # Successes Group 1\n  bi[i] <- sum(x == 0) # Failures Group 1\n  ci[i] <- sum(y == 1) # Successes Group 2\n  di[i] <- sum(y == 0) # Failures Group 2\n}\n\npr1 <- 0.9 # Set percentage of successes in Group 1\npr2 <- 0.7 # Set percentage of successes in Group 2\n\nfor (i in (nSims/2 + 1):(nSims)) { # for the other half (/2) of each simulated study\n  n <- sample(30:80, 1)\n  x <- rbinom(n, 1, pr1) # produce simulated participants (1 = success, 0 = failure)\n  y <- rbinom(n, 1, pr2) # produce simulated participants (1 = success, 0 = failure)\n  ai[i] <- sum(x == 1) # Successes Group 1\n  bi[i] <- sum(x == 0) # Failures Group 1\n  ci[i] <- sum(y == 1) # Successes Group 2\n  di[i] <- sum(y == 0) # Failures Group 2\n}\n\n# Combine data into dataframe\nmetadata <- cbind(ai, bi, ci, di)\n# Create escalc object from metadata dataframe \nmetadata <- escalc(measure = \"OR\", \n                   ai = ai, bi = bi, ci = ci, di = di, \n                   data = metadata)\n# Perform Meta-analysis\nresult <- rma(yi, vi, data = metadata)\n# Print result meta-analysis\nresult## \n## Random-Effects Model (k = 12; tau^2 estimator: REML)\n## \n## tau^2 (estimated amount of total heterogeneity): 0.3174 (SE = 0.2429)\n## tau (square root of estimated tau^2 value):      0.5634\n## I^2 (total heterogeneity / total variability):   56.53%\n## H^2 (total variability / sampling variability):  2.30\n## \n## Test for Heterogeneity:\n## Q(df = 11) = 25.7650, p-val = 0.0070\n## \n## Model Results:\n## \n## estimate      se    zval    pval   ci.lb   ci.ub     ​ \n##   1.8125  0.2190  8.2764  <.0001  1.3833  2.2417  *** \n## \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nconfint(result) # Get confidence interval for indices of heterogeneity## \n##        estimate   ci.lb   ci.ub \n## tau^2    0.3174  0.0355  1.2286 \n## tau      0.5634  0.1883  1.1084 \n## I^2(%)  56.5308 12.6888 83.4276 \n## H^2      2.3005  1.1453  6.0341"},{"path":"meta.html","id":"strengths-and-weaknesses-of-meta-analysis","chapter":"11 Meta-analysis","heading":"11.7 Strengths and weaknesses of meta-analysis","text":"conclusions meta-analyses debated first meta-analyses performed. ironic , far can find, 'garbage -garbage ' criticism meta-analysis originates Eysenck (1978) although valid criticism, Eysenck published literal garbage, found guilty scientific misconduct, led large number retractions expressions concern. Eysenck wrote meta-analysis yielded results like:surprising feature Smith Glass's (1977) exercise mega-silliness advocacy low standards judgment. , advocate practice abandonment critical judgments kind. mass reports—good, bad, indifferent—fed \ncomputer hope people cease caring quality material conclusions based. abandonment scholarship taken seriously, daunting improbable likelihood, mark beginning passage dark age scientific psychology.\nnotion one can distill scientific knowledge compilation studies mostly poor design, relying subjective, unvalidated, certainly unreliable clinical judgments, dissimilar respect nearly vital parameters, dies hard. article, hoped, final death rattle hopes. \"Garbage —garbage \" well-known axiom computer specialists; applies equal force.problem 'garbage , garbage ' remains one common, difficult deal , criticisms meta-analysis. true meta-analysis turn low quality data good effect size estimate, highly heterogeneous effect sizes useful estimate effect size generalizes studies included meta-analysis. decision studies include meta-analysis difficult one, often leads disagreements conclusions meta-analyses performed set studies (Ferguson, 2014; Goodyear-Smith et al., 2012). Finally, meta-analyses can biased, way individual studies biased, topic explored detail chapter bias detection.strength meta-analysis combining highly similar studies single analysis increases statistical power test, well accuracy effect size estimate. Whenever possible, efficient, perform studies large number observations study, unbiased meta-analysis can provide better statistical inferences. Furthermore, including internal meta-analysis multi-study paper (studies sufficiently similar) can way reduce file-drawer problem, allowing researchers publish mixed results. time, researchers raised concern researchers selectively report studies perform internal meta-analysis simply increase flexibility data analysis, likely erroneously claim support hypothesis (Vosgerau et al., 2019). Researchers publish well-designed studies performed research line, studies similar unbiased, meta-analysis improve inferences. time, result meta-analysis may biased, interpreted final answer. reason, analysis heterogeneity effect size estimates, use statistical techniques detect bias, essential part meta-analysis.","code":""},{"path":"meta.html","id":"reportmeta","chapter":"11 Meta-analysis","heading":"11.8 Which results should you report to be included in a future meta-analysis?","text":"useful educational exercise researcher publishes quantitative studies code dozen studies meta-analysis. notorious problem performing meta-analysis researchers report results meta-analyst needs order include study meta-analysis. Sometimes original researcher can contacted missing information can provided, every single study just data-point future meta-analysis, best report required results included future meta-analysis.single best approach guarantee required information future meta-analysis available meta-analysts share (anonymized) data analysis code manuscript. enable meta-analysts compute statistical information need. Access individual observations allows meta-analysts perform analyses subgroups, makes possible perform advanced statistical tests (Stewart & Tierney, 2002). Finally, access raw data, instead access summary statistics, makes easier find flawed individual studies included meta-analysis (Lawrence et al., 2021). open data becomes norm, efforts standardize measures develop specifications datasets facilitate availability raw data input meta-analyses. also facilitate re-use data, allow researchers perform meta-analyses unrelated main research question. want share raw data collect, make sure address informed consent form.summarizing data scientific article, report number observations associated statistical test. articles mention total sample size, observations removed cleaning data, also report final number observations included test. statistical test based multiple conditions (e.g., t-test), report sample size independent group. information missing, meta-analysts often assume total number observations distributed equally across conditions, always correct. Report full test results significant non-significant results (e.g., never write F < 1, ns). Write full test result, including effect size estimate, regardless p-value, non-significant results especially important included meta-analyses. reporting effect sizes, report computed (e.g., reporting standardized mean differences, compute Cohen's d Hedges' g). Report exact p-values test, full test statistics can used recompute p-value. Report means standard deviations group observations, within-subject designs, report correlation dependent variables (currently almost always never reported, needed compute Cohen’s \\(d_{av}\\) perform simulation based power analyses based predicted data pattern). might useful use table summarize statistical tests many tests reported, raw data shared (example supplemental material).","code":""},{"path":"meta.html","id":"metareporting","chapter":"11 Meta-analysis","heading":"11.9 Improving the reproducibility of meta-analyses","text":"Although meta-analyses provide definitive conclusions, typically interpreted state---art empirical knowledge specific effect research area. Large-scale meta-analyses often accumulate massive number citations influence future research theory development. therefore essential published meta-analyses highest possible quality.time, conclusions meta-analyses often open debate subject change new data becomes available. recently proposed practical recommendations increase reproducibility meta-analyses facilitate quality control, improve reporting guidelines, allow researchers re-analyze meta-analyses based alternative inclusion criteria, future-proof meta-analyses making sure collected meta-analytic data shared continuously accumulating meta-analyses can performed, novel statistical techniques can applied collected data become available (Lakens et al., 2016). need improvement reproducibility meta-analysis clear - recent review 150 meta-analyses Psychological Bulletin revealed 1 meta-analysis shared statistical code (Polanin et al., 2020). unacceptable current day age. addition inspecting well meta-analysis adheres JARS Quantitative Meta-Analysis Reporting Standards, following recommendations summarized Table 11.1 substantially improve state---art meta-analyses.Table 11.1: Six practical recommendations improve quality reproducibility meta-analyses.another open educational resource meta-analysis R, see Meta-Analysis R.","code":""},{"path":"meta.html","id":"test-yourself-9","chapter":"11 Meta-analysis","heading":"11.10 Test Yourself","text":"Q1: true standard deviation sample, standard deviation mean (standard error)?sample size increases, standard deviation sample becomes smaller, standard deviation mean (standard error) becomes smaller.sample size increases, standard deviation sample becomes accurate, standard deviation mean (standard error) becomes smaller.sample size increases, standard deviation sample becomes smaller, standard deviation mean (standard error) becomes accurate.sample size increases, standard deviation sample becomes accurate, standard deviation mean (standard error) becomes accurate.Q2: perform meta-analysis just averaging observed effect sizes, effect size d = 0.7 small study 20 observations influence meta-analytic effect size estimate just much d = 0.3 study 2000 observations. meta-analytic effect size estimate computed instead?Effect sizes estimates small studies undergo small study correction included.Effect size estimates small studies ignored computing meta-analytic effect size estimate.Effect sizes weighed based precision estimate, determined standard error.Effect sizes weighed based close meta-analytic effect size estimate, studies removed receiving less weight.Q3: size squares indicating effect sizes forest plot vary function :Power studySize effectSample sizeType 1 error rateQ4: One can compute ‘fixed effect model’ ‘random effects model’ performing meta-analysis studies scientific literature. statement true?generally recommended compute fixed effect model, mainly studies included meta-analysis functionally similar.generally recommended compute random effects model, mainly studies included meta-analysis functionally similar.generally recommended compute fixed effect model, reduces heterogeneity set studies.generally recommended compute random effects model, reduces heterogeneity set studies.Q5: heterogeneity effect size estimates included meta-analysis, fixed effect random effects model yield similar conclusions. variability effect size estimates, two models can yield different results. , see two forest plots based 5 simulated studies. top plot based random effects meta-analysis, bottom plot based fixed effect meta-analysis. random effects meta-analysis incorporates uncertainty variability effect size estimates final meta-analytic estimate. translate difference two plots?\nFigure 11.12: Simulated studies random effects model.\n\nFigure 11.13: Simulated studies fixed effect model.\ndifference meta-analytic effect size estimate plots, effect size estimate 5 studies identical.effect size random effects model identical estimate fixed effect model, confidence interval larger.effect size random effects model identical estimate fixed effect model, confidence interval smaller.effect size random effects model larger estimate fixed effect model, incorporates additional uncertainty bias estimate.Q6: statement true two measures heterogeneity discussed , Cochran’s \\(Q\\) \\(^2\\)?Cochran’s \\(Q\\) relies hypothesis testing approach detecting heterogeneity, studies, can low power. \\(^2\\) relies estimation approach, studies, can large uncertainty.Cochran’s \\(Q\\) relies estimation approach, studies, can large uncertainty. \\(^2\\) relies hypothesis testing approach detecting heterogeneity, studies, can low power.Q7: Researchers perform similar studies research line can combine studies (whether yield statistically significant results, ) internal meta-analysis, combining effect sizes meta-analytic estimate. strength approach, risk?strength internal meta-analysis can reduce Type 1 error rate multiple studies performed, 5% alpha level, weakness selectively including studies internal meta-analysis, researcher additional flexibility p-hack.strength internal meta-analysis can reduce Type 1 error rate multiple studies performed, 5% alpha level, weakness effect size estimate might biased compared estimates single studies, especially heterogeneity.strength internal meta-analysis can prevent publication bias providing way report results (including non-significant results), weakness selectively including studies internal meta-analysis, researcher additional flexibility p-hack.strength internal meta-analysis can prevent publication bias providing way report results (including non-significant results), weakness effect size estimate might biased compared estimates single studies, especially heterogeneity.Q8: best way guarantee statistical results meta-analysis computationally reproducible? Choose best answer.Use open source software, metafor R, share analysis data, share analysis code, anyone can run code data.Use commercial software, 'Comprehensive Meta-Analysis', although allow export data analysis code, allows share picture forest plot.","code":""},{"path":"meta.html","id":"open-questions-9","chapter":"11 Meta-analysis","heading":"11.10.1 Open Questions","text":"difference standard deviation standard error, happens sample size increases?difference standard deviation standard error, happens sample size increases?internal meta-analysis?internal meta-analysis?analyze single study, t-test meta-analysis differ slightly results. ?analyze single study, t-test meta-analysis differ slightly results. ?black squares forest plot, horizontal lines black square?black squares forest plot, horizontal lines black square?Effect sizes meta-analysis simply averaged. , combined instead?Effect sizes meta-analysis simply averaged. , combined instead?difference fixed effect random effects meta-analysis?difference fixed effect random effects meta-analysis?heterogeneity meta-analysis, interesting?heterogeneity meta-analysis, interesting?problem ‘garbage , garbage ’?problem ‘garbage , garbage ’?","code":""},{"path":"bias.html","id":"bias","chapter":"12 Bias detection","heading":"12 Bias detection","text":"Bias can introduced throughout research process. useful prevent detect . researchers recommend skeptical attitude towards claim read scientific literature. example, philosopher science Deborah Mayo (2018) writes: \"Confronted statistical news flash day, first question : results due selective reporting, cherry picking, number similar ruses?\". might make popular first question ask speaker next scientific conference attending, time naïve ignore fact researchers less intentionally introduce bias claims.extreme end practices introduce bias scientific research research misconduct: Making data results, changing omitting data results research isn’t accurately represented research record. example, Andrew Wakefield authored fraudulent paper 1998 claimed link measles, mumps, rubella (MMR) vaccine autism. retracted 2010, caused damage trust vaccines among parts general population. Another example psychology concerned study James Vicary subliminal priming. claimed found flashing 'EAT POPCORN' 'DRINK COCA-COLA' subliminally movie screen cinema, sales popcorn Coca-Cola increased 57.5 18.1 percent, respectively. However, later found Vicary likely committed scientific fraud, evidence study ever performed (S. Rogers, 1992). website Retraction Watch maintains database tracks reasons scientific papers retracted, including data fabrication. unknown often data fabrication occurs practice, discussed chapter research integrity, expect least small percentage scientists fabricated, falsified modified data results least .\nFigure 12.1: Scene Dropout company Theranos falsely claimed devices perform blood tests small amounts blood. scene, two whistelblowers confront bosses pressured remove datapoints show desired results.\ndifferent category mistakes statistical reporting errors, range reporting incorrect degrees freedom, reporting p = 0.056 p < 0.05 (Nuijten et al., 2015). Although best prevent errors, everyone makes , data code sharing become common, become easier detect errors work researchers. Dorothy Bishop (2018) writes: \"open science becomes increasingly norm, find everyone fallible. reputations scientists depend whether flaws research, respond flaws noted.\"Statcheck software automatically extracts statistics articles recomputes p-values, long statistics reported following guidelines American Psychological Association (APA). checks reported statistics internally consistent: Given test statistics degrees freedom, reported p-value accurate? , makes less likely made mistake (although prevent coherent mistakes!) , check information statistical test accurate. Statcheck perfect, make Type 1 errors flags something error actually , easy use tool check articles submit publication.inconsistencies data less easy automatically detect, can identified manually. example, N. J. L. Brown & Heathers (2017) show many papers report means possible given sample size (known GRIM test). example, Matti Heino noticed blog post three reported means table classic study Festinger Carlsmith mathematically impossible. 20 observations per condition, scale -5 5, means end multiple 1/20, 0.05. three means ending X.X8 X.X2 consistent reported sample size scale. course, inconsistencies can due failing report missing data questions, GRIM test also used uncover scientific misconduct.\nFigure 12.2: Screenshot table reporting main results Festinger Carlsmith, 1959.\n","code":""},{"path":"bias.html","id":"publicationbias","chapter":"12 Bias detection","heading":"12.1 Publication bias","text":"Publication bias one biggest challenges science faces. Publication bias practice selectively submitting publishing scientific research, often based whether results ‘statistically significant’ . scientific literature dominated statistically significant results. time, know many studies researchers perform yield significant results. scientists access significant results, results, lacking complete overview evidence hypothesis. extreme cases, selective reporting can lead situation hundreds statistically significant results published literature, true effect even non-significant studies shared. known file-drawer problem, non-significant results hidden away file-drawers (nowadays, folders computer) available scientific community. Every scientist work towards solving publication bias, extremely difficult learn likely true long scientists share results, , Greenwald (1975) notes, ethical violation.\nFigure 12.3: Screenshot final sentences Greenwald, . G. (1975). Consequences prejudice null hypothesis. Psychological Bulletin, 82(1), 1–20.\nPublication bias can fixed making research results available fellow scientists, irrespective p-value main hypothesis test. Registered Reports one way combat publication bias, type scientific article reviewed based introduction, method, statistical analysis plan, data collected (Chambers & Tzavella, 2022; Nosek & Lakens, 2014). peer review experts field, might suggest improvements design analysis, article can get 'principle acceptance', means long research plan followed, article published, regardless results. facilitate publication null results, shown Figure 12.4, analysis first published Registered Reports psychology revealed 31 71 (44%) articles observed positive results, compared 146 152 (96%) comparable standard scientific articles published time period (Scheel, Schijen, et al., 2021).\nFigure 12.4: Positive result rates standard reports Registered Reports. Error bars indicate 95% confidence intervals around observed positive result rate.\npast, Registered Reports exist, scientists share results (Franco et al., 2014; Greenwald, 1975; Sterling, 1959), consequence, try detect extent publication bias impacts ability accurately evaluate literature. Meta-analyses always carefully examine impact publication bias meta-analytic effect size estimate - even though estimated 57% meta-analyses published Psychological Bulletin 1990 2017 report assessed publication bias (Polanin et al., 2020). recent meta-analyses published educational research, 82% used bias detection tests, methods used typically far state---art (Ropovik et al., 2021). Several techniques detect publication bias developed, continues active field research. techniques based specific assumptions, consider applying test (Carter et al., 2019). silver bullet: None techniques can fix publication bias. None can tell certainty true meta-analytic effect size corrected publication bias. best methods can detect publication bias caused specific mechanisms, specific conditions. Publication bias can detected, corrected.chapter likelihoods saw mixed results expected, can strong evidence alternative hypothesis. case mixed results expected, exclusively observing statistically significant results, especially statistical power low, surprising. commonly used lower limit statistical power 80%, can expect non-significant result one five studies true effect. researchers pointed finding mixed results can unlikely (‘good true’) set studies (Francis, 2014; Schimmack, 2012). don’t good feeling real patterns studies look like, continuously exposed scientific literature reflect reality. Almost multiple study papers scientific literature present statistically significant results, even though unlikely.online Shiny app used compute binomial likelihoods displays, scroll bottom page, binomial probabilities find multiple significant findings given specific assumption power tests. Francis (2014) used binomial likelihoods calculate test excessive significance (Ioannidis & Trikalinos, 2007) 44 articles published journal Psychological Science 2009 2012 contained four studies . found 36 articles, likelihood observing four significant results, given average power computed based observed effect sizes, less 10%. Given choice alpha level 0.10, binomial probability hypothesis test, allows claims (10% alpha level) whenever binomial probability number statistically significant results lower 10%, data surprising, can reject hypothesis unbiased set studies. words, unlikely many significant results observed, suggesting publication bias selection effects played role articles.One 44 articles co-authored (Jostmann et al., 2009). time, knew little statistical power publication bias, accused improper scientific conduct stressful. yet, accusations correct - selectively reported results, selectively reported analyses worked. received virtually training topic, educated , uploaded unpublished study website psychfiledrawer.org (longer exists) share filedrawer. years later, assisted Many Labs 3 included one studies published set studies replicating (Ebersole et al., 2016), null result observed, wrote \"conclude actually reliable evidence effect\" (Jostmann et al., 2016). hope educational materials prevents others making fool .","code":""},{"path":"bias.html","id":"bias-detection-in-meta-analysis","chapter":"12 Bias detection","heading":"12.2 Bias detection in meta-analysis","text":"New methods detect publication bias continuously developed, old methods become outdated (even though can still see appear meta-analyses). One outdated method known fail-safe N. idea calculate number non-significant results one need file-drawers observed meta-analytic effect size estimate longer statistically different 0. longer recommended, Becker (2005) writes \"Given approaches now exist dealing publication bias, failsafe N abandoned favor , informative analyses\". Currently, use fail-safe N tool identify meta-analyses state---art.can explain second method (Trim--Fill), ’s useful explain common way visualize meta-analyses, known funnel plot. funnel plot, x-axis used plot effect size study, y-axis used plot ‘precision’ effect size (typically, standard error effect size estimate). larger number observations study, precise effect size estimate, smaller standard error, thus higher funnel plot study . infinitely precise study (standard error 0) top y-axis.script simulates meta-analyses nsims studies, stores results needed examine bias detection. first section script, statistically significant results desired direction simulated, second part null results generated. script generates percentage significant results indicated pub.bias - set 1, results significant. code , pub.bias set 0.05. true effect simulation (m1 m2 equal, difference groups), significant results expected 5% false positives. Finally, meta-analysis performed, results printed, funnel plot created.Let’s start looking unbiased research looks like, running code, keeping pub.bias 0.05, 5% Type 1 errors enter scientific literature.examine results meta-analysis see 100 studies meta-analysis (k = 100), statistically significant heterogeneity (p = 0.69, surprising, programmed simulation true effect size 0, heterogeneity effect sizes). also get results meta-analysis. meta-analytic estimate d = -0.002, close 0 (, true effect size indeed 0). standard error around estimate 0.012. 100 studies, accurate estimate true effect size. Z-value test d = 0 -0.177, p-value test 0.86. reject hypothesis true effect size 0. CI around effect size estimate (-0.026, 0.021) includes 0.examine funnel plot Figure 12.5 see study represented dot. larger sample size, higher plot, smaller sample size, lower plot. white pyramid represents area within study statistically significant, observed effect size (x-axis) far enough removed 0 confidence interval around observed effect size exclude 0. lower standard error, narrow confidence interval, smaller effect sizes needs order statistically significant. time, smaller standard error, closer effect size true effect size, less likely see effects far away 0. expect 95% effect size estimates fall within funnel, centered true effect size. see studies (five, exact) fall outside white pyramid right side plot. 5% significant results programmed simulation. Note 5 studies false positives, true effect. true effect (can re-run simulation set d 0.5 changing m1 <- 0 simulation m1 <- 0.5) pyramid cloud points move right, centered 0.5 instead 0.\nFigure 12.5: Funnel plot unbiased null results.\ncan now compare unbiased meta-analysis biased meta-analysis. can simulate situation extreme publication bias. Building estimate Scheel, Schijen, et al. (2021), assume 96% studies show positive results. set pub.bias <- 0.96 code. keep means 0, still real effect, end mainly Type 1 errors predicted direction final set studies. simulating biased results, can perform meta-analysis see statistical inference based meta-analysis misleading.biased nature set studies analyzed becomes clear examine funnel plot Figure 12.6. pattern quite peculiar. see four unbiased null results, programmed simulation, remainder 96 studies statistically significant, even though null true. see studies fall just edge white pyramid. p-values uniformly distributed null, Type 1 errors observe often p-values range 0.02 0.05, unlike expect true effect. just significant p-values fall just outside white pyramid. larger study, smaller effect size significant. fact effect sizes vary around single true effect size (e.g., d = 0 d = 0.5), rather effect sizes become smaller larger sample sizes (smaller standard errors), strong indicator bias. vertical dotted line black triangle top plot illustrate observed (upwardly biased) meta-analytic effect size estimate.\nFigure 12.6: Funnel plot biased null results mostly significant results.\nOne might wonder extreme bias ever really emerges scientific research. . Figure 12.7 see funnel plot Carter & McCullough (2014) examined bias 198 published studies testing 'ego-depletion' effect, idea self-control relies limited resource. notice similarities extremely biased meta-analysis simulated ? might surprised , even though 2015 researchers thought large reliable literature demonstrating ego-depletion effects, Registered Replication report yielded non-significant effect size estimate (Hagger et al., 2016), even original researchers tried replicate work, failed observe significant effect ego-depletion (Vohs et al., 2021). Imagine huge amount wasted time, effort, money literature completely based bias scientific research. Obviously, research waste ethical implications, researchers need take responsibility preventing waste future.\nFigure 12.7: Funnel plot Carter McCullough (2014) vizualizing bias 198 published tests ego-depletion effect.\ncan also see signs bias forest plot meta-analysis. Figure 12.8, two forest plots plotted side side. left forest plot based unbiased data, right forest plot based biased data. forest plots bit big 100 studies, see left forest plot, effects randomly vary around 0 . right, beyond first four studies, confidence intervals magically just exclude effect 0.\nFigure 12.8: Forest plot unbiased meta-analysis (left) biased meta-analysies (right).\npublication bias researchers publish statistically significant results (p < \\(\\alpha\\)), calculate effect size meta-analysis, meta-analytic effect size estimate higher publication bias (researchers publish effects p < \\(\\alpha\\)) compared publication bias. publication bias filters smaller (non-significant) effect sizes. included computation meta-analytic effect size. leads meta-analytic effect size estimate larger true population effect size. strong publication bias, know meta-analytic effect size inflated, know much. true effect size just bit smaller, true effect size also 0, case ego-depletion literature.","code":"\nlibrary(metafor)\nlibrary(truncnorm)\n\nnsims <- 100 # number of simulated experiments\npub.bias <- 0.05 # set percentage of significant results in the literature\n\nm1 <- 0 # too large effects will make non-significant results extremely rare\nsd1 <- 1\nm2 <- 0\nsd2 <- 1\nmetadata.sig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, \n                           n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)\nmetadata.nonsig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, \n                              n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)\n\n# simulate significant effects in the expected direction\nif(pub.bias > 0){\nfor (i in 1:nsims*pub.bias) { # for each simulated experiment\n  p <- 1 # reset p to 1\n  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100)) # n based on truncated normal\n  while (p > 0.025) { # continue simulating as along as p is not significant\n    x <- rnorm(n = n, mean = m1, sd = sd1) \n    y <- rnorm(n = n, mean = m2, sd = sd2) \n    p <- t.test(x, y, alternative = \"greater\", var.equal = TRUE)$p.value\n  }\n  metadata.sig[i, 1] <- mean(x)\n  metadata.sig[i, 2] <- mean(y)\n  metadata.sig[i, 3] <- sd(x)\n  metadata.sig[i, 4] <- sd(y)\n  metadata.sig[i, 5] <- n\n  metadata.sig[i, 6] <- n\n  out <- t.test(x, y, var.equal = TRUE)\n  metadata.sig[i, 7] <- out$p.value\n  metadata.sig[i, 8] <- paste0(\"t(\", out$parameter, \")=\", out$statistic)\n}}\n\n# simulate non-significant effects (two-sided)\nif(pub.bias < 1){\nfor (i in 1:nsims*(1-pub.bias)) { # for each simulated experiment\n  p <- 0 # reset p to 1\n  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100))\n  while (p < 0.05) { # continue simulating as along as p is significant\n    x <- rnorm(n = n, mean = m1, sd = sd1) # produce  simulated participants\n    y <- rnorm(n = n, mean = m2, sd = sd2) # produce  simulated participants\n    p <- t.test(x, y, var.equal = TRUE)$p.value\n  }\n  metadata.nonsig[i, 1] <- mean(x)\n  metadata.nonsig[i, 2] <- mean(y)\n  metadata.nonsig[i, 3] <- sd(x)\n  metadata.nonsig[i, 4] <- sd(y)\n  metadata.nonsig[i, 5] <- n\n  metadata.nonsig[i, 6] <- n\n  out <- t.test(x, y, var.equal = TRUE)\n  metadata.nonsig[i, 7] <- out$p.value\n  metadata.nonsig[i, 8] <- paste0(\"t(\", out$parameter, \")=\", out$statistic)\n}}\n\n# Combine significant and non-significant effects\nmetadata <- rbind(metadata.nonsig, metadata.sig)\n\n# Use escalc to compute effect sizes\nmetadata <- escalc(n1i = n1, n2i = n2, m1i = m1, m2i = m2, sd1i = sd1, \n  sd2i = sd2, measure = \"SMD\", data = metadata[complete.cases(metadata),])\n# add se for PET-PEESE analysis\nmetadata$sei <- sqrt(metadata$vi)\n\n#Perform meta-analysis\nresult <- metafor::rma(yi, vi, data = metadata)\nresult\n\n# Print a Funnel Plot\nmetafor::funnel(result, level = 0.95, refline = 0)\nabline(v = result$b[1], lty = \"dashed\") # vertical line at meta-analytic ES\npoints(x = result$b[1], y = 0, cex = 1.5, pch = 17) # add point## \n## Random-Effects Model (k = 100; tau^2 estimator: REML)\n## \n## tau^2 (estimated amount of total heterogeneity): 0.0000 (SE = 0.0018)\n## tau (square root of estimated tau^2 value):      0.0006\n## I^2 (total heterogeneity / total variability):   0.00%\n## H^2 (total variability / sampling variability):  1.00\n## \n## Test for Heterogeneity:\n## Q(df = 99) = 91.7310, p-val = 0.6851\n## \n## Model Results:\n## \n## estimate      se     zval    pval    ci.lb   ci.ub   ​ \n##  -0.0021  0.0121  -0.1775  0.8591  -0.0258  0.0215    \n## \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## \n## Random-Effects Model (k = 100; tau^2 estimator: REML)\n## \n## tau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0019)\n## tau (square root of estimated tau^2 value):      0\n## I^2 (total heterogeneity / total variability):   0.00%\n## H^2 (total variability / sampling variability):  1.00\n## \n## Test for Heterogeneity:\n## Q(df = 99) = 77.6540, p-val = 0.9445\n## \n## Model Results:\n## \n## estimate      se     zval    pval   ci.lb   ci.ub     ​ \n##   0.2701  0.0125  21.6075  <.0001  0.2456  0.2946  *** \n## \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"bias.html","id":"trim-and-fill","chapter":"12 Bias detection","heading":"12.3 Trim and Fill","text":"Trim fill technique aims augment dataset adding hypothetical ‘missing’ studies (may ‘file-drawer’). procedure starts removing (‘trimming’) small studies bias meta-analytic effect size, estimates true effect size, ends ‘filling’ funnel plot studies assumed missing due publication bias. Figure 12.9, can see funnel plot , now added hypothetical studies (unfilled circles represent ‘imputed’ studies). look closely, ’ll see points mirror image opposite side meta-analytic effect size estimate (clearest lower half funnel plot). examine result meta-analysis includes imputed studies, see trim fill successfully alerts us fact meta-analysis biased (, add imputed studies) fails miserably correcting effect size estimate. funnel plot, see original (biased) effect size estimate indicated triangle, meta-analytic effect size estimate adjusted trim--fill method (indicated black circle). see meta-analytic effect size estimate bit lower, given true effect size simulation 0, adjustment clearly sufficient.\nFigure 12.9: Funnel plot assumed missing effects added trim--fill.\nTrim--fill good many realistic publication bias scenarios. method criticized reliance strong assumption symmetry funnel plot. publication bias based p-value study (arguably important source publication bias many fields) trim--fill method perform well enough yield corrected meta-analytic effect size estimate close true effect size (Peters et al., 2007; Terrin et al., 2003). assumptions met, can used sensitivity analysis. Researchers report trim--fill corrected effect size estimate realistic estimate unbiased effect size. bias-detection tests (like p-curve z-curve discussed ) already indicated presence bias, trim--fill procedure might provide additional insights.","code":""},{"path":"bias.html","id":"pet-peese","chapter":"12 Bias detection","heading":"12.4 PET-PEESE","text":"novel class solutions publication bias meta-regression. Instead plotting line individual data-points, meta-regression line plotted data points represent study. normal regression, data meta-regression based , precise estimate , therefore, studies meta-analysis, better meta-regression work practice. number studies small, bias detection tests lose power, something one keep mind using meta-regression. Furthermore, regression requires sufficient variation data, case meta-regression means wide range sample sizes (recommendations indicate meta-regression performs well studies range 15 200 participants group – typical research areas psychology). Meta-regression techniques try estimate population effect size precision perfect (standard error = 0).One meta-regression technique known PET-PEESE (Stanley et al., 2017; Stanley & Doucouliagos, 2014). consists ‘precision-effect-test’ (PET) can used Neyman-Pearson hypothesis testing framework test whether meta-regression estimate can reject effect size 0 based 95% CI around PET estimate intercept SE = 0. Note confidence interval wide due small number observations, test might low power, -priori low probability rejecting null effect. estimated effect size PET calculated : \\(d = β_0 + β_1SE_i + _ui\\) d estimated effect size, SE standard error, equation estimated using weighted least squares (WLS), 1/SE2i weights. PET estimate underestimates effect size true effect. Therefore, PET-PEESE procedure recommends first using PET test whether null can rejected, , 'precision-effect estimate standard error' (PEESE) used estimate meta-analytic effect size. PEESE, standard error (used PET) replaced variance (.e., standard error squared), Stanley & Doucouliagos (2014) find reduces bias estimated meta-regression intercept.PET-PEESE limitations, bias detection techniques . biggest limitations work well studies, studies meta-analysis small sample sizes, large heterogeneity meta-analysis (Stanley et al., 2017). situations apply (practice), PET-PEESE might good approach. Furthermore, situations might correlation sample size precision, practice often linked heterogeneity effect sizes included meta-analysis. example, true effects different across studies, people perform power analyses accurate information expected true effect size, large effect sizes meta-analysis small sample sizes, small effects large sample sizes. Meta-regression , like normal regression, way test association, need think causal mechanism behind association.Let’s explore PET-PEESE meta-regression attempts give us unbiased effect size estimate, specific assumptions publication bias caused. Figure 12.10 see funnel plot, now complemented 2 additional lines plots. vertical line d = 0.27 meta-analytic effect size estimate, upwardly biased averaging statistically significant studies . 2 additional lines, meta-regression lines PET-PEESE based formulas detailed previously. straight diagonal line gives us PET estimate SE 0 (infinite sample, top plot), indicated circle. dotted line around PET estimate 95% confidence interval estimate. case, 95% CI contains 0, means based PET estimate d = 0.02, reject meta-analytic effect size 0. Note even 100 studies, 95% CI quite wide. Meta-regression , just like normal regression, accurate data . one limitation PET-PEESE meta-regression: small numbers studies meta-analysis, low accuracy. able reject null based PET estimate, used PEESE estimate (indicated diamond shap) d = 0.17 meta-analytic effect size, corrected bias (never knowing whether model underlying PEESE estimate corresponded true bias generating mechanisms meta-analysis, thus meta-analytic estimate accurate).\nFigure 12.10: Funnel plot PETPEESE regression lines.\n","code":""},{"path":"bias.html","id":"p-value-meta-analysis","chapter":"12 Bias detection","heading":"12.5 P-value meta-analysis","text":"addition meta-analysis effect sizes, possible perform meta-analysis p-values. first approaches known Fisher's combined probability test, recent bias detection tests p-curve analysis (Simonsohn et al., 2014) p-uniform* (Aert & Assen, 2018) build idea. two techniques example selection model approaches test adjust meta-analysis (Iyengar & Greenhouse, 1988), model data generating process effect sizes combined selection model publication bias impacts effect sizes become part scientific literature. example data generating process results studies generated statistical tests test assumptions met, studies average power. selection model might studies published, long statistically significant alpha level 0.05.P-curve analysis uses exactly selection model. assumes significant results published, examines whether data generating process mirrors expected studies certain power, whether data generating process mirrors pattern expected null hypothesis true. discussed section p-values can expect observe uniformly distributed p-values null hypothesis true, small significant p-values (e.g., 0.01) large significant p-values (e.g., 0.04) alternative hypothesis true. P-curve analysis performs two tests. first test, p-curve analysis examines whether p-value distribution flatter expected studies analyze 33% power. value somewhat arbitrary (can adjusted), idea reject smallest level statistical power lead useful insights presence effects. average power set studies less 33%, might effect, studies designed well enough learn performing statistical tests. can reject presence pattern p-values least 33% power, suggests distribution looks like one expected null hypothesis true. , doubt effect set studies included meta-analysis, even though individual studies statistically significant.second test examines whether p-value distribution sufficiently right-skewed (small significant p-values large significant p-values), pattern suggests can reject uniform p-value distribution. can reject uniform p-value distribution, suggests studies might examined true effect least power. second test significant, act set studies examines true effect, even though might publication bias. example, let’s consider Figure 3 Simonsohn colleagues (2014). authors compared 20 papers Journal Personality Social Psychology used covariate analysis, 20 studies use covariate. authors suspected researchers might add covariate analyses try find p-value smaller 0.05, first analysis tried yield significant effect.\nFigure 12.11: Figure 3 Simonsohn et al (2014) showing p-curve without bias.\np-curve distribution observed p-values represented five points blue line. P-curve analysis performed statistically significant results, based assumption always published, thus part p-value distribution contains studies performed. 5 points illustrate percentage p-values 0 0.01, 0.01 0.02, 0.02 0.03, 0.03 0.04, 0.04 0.05. figure right, see relatively normal right-skewed p-value distribution, low high p-values. p-curve analysis shows blue line right figure right-skewed uniform red line (red line uniform p-value distribution expected effect). Simonsohn colleagues summarize pattern indication set studies 'evidential value', terminology somewhat misleading. formally correct interpretation can reject p-value distribution expected null hypothesis true studies included p-curve analysis. Rejecting uniform p-value distribution automatically mean evidence theorized effect (e.g., pattern caused mix null effects small subset studies show effect due methodological confound).left figure see opposite pattern, mainly high p-values around 0.05, almost p-values around 0.01. blue line significantly flatter green line, p-curve analysis suggests set studies result selection bias generated set sufficiently powered studies. P-curve analysis useful tool. important correctly interpret p-curve analysis can tell . right-skewed p-curve prove bias, theoretical hypothesis true. flat p-curve prove theory incorrect, show studies meta-analyzed look like pattern expected null hypothesis true, selection bias.script stores test statistics 100 simulated t-tests included meta-analysis. first rows look like:Print test results cat(metadata$pcurve, sep = \"\\n\"), go online p-curve app http://www.p-curve.com/app4/. Paste test results, click ‘Make p-curve’ button. Note p-curve app yield result p-values smaller 0.05 - test statistics yield p > 0.05, p-curve computed, tests ignored.\nFigure 12.12: Result p-curve analysis biased studies.\ndistribution p-values clearly looks like comes uniform distribution (indeed ), statistical test indicates can reject p-value distribution steep steeper generated set studies 33% power, p < 0.0001. app also provides estimate average power tests generated observed p-value distribution, 5%, indeed correct. Therefore, can conclude studies, even though many effects statistically significant, line selective reporting Type 1 errors, p-value distribution expected true effect studied sufficient statistical power. theory might still true, set studies analyzed provide support theory.similar meta-analytic technique p-uniform*. technique similar p-curve analysis selection bias models, uses results significant non-significant studies, can used estimate bias-adjusted meta-analytic effect size estimate. technique uses random-effects model estimate effect sizes study, weighs based selection model assumes significant results likely published non-significant results. , see output p-uniform* estimates bias-corrected effect size d = 0.0126. effect size statistically different 0, p = 0.3857, therefore bias detection technique correctly indicates even though effects statistically significant, set studies provide good reason reject meta-analytic effect size estimate 0.alternative technique also meta-analyzes p-values individual studies z-curve analysis, meta-analysis observed power ((Bartoš & Schimmack, 2020; Brunner & Schimmack, 2020); example, see (Sotola, 2022)). Like traditional meta-analysis, z-curve analysis transform observed test results (p-values) z-scores. unbiased literature null hypothesis true, observe approximately \\(\\alpha\\)% significant results. null true, distribution z-scores centered 0. Z-curve analysis computes absolute z-values, therefore \\(\\alpha\\)% z-scores larger critical value (1.96 5% alpha level). Figure 12.13 z-scores 1000 studies plotted, true effect size 0, exactly 5% observed results statistically significant.\nFigure 12.13: Z-curve analysis 1000 studies true effect size 0 without publication bias.\ntrue effect, distribution z-scores shifts away 0, function statistical power test. higher power, right distribution z-scores located. example, examining effect 66% power, unbiased distribution z-scores, computed observed p-values, looks like distribution Figure 12.14.\nFigure 12.14: Z-curve analysis 1000 studies true effect size d = 0.37 n = 100 per condition independent t-test without publication bias.\nmeta-analysis studies included differ statistical power, true effect size (due heterogeneity). Z-curve analysis uses mixtures normal distributions centered means 0 6 fit model underlying effect sizes best represents observed results included studies (technical details, see Bartoš & Schimmack (2020). z-curve aims estimate average power set studies, calculates observed discovery rate (ODR: percentage significant results, observed power), expected discovery rate (EDR: proportion area curve right side significance criterion) expected replication rate (ERR: expected proportion successfully replicated significant studies significant studies). z-curve able correct selection bias positive results (specific assumptions), can estimate EDR ERR using significant p-values.examine presence bias, preferable submit non-significant significant p-values z-curve analysis, even significant p-values used produce estimates. Publication bias can examined comparing ODR EDR. percentage significant results set studies (ODR) much higher expected discovery rate (EDR), sign bias. analyze set biased studies used illustrate bias detection techniques discussed , z-curve analysis able indicate presence bias. can perform z-curve following code:see distribution z-scores looks peculiar. expected z-scores 0 1.96 missing. 96 100 studies significant, makes observed discovery rate (ODR), observed power (across studies different sample sizes) 0.96, 95% CI[0.89; 0.99]. expected discovery rate (EDR) 0.053, differs statistically observed discovery rate, indicated fact confidence interval EDR overlap ODR 0.96. means clear indication selection bias based z-curve analysis. expected replicability rate studies 0.052, line expectation observe 5% Type 1 errors, true effect simulation. Thus, even though entered significant p-values, z-curve analysis correctly suggests expect results replicate higher frequency Type 1 error rate.","code":"## t(136)=0.208132209831132\n## t(456)=-1.20115958535433\n## t(58)=0.0422284763301259\n## t(358)=0.0775200850900646\n## t(188)=2.43353676652346\npuniform::puniform(m1i = metadata$m1, m2i = metadata$m2, n1i = metadata$n1, \n  n2i = metadata$n2, sd1i = metadata$sd1, sd2i = metadata$sd2, side = \"right\")## \n## Method: P\n## \n## Effect size estimation p-uniform\n## \n##        est     ci.lb     ci.ub       L.0      pval      ksig\n##     0.0126   -0.0811    0.0887   -0.2904    0.3857        96\n## \n## ===\n## \n## Publication bias test p-uniform\n## \n##       L.pb    pval\n##     7.9976   <.001\n## \n## ===\n## \n## Fixed-effect meta-analysis\n## \n##     est.fe     se.fe   zval.fe pval.fe  ci.lb.fe  ci.ub.fe     Qstat     Qpval\n##     0.2701    0.0125   21.6025   <.001    0.2456    0.2946   77.6031     0.945\nz_res <- zcurve::zcurve(p = metadata$pvalues, method = \"EM\", bootstrap = 1000)\nsummary(z_res, all = TRUE)\nplot(z_res, annotation = TRUE, CI = TRUE)## Call:\n## zcurve::zcurve(p = metadata$pvalues, method = \"EM\", bootstrap = 1000)\n## \n## model: EM via EM\n## \n##               Estimate  l.CI   u.CI\n## ERR              0.052 0.025  0.160\n## EDR              0.053 0.050  0.119\n## Soric FDR        0.947 0.389  1.000\n## File Drawer R   17.987 7.399 19.000\n## Expected N        1823   806   1920\n## Missing N         1723   706   1820\n## \n## Model converged in 38 + 205 iterations\n## Fitted using 96 p-values. 100 supplied, 96 significant (ODR = 0.96, 95% CI [0.89, 0.99]).\n## Q = -6.69, 95% CI[-23.63, 11.25]"},{"path":"bias.html","id":"conclusion","chapter":"12 Bias detection","heading":"12.6 Conclusion","text":"Publication bias big problem science. present almost meta-analyses performed primary hypothesis test scientific articles, articles much likely submitted accepted publication primary hypothesis test statistically significant. Meta-analytic effect size estimates adjusted bias almost always overestimate true effect size, bias-adjusted effect sizes might still misleading. messed scientific literature publication bias, way us know whether computing accurate meta-analytic effect sizes estimates literature. Publication bias inflates effect size estimate unknown extent, already several cases true effect size turned zero. publication bias tests chapter might provide certainty unbiased effect size, can function red flag indicate bias present, provide adjusted estimates , underlying model publication bias correct, might well closer truth.lot activity literature tests publication bias. many different tests, need carefully check assumptions test applying . tests don’t work well large heterogeneity, heterogeneity quite likely. meta-analysis always examine whether publication bias, preferably using multiple publication bias tests, therefore useful just code effect sizes, also code test statistics p-values. None bias detection techniques discussed chapter silver bullet, better naively interpreting uncorrected effect size estimate meta-analysis.another open educational resource tests publication bias, see Meta-Analysis R.","code":""},{"path":"bias.html","id":"test-yourself-10","chapter":"12 Bias detection","heading":"12.7 Test Yourself","text":"Q1: happens publication bias researchers publish statistically significant results (p < \\(\\alpha\\)), calculate effect size meta-analysis?meta-analytic effect size estimate identical whether publication bias (researchers publish effects p < \\(\\alpha\\)) publication bias.meta-analytic effect size estimate closer true effect size publication bias (researchers publish effects p < \\(\\alpha\\)) compared publication bias.meta-analytic effect size estimate inflated publication bias (researchers publish effects p < \\(\\alpha\\)) compared publication bias.meta-analytic effect size estimate lower publication bias (researchers publish effects p < \\(\\alpha\\)) compared publication bias.Q2: forest plot figure looks quite peculiar. notice?effect sizes quite similar, suggesting large sample sizes highly accurate effect size measures.studies look designed based perfect -priori power analyses, yielding just significant results.studies confidence intervals just fail include 0, suggesting studies just statistically significant. suggests publication bias.effects direction, suggests one-sided tests performed, even though might preregistered.Q3: statement true?extreme publication bias, individual studies literature can significant, standard errors large meta-analytic effect size estimate significantly different 0.extreme publication bias, individual studies literature can significant, meta-analytic effect size estimate severely inflated, giving impression overwhelming support \\(H_1\\) actually true effect size either small, even 0.extreme publication bias, individual studies significant, \nmeta-analytic effect size estimates automatically corrected publication bias statistical packages, meta-analytic effect size estimate therefore quite reliable.Regardless whether publication bias, meta-analytic effect size estimate severely biased, never considered reliable estimate population.Q4: statement true based plot , visualizing PET-PEESE meta-regression?\nFigure 12.15: Funnel plot PETPEESE regression lines studies Q2.\nUsing PET-PEESE meta-regression can show true effect size d = 0 (based PET estimate).Using PET-PEESE meta-regression can show true effect size d = 0.23 (based PEESE estimate).Using PET-PEESE meta-regression can show true effect size d = 0.34 (based normal meta-analytic effect size estimate).small sample size (10 studies) means PET low power reject null, therefore reliable indicator bias - might reason worry.Q5: Take look figure output table p-curve app , gives results studies Q2. interpretation output correct?\nFigure 12.16: Result p-curve analysis biased studies Q2.\nBased continuous Stouffer’s test full p-curve, reject p-value distribution expected \\(H_0\\), can reject p-value distribution expected \\(H_1\\) true studies 33% power.Based continuous Stouffer’s test full p-curve, can\nconclude observed p-value distribution skewed enough \ninterpreted presence true effect size, therefore theory used deduce studies incorrect.Based continuous Stouffer’s test full p-curve, can\nconclude observed p-value distribution skewed enough \ninterpreted line p-value distribution expected \\(H_1\\) true studies 33% power.Based continuous Stouffer’s test full p-curve, can\nconclude observed p-value distribution flatter \nexpect studies 33% power, therefore, can conclude studies based fabricated data.Q6: true effect size studies simulated Q2 0 - true effect. statement z-curve analysis true?expected discovery rate expected replicability rate statistically significant, therefore can expect observed effects successfully replicate future studies.Despite fact average observed power (observed discovery rate) 100%, z-curve correctly predicts expected replicability rate (5%, Type 1 errors statistically significant).Z-curve able find indication bias, expected discovery rate expected replicability rate differ statistically.Although observed discovery rate 1 (indicating observed power 100%) confidence interval ranges 0.66 1, indicates studies lower realistic power, fact 100% results significant happened chance.Q7: yet perform trim fill analysis, given analyses (e.g., z-curve analysis), statement true?trim--fill method likely indicate missing studies 'fill'.trim--fill method known low power detect bias, contradict z-curve p-curve analysis reported .trim--fill analysis indicate bias, p-curve z-curve analysis, adjusted effect size estimate trim--fill adequately correct bias, analysis add anything.trim--fill method provides reliable estimate true effect size, provided methods discussed far, therefore reported alongside bias detection tests.Q8: Publication bias defined practice selectively submitting publishing scientific research. Throughout chapter, focussed selectively submitting significant results. Can think research line research question researchers might prefer selectively publish non-significant results?","code":""},{"path":"bias.html","id":"open-questions-10","chapter":"12 Bias detection","heading":"12.7.1 Open Questions","text":"idea behind GRIM test?idea behind GRIM test?definition ‘publication bias’?definition ‘publication bias’?file-drawer problem?file-drawer problem?funnel plot, true studies fall inside funnel (centered 0)?funnel plot, true studies fall inside funnel (centered 0)?true trim--fill approach respect ability detect correct effect size estimates?true trim--fill approach respect ability detect correct effect size estimates?using PET-PEESE approach, important consider meta-analysis small number studies?using PET-PEESE approach, important consider meta-analysis small number studies?conclusions can draw 2 tests reported p-curve analysis?conclusions can draw 2 tests reported p-curve analysis?","code":""},{"path":"prereg.html","id":"prereg","chapter":"13 Preregistration and Transparency","heading":"13 Preregistration and Transparency","text":"long data used support scientific claims, people tried selectively present data line wish true. example scientist Daryl Bem, parapsychologist studies whether people extra-sensory perception allows predict future. using selective reporting, publishing 9 studies top journal claiming people predict future, Bem kick-started replication crisis psychology back 2011. Figure 13.1 can see results discussion study performed (Bem, 2011). study, participants pressed left right button predict whether picture hidden behind left right curtain. moment made decision, even computer randomly determined picture appear, performance better average surprising.\nFigure 13.1: Screenshot Results Discussion section Bem, 2011.\nclear 5 tests guessing average (erotic, neutral, negative, positive, ‘romantic non-erotic’ pictures). Bonferroni correction lead us use alpha level 0.01 (alpha 0.05/5 tests) main result, participants guessed future position erotic pictures guessing average, p-value 0.013, allowed Bem reject null hypothesis, given pre-specified alpha level corrected multiple comparisons.five categories (erotic, neutral, negative, positive, romantic non-erotic pictures) predicted people perform better guessing average , evolved ability predict future? think Bem actually predicted effect erotic pictures , seen data? might trust Bem predicted effect specific group stimuli, 'cooking' - making multitudes observations, selecting significant result, HARK - hypothesize results known (Kerr, 1998) introduction study. think researchers simply trust predicted reported outcome, performed study multiple conditions, found effect one condition? skeptical, doubt can take claims Bem's paper face value?","code":""},{"path":"prereg.html","id":"preregistration-of-the-statistical-analysis-plan","chapter":"13 Preregistration and Transparency","heading":"13.1 Preregistration of the Statistical Analysis Plan","text":"past, researchers proposed solutions prevent bias literature due inflated Type 1 error rates result selective reporting. example, Bakan (1966) discussed problematic aspects choosing whether perform directional hypothesis test looking data. researcher chooses perform directional hypothesis test two-sided hypothesis test yields p-value 0.05 0.10 (.e., test yields p = 0.08, researcher decides seeing result one-sided test also warranted, reports p-value 0.04, one-sided) practice Type 1 error rate doubled (.e., 0.10 instead 0.05). Bakan (p. 431) writes:handled? central registry one registers one's decision run one- two-tailed test collecting data? one, one eminent psychologist suggested , send oneself letter postmark prove one pre-decided run one-tailed test?De Groot (1969) already pointed importance \"work advance investigative procedure (experimental design) paper fullest possible extent\" include \"statement confirmation criteria, including formulation null hypotheses, , choice statistical test(s), significance level resulting confirmation intervals\" \"details mentioned, brief note rationale, .e., justification investigator's particular choices.\"rise internet made possible create online registries allow researchers specify study design, sample plan, statistical analysis plan data collected. time-stamp, sometimes even dedicated Digital Object Identifier (DOI) transparently communicates peers research question analysis plan specified looking data. important, can’t test hypothesis data used generate . come hypothesis looking data, hypothesis might true, nothing done severely test hypothesis yet. exploring data, can perform hypothesis test, test hypothesis.fields, medicine, now required register certain studies, clinical trials. example, International Committee Journal Editors writes:ICMJE requires, recommends, medical journal editors require registration clinical trials public trials registry time first patient enrollment condition consideration publication.use study registries promoted Food Drug Administration (FDA) since 1997. registries description study contact information provided main goal make easier public take part clinical trials. 2000 onwards registries increasingly used prevent bias, regulations become increasingly strict terms reporting primary outcome studies data collection, well updating registry results data collection complete, although rules always followed (Goldacre et al., 2018).requirement register primary outcome interest ClinicalTrials.gov correlated substantial drop number studies observed statistically significant results, indicate removing flexibility data analyzed prevented false positive results reported. Kaplan Irvin (2015) analyzed results randomized controlled trials evaluating drugs dietary supplements treatment prevention cardiovascular disease. observed 17 30 studies (57%) published requirement register studies ClinicalTrials.gov yielded statistically significant results, 2 25 (8%) studies published 2000 observed statistically significant results. course, correlation causation, can conclude causal effect. go doctor sick, doctor tells luckily two cures, one proven effective study published 1996, one study published 2004, cure pick?\nFigure 13.2: Figure Kaplan Irvin (2015) showing substantial drop statistically significant results registration primary outcomes required ClinicalTrials.gov.\nimplemented perfectly, study registries allow scientific community know planned analyses data collection, main result planned hypotheses. However, results might necessarily end published literature, especially risk studies predictions confirmed. One step beyond study registration novel publication format known Registered Reports. Journals publish Registered evaluate studies based introduction, method, statistical analyses, results (Chambers & Tzavella, 2022; Nosek & Lakens, 2014). idea review studies data collection new, proposed repeatedly last half century (Wiseman et al., 2019). discussed section publication bias, Registered Reports substantially larger probability reporting findings support hypotheses compared traditional scientific literature (Scheel, Schijen, et al., 2021).benefits downsides publishing research Registered Report still examined, one benefit researchers get feedback expert reviewers time can still improve study, instead data already collected. Moving process criticism study study performed (traditional peer review) study performed (Registered Reports, implementing 'Red Teams' Lakens (2020) methodological review boards Lakens (2023) universities) idea worth exploring, make process scientific criticism collaborative, reviewers can help improve study, instead deciding whether flaws manuscript consequential recommend publication.","code":""},{"path":"prereg.html","id":"the-value-of-preregistration","chapter":"13 Preregistration and Transparency","heading":"13.2 The value of preregistration","text":"Preregistration primary goal allow others transparently evaluate capacity test falsify prediction, severely hypothesis tested (Lakens, 2019). severity test determined likely prediction proven wrong wrong, proven right right. research process researchers can make decisions increase probability prediction statistically supported, even wrong. example, Daryl Bem decided 5 sets stimuli focus results section, choice focus erotic stimuli (versus mean stimuli, stimuli another condition, negative pictures) justified fact p-value ended statistically significant. opposite can also happen, researchers desire obtain non-significant test result, researchers make decisions lead higher likelihood corroborating prediction (e.g., reducing statistical power), even prediction correct. goal preregistration prevent researchers non-transparently reducing capacity test falsify prediction allowing readers work see planned test prediction access data, evaluate whether changes original plan reduce severely tested prediction.Preregistration adds value people , based philosophy science, increase trust claims supported severe tests predictive successes. Preregistration make study better worse compared non-preregistered study (Lakens, 2019). Instead, merely allows researchers transparently evaluate severity test. severity test theory unrelated whether study preregistered. However, practice, whenever reward structures science introduce researcher bias, preregistration likely increase severity tests. Preregistration add value correct analysis approach completely clear researchers readers, example theory well specified rational analysis plan. scientific fields, however, theories rarely completely constrain test predictions. Despite , important recognize cases deviating preregistered analysis plan lead severely tested hypothesis. example, preregistration failed consider participants drunk respond meaningfully task, forgot specify data normally distributed, changing original analysis plan seen fellow researchers severe test, way increase probability find statistically significant effect. transparently list deviations analysis plan, provide strong justifications deviations, readers can draw conclusions severely hypothesis tested.Preregistration tool, researchers use goal preregistration facilitates. use tool detached philosophy science risks becoming heuristic. Researchers choose preregister become new norm, preregister can justify based philosophy science preregistration supports goals. many types research preregistration necessary. Although always good transparent possible research, philosophy science perspective, unique value preregistration limited research aims severely test predictions. Outside type research, transparency (example sharing data, materials, lab notebook detailing decisions made) can valuable allow researchers evaluate results detail. addition primary goal preregistration allow others evaluate severely prediction tested, researchers reported secondary benefits preregistering, feeling preregistration improved experimental design, analysis plan, theoretical predictions (Sarafoglou et al., 2022). Although necessary publicly preregister reap benefits, public preregistration can motivate researchers carefully think study advance.","code":""},{"path":"prereg.html","id":"how-to-preregister","chapter":"13 Preregistration and Transparency","heading":"13.3 How to preregister","text":"detail preregistration document , easier others transparently evaluate severity tests performed. difficult come aspects one include, researchers created websites guide researchers process (e.g., https://aspredicted.org/), including submission guidelines, templates (van ’t Veer & Giner-Sorolla, 2016). template Van 't Veer Giner-Sorolla excellent start, intended place begin people experience preregistering research. Another useful paper Wicherts et al. (2016) provides checklist aspects consider planning, executing, analyzing, reporting research.\nFigure 13.3: Screenshot Table 1 Wicherts et al., 2016, depicts checklist preregistrations.\nAlthough checklists useful introduce scientists idea preregistration, important raise bar level need high quality preregistrations actually fulfill goal allow peer evaluate severity test. first step towards authors follow reporting guidelines field. psychology, means following Journal Article Reporting Standards (JARS) (Appelbaum et al., 2018). reporting guidelines encompass suggestions needed preregistration document, recommend using JARS preregistration document writing final report, well-thought set recommendations. Taking JARS account planning reporting research likely improve research.Journal Article Reporting Standards inform information needs present title page, abstract paper, introduction, method section, results section, discussion. example, JARS states add Author Note title page includes \"Registration information study registered\". method result sections receive lot attention JARS, two sections also deserve lot attention preregistration. Remember severe test high probability finding predicted effect prediction correct, high probability finding predicted effect prediction incorrect. Practices inflate Type 1 error rate increase possibility finding predicted effect prediction actually wrong. Low power, unreliable measures, flawed procedure, bad design increase possibility finding effect prediction actually correct. Incorrect analyses risk answering question unrelated prediction researchers set test (sometimes referred Type 3 error). see, JARS aims address threats severity test asking authors provide detailed information methods results sections.","code":""},{"path":"prereg.html","id":"journal-article-reporting-standards","chapter":"13 Preregistration and Transparency","heading":"13.4 Journal Article Reporting Standards","text":"Although following focus quantitative experimental studies random assignment conditions (can download JARS table ), JARS includes tables experiments without randomization, clinical trials, longitudinal designs, meta-analyses, replication studies. following items JARS table relevant preregistration:Describe unit randomization procedure used generate random assignment sequence, including details restriction (e.g., blocking, stratification).Describe unit randomization procedure used generate random assignment sequence, including details restriction (e.g., blocking, stratification).Report inclusion exclusion criteria, including restrictions based demographic characteristics.Report inclusion exclusion criteria, including restrictions based demographic characteristics.prevents flexibility concerning participants included final analysis.Describe procedures selecting participants, including\nSampling method systematic sampling plan implemented\nPercentage sample approached actually participated\nSampling method systematic sampling plan implementedPercentage sample approached actually participatedYou might often know percentage sample approach participate, getting information might require pilot data, might able reach desired final sample size (see ) sampling plan.Describe sample size, power, precision, including\nIntended sample size\nDetermination sample size, including\nPower analysis, methods used determine precision parameter estimates\nExplanation interim analyses stopping rules employed\n\nIntended sample sizeDetermination sample size, including\nPower analysis, methods used determine precision parameter estimates\nExplanation interim analyses stopping rules employed\nPower analysis, methods used determine precision parameter estimatesExplanation interim analyses stopping rules employedClearly stating intended sample size prevents practices optional stopping, inflate Type 1 error rate. aware (, JARS remind ) might end achieved sample size differs intended sample size, consider possible reasons might manage collect intended sample size. sample size needs justified, assumptions power analysis (e.g., expected effect size realistic, smallest effect size interest indeed interest others?). used sequential analyses, specify controlled Type 1 error rate analyzing data repeatedly came .range possible sample size justifications, recommend using online Shiny app accompanies sample size justification chapter. Shiny app can found . Shiny app guides four steps.First, researchers specify population sampling . describe sample, researchers can simply follow JARS guidelines, Quantitative Design Reporting Standards:Report major demographic characteristics (e.g., age, sex, ethnicity, socioeconomic status) important topic-specific characteristics (e.g., achievement level studies educational interventions).\ncase animal research, report genus, species, strain number specific identification, name location supplier stock designation. Give number animals animals' sex, age, weight, physiological condition, genetic modification status, genotype, health-immune status, drug test naivete, previous procedures animal may subjected.\nReport inclusion exclusion criteria, including restrictions based demographic characteristics.Researchers also indicate can collect data entire sample (case sample size justification completed), , resource limitations (e.g., time money available data collection).second step, researchers consider effects interest can specify. Ideally, able determine smallest effect size interest, approaches can also used. third step researchers specify inferential goal test hypothesis, measure effect accuracy. Finally, researchers specify total sample size (based number participants number observations per participant) explain informational value study (e.g., sample size large enough yield informative answer research question?). filling relevant fields Shiny app, researchers can download PDF file contains complete sample size justification.Describe planned data diagnostics, including\nCriteria post-data collection exclusion participants, \nCriteria deciding infer missing data methods used imputation missing data\nDefining processing statistical outliers\nAnalyses data distributions\nData transformations used, \nCriteria post-data collection exclusion participants, anyCriteria deciding infer missing data methods used imputation missing dataDefining processing statistical outliersAnalyses data distributionsData transformations used, anyAfter collecting data, first step examine data quality, test assumptions planned analytic methods. common exclude data participants follow instructions, decision procedures prespecified. preregistration discover additional unforeseen consequences added sections. data missing, might want remove participant entirely, use method impute missing data. outliers can undue influence results, might want preregister ways mitigate impact outliers. practical recommendations classify, detect, manage outliers, see (Leys et al., 2019). planning perform statistical tests assumptions (e.g., assumption normality Welch's t-test) need preregister decide whether assumptions met, , .Describe analytic strategy inferential statistics protection experiment-wise error \nPrimary hypotheses\nSecondary hypotheses\nExploratory hypotheses\nPrimary hypothesesSecondary hypothesesExploratory hypothesesThe difference three levels hypotheses adequately explained JARS material, H. Cooper (2020) explains distinction bit , although remains quite vague. way distinguish three categories follows. First, study designed answer primary hypothesis. Type 1 Type 2 error rates primary hypothesis low researcher can afford make . Secondary hypotheses questions researcher considers interesting planning study, main goal study. Secondary hypotheses might concern additional variables collected, even sub-group analyses deemed interesting outset. hypotheses, Type 1 error rate still controlled level researchers consider justifiable. However, Type 2 error rate controlled secondary analyses. effect expected additional variables might much smaller effect primary hypothesis, analyses subgroups smaller sample sizes. Therefore, study yield informative answer significant effect observed, non-significant effect can interpreted study lacked power (reject null hypothesis, equivalence test). labeling question secondary hypothesis, researcher specifies advance non-significant effects lead clear conclusions.Finally, left-category analyses performed article. refer category exploratory results, exploratory hypotheses, researcher might hypothesized analyses , comes tests data analysis. JARS requires researchers report results 'terms substantive findings error rates may uncontrolled'. exploratory result might deemed impressive readers, , depending prior belief, severely tested. findings need independently replicated want able build - else equal, requirement imminent exploratory results.","code":""},{"path":"prereg.html","id":"what-does-a-formalized-analytic-strategy-look-like","chapter":"13 Preregistration and Transparency","heading":"13.5 What Does a Formalized Analytic Strategy Look Like?","text":"hypothesis test methodological procedure evaluate prediction can described conceptual level (e.g., \"Learning preregister improves research\"), operationalized level (e.g., \"Researchers read text control alpha level carefully, precisely specify corroborate falsify prediction preregistration document\"), statistical level (e.g., \"independent t-test comparing coded preregistration documents written people read text show statistically lower number ways hypothesis tested, implies careful Type 1 error control, compared people read text\"). preregistration document, goal specify hypothesis detail statistical level. Furthermore, statistical hypothesis clearly linked conceptual operationalized level. studies people perform multiple tests, often clear pattern results falsify researchers' predictions. Currently, preregistrations differ widely detailed , preregistration sufficient detail treat confirmatory tests predictions (Waldron & Allen, 2022).\nFigure 13.4: Different study types plotted dimension fully exploratory fully confirmatory (Waldron & Allen, 2022).\nPreregistration relatively new practice researchers. surprising often quite room improvement way researchers preregister. sufficient preregister - goal preregister well enough others can evaluate severity tested hypothesis. ? First, important acknowledge difficult describe hypothesis verbally. Just like use notation describe statistics removes ambiguity, verbal descriptions hypotheses rarely sufficiently constrain potential flexibility data analysis.example, verbal description statistical hypothesis previous paragraph (read \"independent t-test comparing coded preregistration documents written people read text show statistically lower number ways hypothesis tested, implies careful Type 1 error control, compared people read text\") clear alpha level plan use t-test, whether perform Student's t-test Welch's t-test. Researchers often implicitly treat p > 0.05 falsifying prediction, common misconception p-values, hypothesis often better falsified using statistical test can reject presence predicted outcomes, equivalence test. Specifying explicitly evaluate hypothesis makes clear prediction proven wrong.Lakens & DeBruine (2020) discuss good way remove ambiguity hypothesis test described preregistration document make sure machine readable. Machines notoriously bad dealing ambiguous descriptions, hypothesis understandable machine, clearly specified. hypothesis tested analysis takes data input returns test results. test results compared criteria, used evaluation test result. example, imagine hypothesis predicts mean one group higher mean another group. data analyzed Welch's t-test, resulting p-value smaller specified criterion alpha (e.g., 0.01) prediction evaluated corroborated. prediction falsified can reject effects deemed large enough matter equivalence test, result inconclusive otherwise. clear preregistration hypothesis test, components (analysis, way results compared criteria, results evaluated terms corroborating falsifying prediction) clearly specified.transparent way specify statistical hypothesis analysis code. gold standard preregistration create simulated dataset looks like data plan collect, write analysis script can run dataset plan collect. Simulating data might sound difficult, great packages R, increasing number tutorials. Since need perform analyses anyway, collect data helps carefully think experiment. preregistering analysis code, make sure steps data analysis clear, including assumption checks, exclusion outliers, exact analysis plan run (including parameters need specified test).addition sharing analysis code, need specify evaluate test result analysis code run data collect. often made explicit preregistrations, essential part hypothesis test, especially multiple primary hypotheses, prediction \"Researchers read text become better controlling alpha level clearly specify corroborate falsify prediction\". hypothesis really predicts outcomes occur, evaluation hypothesis specify prediction falsified one two effects occurs.","code":""},{"path":"prereg.html","id":"readytopreregister","chapter":"13 Preregistration and Transparency","heading":"13.6 Are you ready to preregister a hypothesis test?","text":"often happen theory use make predictions strong enough lead falsifiable hypotheses. Especially early research lines many uncertainties analyses run, effects small matter. stages resear h process, common cyclical approach researchers experiment see happens, use insights reformulate theory, design another experiment. philosopher science Van Fraassen summarizes statement: \"experimentation continuation theory construction means.\" process, often need examine whether certain assumption make hold. often requires tests auxiliary hypotheses concerning measures manipulations use (Uygun Tunç & Tunç, 2022).prepare preregistration document, might faced many uncertainties exactly know address. might sign yet ready preregister prediction. testing hypotheses, corroborating prediction impressive, falsifying prediction consequential theory testing. make arbitrary choices write predictions, test might neither impressive consequential. Sometimes just want collect data describe, examine relationship variables, explore boundary conditions, without testing anything. case, feel forced hypothesis testing straight-jacket (Scheel, Tiokhin, et al., 2021). course, study also allow make claims severely tested, goal every study, especially new research lines.","code":""},{"path":"prereg.html","id":"test-yourself-11","chapter":"13 Preregistration and Transparency","heading":"13.7 Test Yourself","text":"assignment go steps complete high-quality preregistration. assignment continue next chapter, focus computationally reproducible analysis pipeline implementing open science practices sharing data code. Open science set practices reproducibility, transparency, sharing, collaboration based openness data tools allows others reuse scrutinize research. might want complete assignment real research project involved . involved real research projects, can perform simple study analyzing publicly accessible data just assignment. illustrate using hypothesis can answered based movie ratings Internet Movie Database (IMDB). can come hypothesis want based another data source (spend much time data collection, goal assignment).organize preregistration, can follow templates others created, can find https://osf.io/zab38/wiki/home/. default OSF preregistration template good hypothesis testing studies. Keep JARS reporting guidelines mind writing preregistration.One favorite movies Fight Club. stars Brad Pitt Edward Norton. conceptual level hypothesis Brad Pitt Edward Norton great actors, great actors, movies play equally good. operationalized level hypothesis average movies star Brad Pitt Edward Norton receive rating Internet Movie Database. IMDB provides IMDB rating, metascore (provided metacritic.com).\nFigure 13.5: Screenshot IMDB metacritic rating.\noperationalize movie ratings IMDB scores, movies star Brad Pitt Edward Norton movies appeared , according following two search commands IMDB:Brad Pitt\nhttp://www.imdb.com/filmosearch?role=nm0000093&explore=title_type&mode=detail&page=1&title_type=movie&ref_=filmo_ref_job_typ&sort=release_date,desc&job_type=actorFor Edward Norton:\nhttp://www.imdb.com/filmosearch?role=nm0001570&explore=title_type&mode=detail&page=1&title_type=movie&ref_=filmo_ref_job_typ&sort=release_date,desc&job_type=actorQ1: Write hypothesis conceptual level. research areas might able capture hypothesis formal quantitative model, often describe hypothesis verbally. Try precise possible, even though verbal descriptions hypotheses inherently limited. also useful discuss whether data already collected, , explain whether hypothesis influenced knowledge data (typical preregistration, data collected).Q2: Write hypothesis operationalized level. variables study (.e., independent /dependent variables) clearly specified.next steps specify hypothesis statistical level. Sometimes resources limit statistical questions one can answer. Whenever case, useful first perform sample size justification. case, useful first specify statistical question calculate required sample size. already know number movies Brad Pitt Edward Norton appear limited, first proceed sample size justification.Q3: Justify sample size. use Shiny app go steps sample size justification.1.1: Describe population sampling .population consists movies Brad Pitt Edward Norton starred (March 2023) since start career, indexed internet movie database (www.imdb.com). total number observations limited movies Brad Pitt Edward Norton appeared date, 62 39, respectively.1.2: Can collect data entire population?yes.total number observations limited movies Brad Pitt Edward Norton appeared date, 62 39, respectively.Effect Sizes Interest?smallest effect size interest always matter discussion among peers. case, personally believe difference movie ratings less 0.5 10 point scale (used IMDB) sufficiently small support prediction movies Brad Pitt Edward Norton equally good. words, raw difference larger -0.5 smaller 0.5, conclude two sets movies rated equally well.minimal statistically detectable effect given number movies (62 39) can computed examining effect 50% power independent t-test, example computed G*Power. can compute minimal statistically detectable effect, need specify alpha level. know sample size limited, statistical power issue. time, need make decision based available data, take many years larger sample size. context sample size fixed, decision must made, sensible balance Type 1 Type 2 error compromise power analysis. , determine alpha level 0.15 defensible decision. means rather high probability incorrectly concluding two groups movies rating, reduce probability incorrectly concluding two movies rating, actually .-- Sunday, March 05, 2023 -- 12:19:19\nt tests - Means: Difference two independent means (two groups)\nAnalysis: Sensitivity: Compute required effect size\nInput: Tail(s) = Two\nα err prob = 0.15\nPower (1-β err prob) = 0.5\nSample size group 1 = 62\nSample size group 2 = 39\nOutput: Noncentrality parameter δ = 1.4420104\nCritical t = 1.4507883\nDf = 99\nEffect size d = 0.2947141The Minimal Statistically Detectable Effect thus d = 0.295.sensitivity analysis shows given sample size can collect (n = 62 39), smallest effect size interest (half scale point), assuming standard deviation movie ratings sd = 0.9 (estimate based pilot data), statistical power 91% assume difference movie ratings exactly 0. possible movie ratings actually differ slightly, difference exactly 0. assume true difference movie ratings 0.1 power still 86%. decent statistical power direct consequence increasing alpha level. 5% Type 1 error rate, power 64% (assuming true difference movies 0.1), combined error rate ((100 - 64) + 5) = 41% , increasing alpha level 15% combined error rate dropped ((100 - 86) + 15) = 29%, means probability making error, assuming H0 H1 equally likely true, reduced.TOSTER::power_t_TOST(\nn = c(62, 39),\ndelta = 0.1,\nsd = 0.9,\nlow_eqbound = -0.5,\nhigh_eqbound = 0.5,\nalpha = 0.05,\ntype = \"two.sample\"\n)can conclude decent power planned test, given smallest effect size interest high alpha level.Inferential goalOur inferential goal statistical test controlling error rates, therefore, plan make decision. use sensitivity analysis justify error rates (also used compromise power analysis formally minimizing combined error rate, Maier & Lakens (2022)).Informational Value StudyFinally, evaluate informational value study. First, using available data, tried reduce combined Type 1 Type 2 error rate somewhat balancing two error rates. goal make decision based available data. decision relatively high probability wrong, value study allows us make decision well possible, given available data. Therefore, anyone else wants know answer question movies starring Brad Pitt Edward Norton indeed equally good, results give best possible answer currently available, even substantial remaining uncertainty study.Q4: Write hypothesis statistical level specify code analysis. specific possible. Look JARS recommendations , checklist Wicherts et al. (2016), make sure miss details need specify (e.g., pre-process data, software version use, etc.).can now specify test plan perform statistical level. expect missing data outliers, analyze movie ratings equivalence test equivalence bounds -0.5 0.5, alpha level 0.15, group sizes unequal Welch's t-test (assume equal variances) performed Delacre et al. (2017). following analysis code (run R version 4.2.0, TOSTER package version 0.4.1) assumes data stored imdb_ratings dataframe column movie ratings Brad Pitt (column name brad_pitt_score) column movie ratings Edward Norton (column name edward_norton_score)TOSTER::t_TOST(\nx = imdb_ratings\\(brad_pitt_score,  y = imdb_ratings\\)edward_norton_score,\nlow_eqbound = -0.5,\nhigh_eqbound = 0.5,\neqbound_type = \"raw\",\nalpha = 0.15,\nvar.equal = FALSE\n)Finally, need specify criteria evaluate results. t_TOST function also perform null-hypothesis significance test, convenient, can now consider hypothesis supported equivalence test significant p < 0.15, 70% confidence interval falls completely within equivalence bounds. can consider hypothesis falsified null hypothesis significance test significant p < 0.15. neither two tests significant, results inconclusive. tests significant, hypothesis also falsified, effect, small matter. formally, hypothesis corroborated TOST p < 0.015 & NHST p > 0.015, falsified NHST p < 0.015, inconclusive otherwise.now completed preregistration simple study. real studies, preregistration process often easy. sample size justification often requires knowledge variables analysis, power analyses become uncertain complex analysis, might challenging make decisions data pre-processing deal outliers. struggle preregistration, might simply ready preregister. might tightening phase research, equivalent phase 3 trial clinical trials, need perform descriptive research can perform real test hypothesis.","code":""},{"path":"prereg.html","id":"practical-aspects-of-an-online-preregistration","chapter":"13 Preregistration and Transparency","heading":"13.7.1 Practical Aspects of an Online Preregistration","text":"last years multiple online services created allow preregister hypothesis plan. discuss three solutions: ZPID, OSF, AsPredicted. decreasing order, three services differ high set bar researchers preregister platform. ZPID specific psychological science, OSF accessible anyone, AsPredicted requires email address academic institution preregister.preregister, preregistration archived. cost time (ZPID due manual checks) money (long-term data storage). preregister real studies (even though AsPredicted allows class assignments preregistrations). assignment suffices store PDF file contains information related preregistration, without actually creating time-stamped version one databases.","code":""},{"path":"prereg.html","id":"pre-registering-on-psycharchives-by-zpid","chapter":"13 Preregistration and Transparency","heading":"13.7.2 Pre-registering on PsychArchives by ZPID","text":"Go https://pasa.psycharchives.org. Log ORCID one can make one (available people working studying research institution), make dedicated Leibniz Psychology account.Click ‘Start new submission’see recommendation submit preregistration PDF/file, PDF allow certain restrictions hinder long-term archiving. already reveals PsychArchives ask meet certain standards deem best practices, might think . good thing!Microsoft word, can save file PDF/compliant pdf file choosing ‘File’> ‘Save ’, choose PDF dropdown menu, click ‘options…’ dropdown menu:Click ‘Options…’ button:check box: PDF/compliantWhen opening PDF/compliant PDF, PDF readers show warning message:Click ‘Next’. can now upload preregistration document. PsychArchives motivate add descriptions meta-data files.choose sharing level (level 0 means file public use, level 1 scientific use ) license. Click ‘Save Next’.Now see PsychArchives setting higher bar services, need specify meta-data file. meta-data make files share (preregistration, paper based preregistration, data code used paper) easily findable, important scientific community wants reap benefits open science future. preregistration yet linked files, real research project, future submissions PsychArchives linked preregistration. Adding good descriptions files also helps others find future.submit preregistration, see preregistration manually checked. Staff PsychArchives check whether submission adheres guidelines. , told improve, resubmit file. Note check uploaded files meta-data – staff check quality preregistration!","code":""},{"path":"prereg.html","id":"pre-registering-on-the-open-science-framework","chapter":"13 Preregistration and Transparency","heading":"13.7.3 Pre-registering on the Open Science Framework","text":"Go www.osf.io, create account log , click button 'Create new project'. Enter title. Europe, want adhere GDPR privacy regulations, make sure select 'Germany' storage location. Click 'Create', click 'Go project'.Add description, others understand project , license, others know can re-use material sharing.can choose make project public now, make public later. common researchers make project public upon publication, worried others re-using content (ideas ) can make project open immediately. , click Make public button top right project.preregister study, click 'Registrations' button top bar, click 'new registration'.Reminder: use OSF preregister just practice preregister. OSF registries website intended searchable database official scientific registrations preregistrations. complete registration, remain OSF Registries ever, way remove . costs money reduces usefulness database. register real studies.hypothesis testing research default OSF preregistration template provide useful structure preregistration. also allow upload supplementary files (html file detailing sample size justification created ).asked range relevant questions complete. completed preregistration related study comparing movie ratings movies starring Brad Pitt Edward Norton, see: https://osf.io/wk6xm.share preregistration review process, OSF allows create 'view-. link. long enter identifying information files project, peer reviewers able see preregistration, without revealing identity: https://help.osf.io/article/201-create--view--link---project.","code":""},{"path":"prereg.html","id":"pre-registering-on-aspredicted","chapter":"13 Preregistration and Transparency","heading":"13.7.4 Pre-registering on AsPredicted","text":"AsPredicted offers preregistration service focus simplicity. website allow preregistrations long. can typically accomplished removing justifications choices preregistration document. makes life easier people need read preregistration. AsPredicted focuses distinguishing exploratory confirmatory research, deviations analysis plan reduce severity test, think important make sure word limit limit ability peers evaluate whether changes original analysis plan increase reduce severity test. feel word limit constrains writing preregistration, can use AsPredicted template OSF preregister.Go https://aspredicted.org/ create new AsPredicted pre-registration clicking 'create' button. Fill name, e-mail, institution.Scroll , answer questions 1 11. 2) paste answer Q1, 3) paste answer Q2, 4) explain many groups compare (e.g., Edward Norton vs. Brad Pitt), 5) 6) enter answer Q4, 7) enter answer Q3. Answer remaining questions. Please indicate 10) using AsPredicted ‘Class project assignment’ want complete actual preregistration.Preview pre-registration, submit preregistration. added co-authors, need approve submission. AsPredicted gives option make anonymous PDF file peer review purposes, can just make preregistration public.share preregistration review process, AsPredicted allows download anonymous PDF. can see preregistration corresponding research question : https://aspredicted.org/nx35m.pdf","code":""},{"path":"computationalreproducibility.html","id":"computationalreproducibility","chapter":"14 Computational Reproducibility","heading":"14 Computational Reproducibility","text":"Technology greatly improved scientists work. internet made \neasy share information – including data, materials, code – new\nsoftware online platforms emerged facilitate workflow \nscientists (Spellman, 2015). One important goal scientific workflow make sure \nfinal work publish computationally reproducible. Computational\nreproducibility means use data published\narticle, can reproduce results. words, authors\npublished article send data code, able get\nexact numbers report article. Current research \ncomputational reproducibility scientific articles suggests often \npossible run original code data reproduce results. Sometimes\ncode simply run data, analyses part \ncode.However, computational reproducibility important, scholars \nable verify results, build results. consider\ncomputational reproducibility minimum standard workflow. However,\nmeeting standard requires training. PhD, often \nproblem known ‘data rot’. submitted article publication, \nreceived reviews several months, always easily reproduce\nanalyses. example, might stored dealt outliers,\nexactly reproduce original results. Sometimes, ‘data rot’ \neaten away either data analysis code, longer worked.Obviously, thing ‘data rot’. problem use \nreproducible workflow. chapter, learn computationally\nreproducible workflow looks like, can share computationally\nreproducible results published paper. goal applying \ncomputationally reproducible workflow projects allow\nsomeone else (, one year now) take data, run code,\nget exactly results reported work.Although multiple ways achieve fully reproducible workflow, \nchapter aim introduce believe might one emerging\nstandard fully reproducible workflow. example, learn\nwork version control system (GitHub, integrates nicely\nOpen Science Framework) programming R,\nstores previous versions files. learn write \ncompletely reproducible data analysis script(including figures),\ncan save HTML file PDF file, using RMarkdown.\nFinally, take look Code Ocean, novel online platform\nallows share computationally reproducible code online, making \nextremely easy others run (small variations ) code. \nlearn become experienced programmer end chapter, \nsee fully reproducible workflow look like, get initial\nexperience tools likely want explore future.Getting software code work system might challenge, \nregrettably, can’t offer ICT support. Differences Windows, Linux, \nApple operating systems means might need search internet \nsolutions problems run – normal, even experienced\nprogrammers time. get stuck, can check \ncode look like visiting public versions \npart example:GitHub repository: https://github.com/Lakens/reproducibility_assignmentOSF project: https://osf.io/jky8s/Code Ocean container: https://codeocean.com/capsule/2529779/tree/v1","code":""},{"path":"computationalreproducibility.html","id":"step-1-setting-up-a-github-repository","chapter":"14 Computational Reproducibility","heading":"14.1 Step 1: Setting up a GitHub repository","text":"haven’t created GitHub account , now. Go \nhttps://github.com/ create account. Git version control system \ntracking changes computer files coordinating work files among\nmultiple people. Version control allows track changes files revert\nback previous versions needed. GitHub GitLab web-based hosting\nservices make easier use version control Git. using\nGitHub familiar , integrates \nslightly tools, feel free use GitLab instead.account, can create new repository. repository \ncollection folders files make project. top-right \nGitHub page, click + symbol, select ‘New repository’ \ndropdown menu.first thing name repository. comes naming folders\nfiles, important follow best practices file naming:Keep names short, clear. data_analysis_project easier understand\nothers dat_an_prjctKeep names short, clear. data_analysis_project easier understand\nothers dat_an_prjctDo use spaces. Options include:use spaces. Options include:Underscore: this_is_a_file.R (personal favorite)Underscore: this_is_a_file.R (personal favorite)Camelcase: ThisIsAFile.RCamelcase: ThisIsAFile.RDashes: ---file.RDashes: ---file.RNo spaces: thisisafile.RNo spaces: thisisafile.RIf want number multiple sequential files, use 1_start, 2_end,\nuse leading zeroes whenever might number 10 files, \nexample 01, 02, etc., 001, 002, etc.want number multiple sequential files, use 1_start, 2_end,\nuse leading zeroes whenever might number 10 files, \nexample 01, 02, etc., 001, 002, etc.use special characters $#&*{}: file names.use special characters $#&*{}: file names.want use date information, use YYYYMMDD format.want use date information, use YYYYMMDD format.Let’s name repository: reproducibility_assignmentYou can add short description (e.g., ‘assignment practice \nopen reproducible data analysis workflow’). academic \nstudent, can get academic account, gives extra options, \nkeeping repositories private: https://education.github.com/packClick checkbox ‘Initialize repository README’. readme\nfile useful way provide detailed description project,\nvisible people visit GitHub project page.also asked whether want add license. Adding license way easily communicate others can use data, code, materials share GitHub repository. Note making choice license also choice: add license work exclusive copyright default, means others can’t easily re-use . can learn licenses, now, simple choice MIT license, puts limited restrictions reuse, restrictive licenses also exist. can select choice license (MIT license) dropdown menu. lets people anything want code long provide attribution back don’t hold liable. also creative commons\nlicenses can use sharing something else software, research materials (example, educational material shared CC--NC-SA 4.0 license).now ready create repository. ClickIt might feel unintuitive, important remember \nexpected directly interact new GitHub repository GitHub\nwebsite. repository page give information contents \nrepository, history files repository, \nparticularly easy add files download files directly website.\nidea use software interact GitHub repository.","code":""},{"path":"computationalreproducibility.html","id":"step-2-cloning-your-github-repository-into-rstudio","chapter":"14 Computational Reproducibility","heading":"14.2 Step 2: Cloning your GitHub repository into RStudio","text":"R Studio can communicate GitHub. allow RStudio work together \nGitHub, first need set system. detailed explanation \ndifferent operating systems provided\n.\nFirst, download Git: https://git-scm.com/downloads operating system,\ninstall (can accept defaults installation process). \nhaven’t done already, download install R:\nhttps://cran.r-project.org/, download install free version R\nStudio (scroll installers):\nhttps://www.rstudio.com/products/rstudio/download/.R Studio, go Tools > Global Options, select Git/SVN menu option.Check Git executable (“git.exe”) found automatically. ,\nneed click ‘ Browse…’ button find manually. \nalways location installed Git.Click ‘Create RSA Key…’ button. window appear:can close window. Still RStudio options, click blue\nhyperlink ‘View public key’. window appear, telling can use\nCTRL+C copy key. .Go GitHub, go settings select option SSH GPG keys:Click ‘New SSH key’Enter name (e.g., RStudio) paste key correct window. Click ‘Add\nSSH Key’. allow send code R Studio GitHub\nrepositories without enter GitHub login name password every\ntime. words, R Studio now connected GitHub account \nrepository. now ready create version controlled project R\nStudio.Restart RStudio. RStudio, go File>New Project:get three choices. Choose ‘Version Control’ option:Choose ‘Git’ option:cloning online GitHub repository created. Cloning term\nused Git means creating local copy files repository \ncomputer. can copy-paste URL GitHub repository (e.g.,\nhttps://github.com/Lakens/reproducibility_assignment). copy-paste \nURL top field, automatically create Project directory name \nsimilar name gave project GitHub. can select folder\ncomputer clicking ‘Browse’ button indicate want \nsave local copy repository.Click ‘Create Project’. R quickly download files repository,\nopen new project. see project creation successful\n‘Files’ tab RStudio interface shows downloaded \nfiles GitHub repository (README.md LICENSE files). RStudio also\ncreated .Rproj file .gitignore file. project file used \nstore information project, required use GitHub.can also see version control project top right \ninterface, now ‘Git’ tab. click , see:see range buttons, Diff, Commit, Pull, Push buttons.\nused interact GitHub. Many computer programmers interact\nGitHub command line, :$ git commit -m \"git commit message\"Learning use git command line needed people \njust want basic version control. , exclusively focus version\ncontrol git menu options RStudio. now time create \nfile want control versions make .","code":""},{"path":"computationalreproducibility.html","id":"step-3-creating-an-r-markdown-file","chapter":"14 Computational Reproducibility","heading":"14.3 Step 3: Creating an R Markdown file","text":"R Markdown files provide way save execute code, time\nallowing create reports data analysis (even full scientific\narticles can submit publication!). complete introduction R\nMarkdown available . \nmain strength R Markdown documents allow create fully\nreproducible document. means just analysis code\nR script, manuscript combines text code can\ncompile create PDF html version manuscript. HTML PDF files\nadvantage people can read regular software. R\nMarkdown file contains code performs analyses time document\ncompiled. Instead copy-pasting values analysis software \nword document, combine code text RMarkdown file create \nmanuscript every number figure can traced back exact code\ngenerated . advantage everyone can use RMarkdown\nfile generate document (e.g., manuscript) .can still make errors analysis use R Markdown files. \nimportant difference making programming errors \nstored R Markdown document. Compared typo copying numbers \nanalysis word document, errors analysis RMarkdown file\nlead document. document reproducible, errors\nreproducible well. impossible prevent errors, \npossible make reproducible. make easier identify \ncorrect errors. understand might worry others seeing errors \nallow see exactly done. make mistakes,\nimportant science able identify correct \nmistakes. important aspect moving reproducible workflow, \nsharing files underlying manuscript publicly, \nlearn accept make errors, appreciate people correct\n.Let’s start creating new R Markdown document R Studio clicking New\nFile > R Markdown…gives new window can specify title RMarkdown\ndocument author name. Enter title ‘Main Analysis’, feel free \nchange Author subfield anything prefer. RMarkdown files can \ncompiled (also referred ‘knitted’) HTML file, PDF document, \nword document. generate PDF files need install MiKTex won’t\nexample (good tutorial install MiKTeX available\n).\nleave default output format HTML click OK.Let’s start saving new file: Click save button, save file\nname ‘main_analysis.Rmd’. working R Studio\nproject, file automatically saved folder \nfiles project. look files tab bottom right pane,\nsee new file appear. Now let’s take look R Markdown file.R Markdown file default includes several sections get started.\nFirst, header section. header section, code \ndetermines final document rendered. section sensitive, \nsense needs programmed exactly right – including spaces tabs –\nrecommended change much without looking detailed\ndocumentation change section. want technical details:\nR Markdown file fed knitr software, creates normal markdown\nfile, uses pandoc software generate specific document \nrequested. happens automatically.header followed set-section can define general\noptions entire R Markdown file. , see two main sections:\nMarkdown code, markup language plain text formatting syntax\ncan easily converted HTML formats. , see R code\nused analyze data create figures. see final result \ncode, hit theKnit button toolbar top pane.Either new window appear allows view HTML file \ncreated, document appear ‘viewer’ tab RStudio. see\nformatted HTML document combined text output R code.Close window – now ready analyze data.","code":""},{"path":"computationalreproducibility.html","id":"step-4-reproducible-data-analysis-in-r-studio","chapter":"14 Computational Reproducibility","heading":"14.4 Step 4: Reproducible Data Analysis in R Studio","text":"Delete text ## R Markdown – keep header set-\nsections default document.First, need analyze data. download data directly \nexisting GitHub repository created. Students introduction psychology\ncourse performed simple Stroop experiment. Stroop, participants\nnamed colors congruent trial (e.g., word 'red' written red\nfont) incongruent trial (e.g., word 'red' written green font). \ntime took name words recorded seconds (e.g., 21.3 seconds)\ncongruent incongruent trial. four columns \ndataset:Participant NumberParticipant NumberResponse Time Congruent StimuliResponse Time Congruent StimuliResponse Time Incongruent StimuliResponse Time Incongruent StimuliYear Data CollectionYear Data CollectionClick button ‘+C Insert’ insert code – dropdown menu visible.\nSelect R.R Markdown file, ’ll see new section R code starts \nthree backticks followed {r} ends three backticks. can also just create sections \nmanually typing two lines.Copy-paste code – make sure get text – paste \nstart line end line R code chunk.copy-pasting text, code section look like :code creates data.frame called ‘stroop_data” contains data, \nsaves data .csv file called ‘stroop.csv’. Click Knit button look document:see something like:might look impressive – real action file pane \nbottom right part screen. Close window showing HTML output\nlook file pane. now see bunch files:One file stroop.csv – data file Stroop data \ndownloaded internet saved project folder, using R code.really need keep downloading file internet \ncan also just load local folder. let’s change code. won’t\ncompletely delete code – just comment placing # \nfront . way, can still remember downloaded file ,\nwon’t use code.always important provide comments code write,\nadd explanation line downloaded code:#run download dataThen, select lines code \nchunk, press (Windows) CTRL+SHIFT+C (click ‘Code’ toolbar\n‘comment/uncomment lines’). add # front lines,\nmaking comments instead code executed every time. end\n:Now need add line code run, load\nstroop.csv dataset local folder. Underneath last commented \nline code, within R code block, add:stroop_data <- read.csv(\"stroop.csv\", sep = \" \", header = TRUE)Click save, press CTRL+S, save file. Knit file. see:Close HTML file. ’ve done quite lot work. shame \nwork lost. seems perfect time save version R\nMarkdown file, just locally, also GitHub.","code":"stroop_data <- read.table(\"https://raw.githubusercontent.com/Lakens/Stroop/master/stroop.txt\",\nsep = \"\\\\t\", header = TRUE)`\n\nwrite.table(stroop_data, file = \"stroop.csv\", quote = F, row.names = F)"},{"path":"computationalreproducibility.html","id":"step-5-committing-and-pushing-to-github","chapter":"14 Computational Reproducibility","heading":"14.5 Step 5: Committing and Pushing to GitHub","text":"time store changes cloud, GitHub. process takes two\nsteps. First, record changes repository (aka code files \ncreated), called ‘commit’. require internet\nconnection, just recording changes locally. However, \nwant make sure recorded changes also stored GitHub, \nrequires push files GitHub.look Git tab top right pane R Studio, see Commit\nbutton, Push button, see bunch files. status files\nindicated two question marks yellow. question marks indicate\nfiles yet tracked GitHub. Let’s change .Click commit button. menu opens. can choose ‘stage’ changes\nmade. Staging basically mean selecting files \nwant record, commit. can several ways, double\nclicking file, selecting files clicking ‘Enter’. staging\nfiles, yellow question marks change green ‘’ symbol. Every commit\naccompanied commit message describe changes\nmade – , recording changes. can type \nanything want – common write something like ‘initial commit’ \nfirst time commit changes. menu look like screenshot :Now ready commit changes. Click ‘Commit’ button. new\nwindow opens shows changes committed. see 5\nfiles changed. can close window close previous menu.R Studio now reminds difference local copy \nrepository, remote version repository GitHub. Git\ntab see reminder: “branch ahead ‘origin/master’ 1 commit.”.means files updated recorded computer commit\nyet synchronized remote repository GitHub. can solve\n‘pushing’ (aka synchronizing) changes remote repository.\nSimply click push button:Another pop-window appears:window informs us errors, successfully pushed \nchanges remote version repository. can close window.can check successfully pushed files GitHub visiting \nGitHub page repository browser. see something like:Congratulations first GitHub push! want read extensive introduction Git, see Vuorre & Curley (2018).","code":""},{"path":"computationalreproducibility.html","id":"step-6-reproducible-data-analysis","chapter":"14 Computational Reproducibility","heading":"14.6 Step 6: Reproducible Data Analysis","text":"far, read data. goal R Markdown file create manuscript contains fully reproducible data analysis. chapter, teach analyze data R (can highly recommend learning – plenty excellent online resources). Instead programming scratch, visit raw text version R Markdown file analyze Stroop data. website, select text (CTRL+), copy (CTRL+C). go main_analysis.Rmd file R Studio. Select text (CTRL+) press delete. ’s right – delete everything. don’t need worry losing anything – version controlled file GitHub repository, means can always go back previous version! (now empty) main_analysis.Rmd file, press CTRL+V paste text. file look like first screenshot .R Markdown file number things, explain detail\n. example, automatically install libraries needs, load \ndata, create report HTML. can press Knit button, HTML\ndocument load. see output second screenshot .important note none numbers text \nstatic, copy-pasted. calculated moment document\ncreated, directly raw data. true figures, \ncreated raw data moment manuscript compiled. \naccess .Rmd (RMarkdown) file, can perfectly reproduce \nreported data analysis.Since made substantial changes, perfect moment commit\npush changes GitHub! Go Git tab top right pane.\nClick ‘Commit’. window open. main_analysis.Rmd file \nselected, see red green chunks text. tell old\n(red) new (green).Select files changed, ‘stage’ (example pressing\nenter). checkboxes front files, ‘Staged’ column, \nchecked.Type commit message, ‘update mean analysis’ ‘commit message’\nfield. Press ‘Commit’ button. Close window pops inform \nresult commit. click ‘push’. Close window informs\npush command, close commit window. can always visit\nGitHub repository online look full history document \nsee changes made.Let’s take look sections new R Markdown document. First \nheader:sets general (global) options code chunks R Markdown file. echo,\nwarning, message = FALSE hide code chunks, warning messages, \nmessages, ‘include=true’ make figures appear text. \ncan set variables TRUE, hit Knit see change.\nSometimes might want share HTML file code visible, \nexample sharing collaborators.scroll , can see introduction text, code generates\nfirst figure, code performs analyses. variables \nused Results section. Let’s look section:section shows can mix text R code. start code\nnormal text. *M* still normal text (* * make sure M italicized, just ~av~ indicates letters subscript), see R code. R Markdown can embed R code within `r`.\nR code within two backticks executed. case, mean\nCongruent reaction times calculated, rounded 2 digits. can\nsee number text.Learning program takes time. can see things quite tricky \nprogram. example, code:lot code make sure exact p-value reported, unless \np-value smaller 0.001, case ‘p < 0.001’ printed (papaja\npackage makes reporting statistics lot easier!). first time need\nprogram something like takes lot time – remember can re-use\ncode future, can steal lot code others! can\ncomplete full introduction Markdown\n.","code":"\nifelse(ttest_result$p.value > 0.001,\" = \", \" < \")\nifelse(ttest_result$p.value > 0.001, formatC(round(ttest_result$p.value,\n digits = 3), digits = 3, format = \"f\"), \"0.001\")"},{"path":"computationalreproducibility.html","id":"extra-apa-formatted-manuscripts-in-papaja","chapter":"14 Computational Reproducibility","heading":"14.6.1 Extra: APA formatted manuscripts in papaja","text":"want write reproducible manuscript APA style (common \nexample psychology) might want try R package\npapaja created Frederik Aust. Install \npapaja package. Restart RStudio., create new R Markdown document, \ninstead selecting document option, select ‘Template’ option, \nselect template APA article (6th edition) provided papaja package.see template lot fields fill , \ntitle, author names affiliation, author note, abstract, etc. Papaja\ntakes care information ends nice lay-– exactly\nfollowing APA rules. means installed MiKTeX (\nable convert PDF), can knit document pdf, submit APA\nformatted document completely reproducible. tutorial covering \noptions papaja, including add citations:\nhttps://crsh.github.io/papaja_man/index.html","code":""},{"path":"computationalreproducibility.html","id":"step-7-organizing-your-data-and-code","chapter":"14 Computational Reproducibility","heading":"14.7 Step 7: Organizing Your Data and Code","text":"important always organize data files analysis files. \nhelps others quickly find files looking . general, \nrecommend TIER protocol: https://www.projecttier.org/tier-protocol/.\nshare datafile slightly larger one example, make\nsure add codebook others understand variables mean. nice package can help generate machine readable codebook, see Arslan (2019).organizing code, take great care make sure personally identifying information data stored safely. Open science great, responsible share data responsibly. means need ask participants permission share data informed consent form (useful resource\nResearch Data Management Support page University Utrecht).\nWhenever collect personal data, make sure handle data responsibly. Information specialists university library able help.","code":""},{"path":"computationalreproducibility.html","id":"step-8-archiving-your-data-and-code","chapter":"14 Computational Reproducibility","heading":"14.8 Step 8: Archiving Your Data and Code","text":"Although uploaded data code GitHub, publish article want share data code, important remember GitHub data repository guarantees long term data storage.\nGitHub currently owned Microsoft, companies can choose free service whatever want. makes less suitable link GitHub scientific articles, articles around decades now. scientific publications, want link stable long-term data repository. list data repositories, click .\nuse Open Science Framework (OSF) example stable data storage, easy just integrate GitHub repository within OSF project.Log OSF https://osf.io/ (create account haven’t already done ). Click ‘Create new project’. Give project name (example ‘Stroop Reproducible Analysis Assignment’).important add license work, also OSF project. created project, can click ‘Add license’:can choose MIT license. fill year, copyright holders – case, , fill name (appear license text). clickAlthough upload files OSF, can also simply link \nGitHub project OSF. menu bar OSF project, click \n‘Add-ons’. list, scroll GitHub:Follow step--step guide connect GitHub provided OSF:\nhttps://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub---ProjectSelect repository contains reproducibility assignment click\n‘Save’.Click title OSF project page go back main project page. \nnow see ‘Files’ pane GitHub repository linked:good moment click ‘Make Public’ button top right \nproject. making project public, people able find \nOSF. don’t want make project public just yet, want \ngive others access files, can create ‘View-’ link \nOSF. Go ‘Contributors’ click +Add button next View-links. \nstep--step guide, see \ntutorial.can use view-link share access files reviewers. can create anonymized view-link hide contributor names project - particularly useful blinded peer review. Giving access files peer review greatly helps reviewers – lot easier answer questions might materials, data, code. aware means people don’t know access files. far, don’t know negative experiences process, important aware others access files published.OSF page now just links files GitHub page. independently store . means yet long term stable data storage solution.create snapshot files GitHub repository stored long time, create Registration project. create Registration project example. Creating registration starts several formal procedures: data linked repositories (GitHub) stored OSF, project appears list registrations. register want create stable copy work. see example files OSF project registered. see GitHub repository linked project turned Archive GitHub – creates stable version project, moment registered.good moment create stable version project manuscript accepted publication. can create Registration use Digital Object Identifier (DOI) link code, data, materials paper (can add link DOI manuscript check proofs article published). Note recommended link materials using DOI. DOI persistent link (meaning keep working) website address might change. registration automatically get DOI. creating Registration, need click ‘Create DOI’ link create persistent object identifier.ready create Registration, follow instructions OSF: https://help.osf.io/article/158-create--preregistration. example Registration made store work related one scientific publications, see https://doi.org//10.17605/OSF.IO/9Z6WB (link example link OSF project using DOI).","code":""},{"path":"computationalreproducibility.html","id":"extra-sharing-reproducible-code-on-code-ocean","chapter":"14 Computational Reproducibility","heading":"14.8.1 EXTRA: Sharing Reproducible Code on Code Ocean","text":"used workflow create reproducible manuscript, might want make easy people explore data code. People can simply clone GitHub repository – still requires install software used, even R installed, requires install packages used analyze data. can potentially lead reproducibility problems. Packages R update change time, code works one machine might run well another computer. Furthermore, even shared perfectly, downloading files getting code running takes time. Several solutions exist, \nPackrat, dependency management system R, Docker, can create container works virtual machine includes computing environment including libraries, code data need reproduce analysis (Wiebels & Moreau, 2021)., ’ll focus software solution designed easier use Docker, provides many benefits, called Code Ocean. Code Ocean cloud-based computational reproducibility platform. can create computing capsule runs online contains packages code needs run. Although Code Ocean (yet) guarantee long term storage data code, interesting way make reproducible code available fellow researchers, makes easy researchers (reviewers) make small changes code, examine results. Create (free) account CodeOcean. Go dashboard.\nClick ‘New Capsule’ button choose option ‘Import Git Repository.Enter web address GitHub repository click import.Click ‘Environment’ left pane. middle pane, click R icon \nselect programming language. time writing, default language \n3.5.3, can click ‘2 versions’ select R 3.6.need set Code Ocean environment contain packages need.\nClick +Add behind apt-get, type pandoc click return key two times\n(need specify version pandoc). Click +Add behind R (CRAN) type \nggplot2, click return twice (need select specific version). Click\n+Add , type reshape2, click return twice. Click add , type\nrmarkdown click return twice.left pane, drag drop main_analysis.Rmd file ‘code’ folder, ‘stroop.csv’ file ‘data’ folder. Select code folder, click + button bar top left pane add \nfile. Name file ‘run.sh’. file tell Code Ocean run. Select file. middle pane empty. Add following code:\\#!/bin/bashRscript -e \"rmarkdown::render(input = 'main_analysis.Rmd', \\\\output_dir = '../results', clean = TRUE)\"need make one final change. Select main_analysis.Rmd. Scroll \nline 28 data read . Change:stroop_data \\<- read.csv(\"stroop.csv\", sep = \" \", header = TRUE)tostroop_data \\<- read.csv(\"../data/stroop.csv\", sep = \" \", header = TRUE)done! Click ‘Reproducible Run’ button right pane. \nget output:Click ‘main_analysis.html’. see script generated \nreproducible results. Commit changes. (principle – \nexample won’t) make completely reproducible container openly\navailable share published paper.now completely reproducible analysis file online. Anyone just\nreproduce data analysis – can go main_analysis.Rmd file, \nchange anything want code, run . example, let’s say\ndislike black straight line first scatterplot, want \nred. easy change ‘black’ ‘red’ line 42, re-run \nanalysis, get figure red line. Although might \nexciting, ability easily re-analyze data might \nuseful realistic scenarios. example, imagine reviewing \npaper researchers plot data. Without install \nsoftware, can just type hist(stroop_data$Congruent) data \nread (e.g., line 30), run code , see \nhistogram reaction times Congruent condition. Give try.","code":""},{"path":"computationalreproducibility.html","id":"some-points-for-improvement-in-computational-reproducibility","chapter":"14 Computational Reproducibility","heading":"14.9 Some points for improvement in computational reproducibility","text":"recently tried computationally reproduce Registered Reports published psychological literature (Obels et al., 2020). noticed issues , solved, easily improve computational reproducibility work.First, always add codebook data files. already noted , yes, bit work fun , essential include codebook share data. Data easier understand reusable variables values clearly described. Researchers ensure codebook variable names language article.Second, annotate code clear code . Well-annotated code makes clear analysis code , order scripts run multiple scripts (e.g., pre-process raw data, compute sum scores, analyze results, generate graphs), output section analysis code generates. Sometimes might even helpful , final manuscript, copy-paste sentences results section back code file, clear sentences manuscript relate code file. also helps clearly structure code (e.g., using README) others know output analysis code creates order code run.Third, check whether code shared still reproduces analyses revisions - researchers often make changes peer review process, forget update analysis files.Finally, remember code R relies specific libraries (also called packages). List packages code needs run top script. packages update, necessary report version numbers packages used (example using packrat, copying output sessionInfo() function comment script). Remember folder names folder structures differ computers, therefore use relative locations (absolute paths like “c:/user/myfolder/code”). RStudio projects '' package provide easy way use relative paths. multiple scripts used analysis, include order scripts performed data README file.","code":""},{"path":"computationalreproducibility.html","id":"conclusion-1","chapter":"14 Computational Reproducibility","heading":"14.10 Conclusion","text":"chapter, used number platforms software solutions, GitHub, Open Science Framework, R Studio, R, R Markdown. Following example able use tools research. Learning use tools take time. many frustrations code software doesn’t work want, gotten local remote GitHub repositories much sync just need delete everything local computer re-download files GitHub (git reset --hard [HEAD] friend). lot resources available online find answers, ask help. experience, work, doable, even limited knowledge programming. can get basic reproducible workflow running simply using steps described , learn new skills need . Learning skills appreciated within outside academia (useful PhD students) quickly save time (e.g., recreating figures revision, analyzing similar datasets future). reproducible workflow also improves quality scientific work, makes easier scientists re-use work future.another open educational resource making scientific research accessible reproducible, see Open Science Manual Claudio Zandonella Callegher Davide Massidda.","code":""},{"path":"integrity.html","id":"integrity","chapter":"15 Research Integrity","heading":"15 Research Integrity","text":"research, important guided responsible conduct research, colloquially, good research practices. Good research practices professional standards goal maximize quality reliability research. abstract level beliefs good research practices change substantially time. practice implementation good research practices changes function social, political, technological developments. example, increasingly seen good research practice share data underlying research report. difficult internet, become much easier now free data repositories exist online. consequence, increasingly see research funders expect data collected grants open whenever possible.distinction made research integrity research ethics. Research integrity set principles based professional standards. Research ethics set moral principles, autonomy, beneficence, non-maleficence, justice (Gillon, 1994). principle autonomy leads research practices informed consent, requirement truthful participants, confidentiality. principle non-maleficence follows researchers avoid research harms participants, research burdensome (Varkey, 2021).professional standards Codes Conduct Research Integrity vary slightly documents (Komić et al., 2015). chapter, discuss European Code Conduct Research Integrity, Netherlands Code Conduct Research Integrity. Throughout chapter, code conduct research integrity abbreviated 'code conduct'. might adhere codes conduct, depending work.European code conduct states, “basic responsibility research community formulate principles research, define criteria proper research behaviour, maximise quality robustness research, respond adequately threats , violations , research integrity.” Codes conduct always living documents, believe ‘proper research behavior’ changes time. certain core principles see underlying codes conduct research integrity, honesty, transparency, scrupulousness, accountability, reliability, respect, independence. underlying principles translated specific behaviors considered proper behavior – researchers, research institutions. ever read code conduct? , institution already violation code conduct, responsibility “develop appropriate adequate training ethics research integrity ensure concerned made aware relevant codes regulations”.Dutch Code Conduct Research integrity states \"Conduct research can scientific, scholarly /societal relevance\". course, might perform study purely educational purposes, study perform additional value (although always nice ). researchers prevent research waste, perform studies little value. Chalmers Glasziou (2009) discuss four sources research waste: Choosing wrong questions research, studies unnecessary, poorly designed (need evaluate value information collect, explained chapter sample size justification), failure report research promptly (explained chapter bias), biased unusable reports research (can prevented reporting study can included future meta-analysis). Dutch code conduct also explicitly states researchers \"Make sure research design can answer research question\". can see, many topics discussed textbook relate preventing research waste, thereby related research integrity.discussed chapter pregistration transparency researchers share data possible, also code conduct: \"far possible, make research findings research data public subsequent completion research. possible, establish valid reasons non-disclosure.\" discussed , General Data Protection Regulation (GDPR) requires European researchers ask permission share data collected. Old informed consent forms question, even often stated data destroyed several years data collection. good example updated professional standards, nowadays, much common expect data available alongside published article perpetuity. therefore want make sure use updated consent forms allow share data collect.Younger researchers sometimes feel supervisors require act ways line code conduct. young researchers go along pressures, others explicitly say willing violate code conduct get closer completing PhD (van de Schoot et al., 2021). Others trust supervisors know right thing , even though supervisors might feel forced act ways violate code conduct managers. surprisingly, pressuring people power violate code conduct violation code conduct. example, Netherlands code conduct states “supervisor, principal investigator, research director manager, refrain action might encourage researcher disregard standards chapter”.researchers noted hypercompetition science research grants, well researchers individually rewarded number published articles, can lead unethical behavior (M. S. Anderson et al., 2007; Edwards & Roy, 2017). Netherlands code conduct stresses importance creating open, safe, inclusive research culture researchers can discuss pressures, well guarantee good research practices always followed. want report discuss suspected irregularities perceive violation code conduct, universities typically internal external confidential advisors can reach , sometimes even possible report suspicions completely anonymously services SpeakUp. highly recommend, scientific integrity well well-, discuss problematic behavior encounter people can trust, confidential advisor.problems researchers violating code conduct right thing always easiest thing . Violating code conduct can come immediate individual rewards, higher probability publishing paper high impact journal, comes long term collective costs reliability scientific research, can also impact public's trust science (Anvari & Lakens, 2018; Wingen et al., 2020). Social scientists might recognize situation social dilemma, best individual aligned best collective. Changes incentives structures can perhaps align individual collective rewards. One way find punish researchers knowingly violate code conduct (example, see story Brian Wansink). New bias detection tests p-curve z-curve analysis can also used identify researchers systematically used questionable research practices (discussed next section). end, even though might sound idealistic, believe scientists put science first. pursue career science public university paid tax money generate reliable knowledge. Nothing pursuing additional goals, successful career, get way responsibility society trusted , generating reliable trustworthy knowledge.","code":""},{"path":"integrity.html","id":"QRP","chapter":"15 Research Integrity","heading":"15.1 Questionable Research Practices","text":"Although theory researchers follow code conduct research integrity, many researchers . Researchers across scientific disciplines admit certain practices dubbed ‘questionable research practices’. name somewhat unfortunate, practices questionable , directly violate code conduct. nevertheless referred ‘questionable’ mainly many researchers aware problematic nature practices, slowly needed accept problematic always .Questionable research practices generally describe practices violate requirement code conduct \"Make sure choice research methods, data analysis, assessment results consideration possible explanations determined non-scientific non-scholarly (e.g. commercial political) interests, arguments preferences.\" additional commercial political interests, many scientists interest publishing scientific articles, good career. Questionable research practices make easier researchers publish article, either increase probability able report statistically significant results, hide imperfections, makes results seem convincing . practices come expense truth.Researcher admit engaging questionable research practices, depending community researchers surveyed, several problematic practices engaged least many scholars. Figure 15.1 summarizes results 14 different surveys (Agnoli et al., 2017; Bakker et al., 2021; Chin et al., 2021; Fiedler & Schwarz, 2016; Fraser et al., 2018; John et al., 2012; Latan et al., 2021; Makel et al., 2021; Moran et al., 2022; Motyl et al., 2017; Rabelo et al., 2020; Swift et al., 2022). However, coding open ended questions suggest substantial measurement error participants answer items, unclear whether percentages Figure 15.1 directly translate percentage researchers actually engaging questionable practices (Motyl et al., 2017).Many researchers selectively publish results analyses significant results, despite Dutch code conduct stipulating researchers \"justice research results obtained.\" European code conduct stating \"Authors publishers consider negative results valid positive findings publication dissemination.\" Registered Reports important step aligning research practices code conduct comes publishing null results.Researchers also flexibly analyse data selectively reporting conditions, measures, covariates, host data analytic strategies inflate Type 1 error rate, increase probability obtaining statistically significant result. Preregistration important step increasing transparency data-driven choices analyses reported scientific articles, allows researchers evaluate whether deviations statistical analysis plan decrease severity test, increase (Lakens, 2019). increasing awareness problematic nature practices, hopefully see strong decline occurrence, researchers learn correct approaches maintain flexibility analyses (example replacing optional stopping sequential analysis. Wigboldus & Dotsch (2016) make important distinction questionable research practices, questionable reporting practices. Whenever doubt, transparently reporting decisions made analyzing data give researchers information need evaluate reported results.\nFigure 15.1: Self-admittance engaging questionable research practices least 14 surveys among variety samples researchers .\n","code":""},{"path":"integrity.html","id":"fabrication-falsification-and-plagiarism","chapter":"15 Research Integrity","heading":"15.2 Fabrication, Falsification, and Plagiarism","text":"Beyond questionable research practices, fabricating data making results recording real, falsification manipulating manipulating aspects research, including data, without scientific justification. Data fabrication research practice outright dishonest. substantial number cases researchers fabricated complete datasets dozens experiments. examples already mentioned chapter bias detection. can difficult prove fabrication, researchers often keep bad records data collection. example, Susannah Cahalan makes convincing case book 'Great Pretender' famous study David Rosehan ‘sane insane places’ largely made . study, healthy confederates pretended hear voices admitted -patients suffering schizophrenia. detailed investigation raises severe doubts study performed described (see also Scull (2023)).One might hope falsification fabrication rare, recent large scale survey Netherlands yielded prevalence estimates around 4% (Gopalakrishna et al., 2022). Data fabrication can also occur smaller scale. Imagine collecting data study. part study, task ask age participants gender, demographic statistics reported describing sample. collecting data, notice forgotten collect demographic data two individuals. might tempted , based memory, guess demographic statistics two individuals, admit made mistake data collection wrote demographic information. However, also constitute data fabrication. instead transparently mention mistake made. Mistakes happen, important create culture people can admit mistakes, can learn prevent future (Bishop, 2018).Note can fine simulate data perform power analysis – one just present data collected real participants. Dutch code conduct states: “fabricate data research results report fabricated material fact. justice research results obtained. remove change results without explicit proper justification. add fabricated data data analysis.”European code conduct defines plagiarisms : using people’s work ideas without giving proper credit original source, thus violating rights original author(s) intellectual outputs.\" possible re-use text, source cited, quotation marks used identify text quote another source. special case plagiarism 'self-plagiarism' text recycling text author used different articles. disagreement problematic practice (Bird & Sivilotti, 2008), expected, always academics diverging opinion. general, researchers supposed re-use large portions previous work present new work just increase number published articles. many researchers believe perfectly fine re-use descriptions method sections need communicate information new paper (Pemberton et al., 2019). guidelines Committee Publication Ethics (COPE) similarly state:guidelines cover deal text recycling submitted manuscript published article include situations text recycling may acceptable well unlikely . example, may entirely appropriate overlap methods section research article (referring previously used method) citation original article. However, undisclosed overlap, overlap results, discussion, conclusions unlikely acceptable.Self-plagiarism thus mainly seen problematic researchers use publish similar content multiple times purely make look like productive.additional problematic research practices beyond fabrication, falsification, plagiarism, QRP's. Gopalakrishna et al. (2022) also considered behavior insufficiently mentoring supervising junior researchers, unfairly reviewing articles grants, inadequate note-taking research done questionable practices.","code":""},{"path":"integrity.html","id":"informed-consent-and-data-privacy","chapter":"15 Research Integrity","heading":"15.3 Informed consent and data privacy","text":"collecting data participants outside naturalistic observations public space, consent participate research. consent form participants read sign data collection important research ethics, data privacy. consent form explains goal study, highlights participation voluntary participants can stop want, explains risks benefits (payment), informs data privacy issues, details participants can contact issues study.Consent also legal basis use personal data General Data Protection Regulation (GDPR). consent form identify data controller contact details Data Protection Officer, description participants' rights (e.g., withdraw data certain amount time study), information long data stored shared. According GDPR special categories personal data can collect informed consent, racial ethnic origin, political opinions, religious philosophical beliefs, genetic biometric data data, questions person’s sex life sexual orientation. collecting data, necessary research purpose. Data privacy officers university can assist process.Open data important - essential maintain data privacy sharing data public repository. means carefully removing personal identifiers (names, IP addresses, ID numbers participants data panels, etc) dataset publicly sharing data. use version control system make sure identifying information absent initial version data files share, users just access latest version file, also complete file history. good overview GDPR research, see information Groningen University.","code":""},{"path":"integrity.html","id":"conflicts-of-interest","chapter":"15 Research Integrity","heading":"15.4 Conflicts of Interest","text":"conflict interest research situation researcher interests outcome research may lead personal advantage can get way generating true knowledge. central feature conflict interest two competing interests: one good research, one failing good research. example, researcher might receive additional income consultant company, working study evaluates product company produces, novel drug. study shows drug benefits compared existing drugs, researcher might worry honestly communicating research finding make company decide longer hire services consultant. researcher might work advocacy organization perform study topic examines many people impacted topic, high estimates might interest advocacy organization. argument can made scientists conflict interest whenever publish scientific paper, publishing good career scientist, studies easier publish whenSimply conflict interest violation code conduct, long researchers transparent . European code conduct states: \"authors disclose conflicts interest financial types support research publication results.\" Conflicts interest can also emerge review scientific work peers (e.g., grant proposals, scientific articles). , personal relationships can become conflict interest, either close friends researcher, feel researcher rival competitor. situations, declare conflict interest, editor grant review panel typically try find another reviewer.","code":""},{"path":"integrity.html","id":"ethics","chapter":"15 Research Integrity","heading":"15.5 Research ethics","text":"perform research institutions require obtain permission ethical review board (ERB), sometimes called institutional review board (IRB). Specific types research might reviewed specialized boards. example, medical research reviewed medical ethics review committee (METC), animal research animal ethics committee. goal ethics review balance two goals: protect subjects enable research benefit society (Whitney, 2016). Declaration Helsinki provides important basis evaluation research human subjects. highlights right individuals self-determination right make informed decisions whether want participate stop participating research.Declaration Helsinki builds Nuremberg Code, set ethical principles developed second world war response unethical research Nazi doctors performed unconsenting prisoners concentration camps (ethical discussion whether unethical research used cited, see Caplan (2021) Moe (1984)). Another example unethical experiments human subjects Tuskegee syphilis study 400 African American men syphilis included study examine effects disease untreated. men included study give consent go untreated receive diagnosis. study ended continuing 40 years.Although studies performed universities much lower risk harm, still important evaluate possible harm participants benefits science. Researchers might show negative stimuli, ask participants remember events experienced negative, can still experienced harmful. might equally effective alternatives can used designing study, still allow researcher answer research question. addition preventing harm, researchers must inform participants study ask consent participate. information informed consent truthful. necessary lie participants informed consent study perform (example, participants believe interact participants, people actually confederates part study) explained data collection completed debriefing. Researchers also maintain confidentiality participants. Take special care collecting open questions plan share data public repository.","code":""},{"path":"integrity.html","id":"test-yourself-12","chapter":"15 Research Integrity","heading":"15.6 Test Yourself","text":"Q1: Try define ‘data fabrication’ single sentence. Start sentence ‘Data fabrication process ’. definition cover forms data fabrication dishonest, cover honest processes, simulating datasets.Q2: Imagine analyzing data, one participant entered age 117 text-entry question experiment performed behind computer. Although impossible age, perhaps likely participant intended enter value 17. change value 17? Now imagine measured amount time (seconds) people browse website using system clock computer, extremely accurate, time measurement perfectly reliable. experimental condition, control condition. statistically significant difference two groups. However, change data one participant control condition 117 seconds 17 seconds, difference groups statistically significant, confirms prediction made designing study.difference two situations? second recoding 117 7 violation code conduct research integrity, according quote Netherlands Code Conduct Research Integrity three paragraphs question? write average age participants changed age one participant 117 17, need provide addition statement ‘mean age participants 20.4’ number based data changed?Q3: practice sometimes reporting results, times reporting results referred selective reporting. comes selective reporting, intention researcher matters. might make sense report study flawed (e.g., programming mistake experiment, participants misunderstood instructions provided useless input). might also make sense extensively report study badly designed – example, thought manipulation specific effect, manipulation work intended. However, even data might useful others, knowledge manipulation thought specific effect effect might prevent others future making mistake. least sometimes beneficial science results shared way. , see , researchers also choose selectively report studies based whether results statistically significant .scientist performs several experiments, shares results experiments , looking results, yield outcome supported predictions. scientist never shares results experiments fail support predictions. morally acceptable unacceptable think actions scientist ?Q4: scientist performs several experiments, shares results experiments , looking results, judged well-designed. scientist never shares results experiments , looking data, judged badly designed. morally acceptable unacceptable think actions scientist ?Q5: scientist performs one experiment several dependent variables analyzed multiple ways, shares results analyses , looking results, yield outcome supported predictions. scientist never shares results analyses fail support predictions. morally acceptable unacceptable think actions scientist ?Current practice researchers selectively report studies. Franco et al. (2014) examined happened 106 studies part large collaborative national representative survey, found results yielded non-significant effects, 31 studies written , 7 written published yet, 10 published. results showed strong (statistically significant) effects, 4 written , 31 written yet published, 56 published. clear evidence researchers selectively report results confirmed hypotheses, discussed chapter bias.recent study Pickett Roche (2017) examined public perception data fabrication, selective reporting. results summarized table . can see, selective reporting judged morally unacceptable large proportion public (71% believe morally unacceptable), majority public thinks consequences done (e.g., 73% believe researchers receive funding ban). percentages study Pickett Roche reflect judgments morally acceptable unacceptable selective reporting ?\nFigure 15.2: Table Pickett Roche (2017) showing judgments moral selective reporting1 data fraud eyes members general public.\nQ6: Assuming results observed Pickett Roche, well results studies questionable research practices summarized Figure 15.1 accurate representative, seems large divide current research practices, general public think morally acceptable. think divide problematic? think general public perfectly aware current practices related selective reporting, reason evaluate ways scientists work negatively, think good explanation current practices, general public evaluate current practices positively?Q7: Given researchers admit using questionable research practices, must benefits. benefits using questionable research practices?Q8: downsides using questionable research practices?improve research practices, seen many scientific fields move towards greater transparency. includes sharing data materials, clearer reporting choices made data analysis, pre-registering planned studies. almost impossible prevent fraud, making research transparent make easier detect questionable research practices, selective reporting. time, universities need train people research ethics, make sure climate researchers (including !) feel comfortable right thing.","code":""},{"path":"integrity.html","id":"grade-yourself","chapter":"15 Research Integrity","heading":"15.7 Grade Yourself","text":"assignment, grade . able check suggested answers (indication good answer, although exhaustive – answer might highlight correct important points mentioned answers ). Read answers determine grade answers. Use grading 1 (bad answer) 10 (excellent answer). truthful just.Answer Q1: Data fabrication process data generated can pass real data, based real underlying observations actually made researcher. data nevertheless presented based real observations.Score 1 (answer) 10 (perfect answer) points. grade higher, better indicated fabricated data look similar real observations, intentionally presented real.Answer Q2: difference two cases second case, researcher intention generate outcome line outcome want observe. terms quote Netherlands Code Conduct Research Integrity, missing “explicit proper justification”. need provide report average based 17 instead 117 footnote statement indicating (‘changed one age value 117 17’) justification (‘strongly suspected value type participant actually 17 years old’).Score 1 (answer) 10 (perfect answer) points. grade higher, aspects answer provided (explaining difference two cases based absence proper justification, specifying aspect Netherlands Code Conduct Research Integrity missing second case, need describe changed, justification changing .Q3, Q4, Q5, Q6 personal opinion, graded.Answer Q7: 1) biased towards presenting support hypothesis world, 2) much strongly rewarded career publishing results ‘work’ null results, thus spend time former, 3) even researchers try publish results, journals less likely accept publication, 4) easier publish paper coherent story (significant results). general, can expect benefits questionable research practices individual scientists short run.Score 1 (answer) 10 (perfect answer) points. grade higher, reasons provided, including, limited , three .Answer Q8: individual scientist, risk colleagues find , lose prestige (extreme cases, job). Failures replicate work might also impact prestige. society, downside scientific research reliable . science, downside reputation science, trust people place science, damaged. general, can expect costs questionable research practices society long run.Score 1 (answer) 10 (perfect answer) points. grade higher, reasons provided, including, limited\n, three ., work research integrity feel like need something cheer , video might help.","code":""},{"path":"glossary.html","id":"glossary","chapter":"16 Glossary","heading":"16 Glossary","text":"Progressadaptive designsadversarial collaboration: good-faith effort resolve scientific debates jointly carrying research. Originally proposed Mellers, Hertwig, Kahneman 2001 substitute common approach publishing commentaries rejoinders articles. Researchers expected clearly express others theoretical views, collectively design studies test diverging predictions, publish results studies performed. Sources: Mellers, B., Hertwig, R., & Kahneman, D. (2001). frequency representations eliminate conjunction effects? exercise adversarial collaboration. Psychological Science, 12(4), 269–275. https://doi.org/10.1111/1467-9280.00350alpha level: threshold chosen Neyman-Pearson hypothesis testing distinguish test results lead decision reject null hypothesis, , based desired upper bound Type 1 error rate. alpha level 5% commonly used, alpha levels can used long determined preregistered researcher data analyzed.alpha spending function: specification total alpha level distributed across multiple looks data sequential design.-priori power analysis: calculation sample size required achieve desired statistical power (Type 2 error rate) testing hypothesis specific statistical test, given alpha level effect size interest.auxiliary hypotheses: Premisses assumptions taken granted relied upon testing hypothesis. negative test result therefore means hypotheses one auxiliary hypotheses must false. Source: Hempel. (1966). Philosophy Natural Science. Pearson.Bayes factor: Bayes factor measures strength evidence one model (e.g., null hypothesis) relative another model (e.g., alternative hypothesis); ratio probabilities densities observed data two models amount one’s belief one hypothesis versus another change collected data .bias-variance tradeoffcapture percentagecausal enquiry: Establishing cause B, assessing whether cause B.ceteris paribusCohen's d: standardized mean difference effect, computed dividing mean difference standard deviation, allows effect size compared across different measures.common language effect size: Also known probability superiority, common language effect size percentage expresses probability randomly sampled person one group higher observed measurement randomly sampled person group (designs) (within-designs) probability individual higher value one measurement .compromise power analysis: compromise power analysis sample size effect fixed, error rates test calculated, based desired ratio Type Type II error rate.computational reproducibility: Computational reproducibility means anyone can recreate reported results (test results, tables, figures) basis available data, analysis, necessary files.confidence interval: interval around estimate , long run, capture population value desired percentage time.confirmatory resultcorroboratecredible intervalcritical effect size/minimal statistically detectable effectcrud factorcross-validationdegrees freedomdirectional test: statistical test null hypothesis consists values smaller specific value (e.g., x ≤ 0) alternative hypothesis consists values opposite direction (e.g., x > 0). Also referred one-sided test.effect sizeequivalence testing: statistical procedure reject hypothesis data extreme extreme effect interest. Equivalence tests TOST (two-one-sided tests) procedure can used falsify claim effects exist large enough matter.estimation approachempirical researchevidenceexploratory resultfalse negative: decision error null hypothesis rejected, even though true effect population.false positive: decision error null hypothesis rejected, even though true effect population.false positive report probability/false positive riskfalsifiability: extent claim can disproven, falsified. falsifiability claims essential requirement philosophies science based methodological falsificationism.fail-safe nfile-drawer problemFisherian approachfollow-bias: term used indicate power analyses use effect size estimates pilot studies performed effect size close zero (lead infeasibly large sample sizes), average power analyses based effect sizes pilot studies biased, returning small sample sizes, leads underpowered studies.funnel plotHARKing: Hypothesizing results known. practice developing hypothesis looking data, presenting hypotheses developed looking data. HARKing leads tests , unbeknownst naive reader, lack severity.heterogeneityhighest density intervalhypothesis testinconclusiveinformation qualityinformed consentinstitutional review boardinterim analysisintersection-union testing approachinterval hypothesis/range predictionlikelihoodlikelihood approachLindley's paradox: statistical fact possible reject null hypothesis (e.g., p < 0.05) data provide stronger evidence (e.g., indicated likelihood ratio) null hypothesis alternative hypothesis.maximum likelihood estimatormeta-analysismeta-regression: application regression models meta-analysis estimate properties underlying model generating distribution effect sizes, mean effect size, degree funnel plot asymmetry. Examples include Egger's regression PET-PEESE.minimal statistically detectable effect: smallest effect size , observed, lead rejection null hypothesis.minimum effect test: statistical hypothesis test value tested null effect, smallest effect size interest.model comparisonNeyman-Pearson approach: approach statistical inferences observed data used make decisions rejection non-rejection hypotheses controlling maximum error rate.nil null modelnull modelnumber needed treatOpen science: set practices reproducibility, transparency, sharing, collaboration based openness data tools allows others reuse scrutinize research.optional stoppingPET-PEESEp-curve analysispopulationpositive predictive valuepost-hoc power analysispractical significancep-value: probability observed data, extreme data, null hypothesis true. lower p-value, higher test statistic, less likely observe data null hypothesis true.precisionprediction intervalpredictive validitypreregistration: practice registering properties study online database (can made) publicly accessible. Can used communicate study performed (common practice randomized controlled trials) communicate properties study (hypotheses, experimental design, statistical analysis plan) planned researchers access data.probabilityprobability density function: probability density function (pdf) function completely characterizes distribution continuous random variable. provides likelihood value random variable fall certain range values. Source: https://www.statlect.com/glossary/probability-density-functionprobability superiority:publication biasrandomized controlled trialrange predictionregistered reportreplication studyresearch ethicsresearch integrityresearch misconductresearch wasteROPE procedure: estimation based approach posterior distribution compared region practical equivalence determine whether data close enough absence meaningful effect, conceptually similar equivalence testing.samplesample selection biassensitivity power analysis: Computation statistical power achieved given chosen alpha level sample sizesequential analysis: Repeatedly testing hypothesis interim analyses data collected controlling Type 1 error rate across analyses performed.severity: extent claim well-probed, severely tested. Data support claim extent test claim high probability corroborating claim claim false.significant/statistical significancesimulation studysmall telescopes approachsmallest effect size intereststatistical powerstatistician’s fallacystudy registrytrim--fill: trim fill method aims estimate number studies missing meta-analysis due bias. Tim--fill aims correct funnel plot asymmetry trimming (removing) smaller studies, estimating true effect size, filling (add) studies assumed missing due bias. method based nonparametric data augmentation technique. addition, trim--fill method aims adjust effect size estimate meta-analysis based augmented effect sizes. adjustment adequately correct bias, method can therefore used calculate 'bias corrected' effect size estimate. Trim--fill can used identify presence bias leads funnel plot asymmetry. Trim--fill developed Duval Tweedie (2000a, 2000b). Sources: https://handbook-5-1.cochrane.org/chapter_10/10_4_4_2_trim_and_fill.htm https://www.metafor-project.org/doku.php/plots:funnel_plot_with_trim_and_filltwo one-sided tests (TOST) procedureType 1 error rateType 2 error ratetrue negative: correct decision null hypothesis rejected true effect population.true positive: correct decision null hypothesis rejected true effect population.union-intersection testing approachverisimilitudeversion control: systems changes data (code text) systematically recorded. Version control makes possible go back previous versions identify changes introduced, prevents multiple individuals working code accidentally overwriting 's changes. Subversion (SVN) git two popular centralized version control systems.z-curve analysis","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
