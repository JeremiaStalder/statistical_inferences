As Schmidt [-@schmidt_shall_2009] writes "There is no such thing as an exact replication." However, it is possible to 1) repeat an experiment where a researcher stays as closely as possible to the original study, 2) repeat an experiment where there it is likely there is some variation in factors that are deemed irrelevant, and 3) knowingly vary aspects of a study design.

Publishing brief reports of replication studies is one solution to identify false positives in the scientific literature [@tullock_publication_1959]. Exact numbers of how many replication studies are performed are difficult to get, as there is no complete database that keeps track of all replication studies (but see [Curate Science](https://curatescience.org/), [Replication WIKI](https://replication.uni-goettingen.de/wiki/index.php/Main_Page) or the [Replication Database]https://metaanalyses.shinyapps.io/replicationdatabase/). Replication studies in psychology are rare. One reason is that journals prefer novel findings over replication studies. Over the last years, several journals have started to explicitly state they will publish replication studies. And **replication and extension studies** (where an original study is replicated, but additional conditions are added that test a novel hypotheses) are a great way to build cumulative knowledge [bonett_replication-extension_2012].


Replication Value idea Pe   der Isager

Within a methodological falsificationist philosophy of science, the goal of replication studies is to make claims about observational statements: By repeating a specified methodological procedure, a predicted result was observed (or not). As De Groot [-@de_groot_methodology_1969, p. 89] writes: "If one knows something to be true, he is in a position to predict; where prediction is impossible there is no knowledge". In a replication study, the prediction can be as weak as 'if I repeat the procedure in the original study, the same result should be observed'. 'Close' or 'direct' replication studies are therefore not a test of a theory, and they do not tell us anything about the truth of claims. That is not their role in science. Their only function is to test whether repeating a methodological procedure will lead to a predicted observation. 

Because of Type 1 and Type 2 errors a series of replication studies should yield mixed results, but a series of well-designed studies (e.g., with sufficient power) should be able to distinguish between replicable and non-replicable effects. Of course replicable effects can be due to mechanisms that are unrelated to the theory that is tested, and non-replicable effects might be due to unknown moderators that vary across replication studies. To make *theoretical inferences* based upon *statistical inferences* researchers need to critically reflect on the **validity** of their claims [@shadish_experimental_2001; @schiavone_consensus-based_2023]. 


There are studies that are impossible to replicate directly. 


Not all researchers agree that their science has inter-subjectively repeatable observations. In what is called the 'crisis in social psychology' Gergen [-@gergen_social_1973] argued social psychology was not a cumulative science: 

> It is the purpose of this paper to argue that social psychology is primarily an historical inquiry. Unlike the natural sciences, it deals with facts that are largely nonrepeatable and which fluctuate markedly over time. Principles of human interaction cannot readily be developed over time because the facts on which they are based do not generally remain stable. Knowledge cannot accumulate in the usual scientific sense because such knowledge does not generally transcend its historical boundaries.

In this philosophy of psychology researchers give up the aim to build theories upon which generalizable predictions can be made. Most younger researchers no longer know about this 'crisis in psychology' in the 1970's, perhaps because sufficient effects that proved replicable over time were observed. Nevertheless, this does not mean all scientific inquiry in a field such as social psychology should be expected to lead to replicable findings, as it is simply a fact that social behavior changes over time. For example, in the classic 'foot-in-the-door' effect study, Freedman and Fraser [-@freedman_compliance_1966] first called residents in a local community over the phone to ask them to answer some questions (the small request). If participants agreed, they were asked a larger request, which consisted of "five or six men from our staff coming into your home some morning for about 2 hours to enumerate and classify all the household products that you have. They will have to have full freedom in your house to go through the cupboards and storage places." The idea that anyone would nowadays agree to such a request when called by a stranger over the telephone (let along more than 50% as in the original study) seems highly improbable. 

>After this intensive effort to reproduce a sample of published psychological findings, how many of the effects have we established are true? Zero. And how many of the effects have we established are false? Zero. Is this a limitation of the project design? No. It is the reality of doing science, even if it is not appreciated in daily practice. Humans desire certainty, and science infrequently provides it. As much as we might wish it to be otherwise, a single study almost never provides definitive resolution for or against an effect and its explanation. The original studies examined here offered tentative evidence; the replications we conducted offered additional, confirmatory evidence. In some cases, thereplicationsincreaseconfidenceinthereliability of the original results; in other cases, the replications suggest that more investigation is needed to establish the validity of the original findings. Scientific progressisacumulativeprocess of uncertainty reduction that can only succeed if science itself remains the greatest skeptic of its explanatory claims.
